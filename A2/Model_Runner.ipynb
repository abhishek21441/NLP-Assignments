{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file calls all the models and runs them on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import KeyedVectors\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "pad_len = 174\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "with open('test_data_task1.json', 'r', encoding='utf-8') as test_file_task1:\n",
    "    test_data_temp_task1 = json.load(test_file_task1)\n",
    "    test_data_list_task1 = list(test_data_temp_task1.values())\n",
    "    test_data_task1 = [test_dict for test_dict in test_data_list_task1 if len(test_dict[\"labels\"]) <= pad_len]\n",
    "\n",
    "with open('legaltags2labels.json', 'r', encoding='utf-8') as legaltags2labels:\n",
    "    legaltag_label = json.load(legaltags2labels)\n",
    "\n",
    "with open('labels2legaltags.json', 'r', encoding='utf-8') as labels2legaltags:\n",
    "    label_legaltag = json.load(labels2legaltags)\n",
    "\n",
    "\n",
    "# Task 2\n",
    "with open('ATE_test.pkl', 'rb') as test_file_task2:\n",
    "    test_data_dict_task2 = pickle.load(test_file_task2)\n",
    "    test_data_list_task2 = list(test_data_dict_task2.values())\n",
    "    test_data_task2 = [test_dict for test_dict in test_data_list_task2 if len(test_dict[\"labels\"]) <= pad_len]\n",
    "\n",
    "task2_dict = {\"O\" : 0, \"B\" : 1, \"I\":2}\n",
    "task2_dict_rev = {0 : \"O\" , 1 : \"B\" , 2:\"I\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float_conversion(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "GloVe_embeddings_dict = {}\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        # print(len(values))\n",
    "        vector = np.asarray([safe_float_conversion(x) for x in values[1:] if x])\n",
    "        # print(len(vector))\n",
    "        if word not in GloVe_embeddings_dict:\n",
    "            GloVe_embeddings_dict[word] = vector\n",
    "\n",
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(GloVe_embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(GloVe_embeddings_dict[word], embedding))\n",
    "\n",
    "def nearestwords(word,number):\n",
    "    return find_closest_embeddings(GloVe_embeddings_dict[word])[:number]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Fasttext Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Insert fasttext embeddings\n",
    "# # !pip install gensim\n",
    "# # !pip install fasttext\n",
    "\n",
    "# import fasttext\n",
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\n",
    "# model = fasttext.load_model(model_path)\n",
    "# model.words\n",
    "\n",
    "\n",
    "# len(model.words)\n",
    "\n",
    "# fasttext_dict = {}\n",
    "\n",
    "# for word in model.words:\n",
    "#   fasttext_dict[word] = model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_t1(input_dict, pretrained_model, label_dict, pad_len=174):\n",
    "    input_sentence = input_dict[\"text\"].split()\n",
    "    input_tags = input_dict[\"labels\"]\n",
    "    idxs, tags = [], []\n",
    "\n",
    "    for index in range(len(input_sentence)):\n",
    "        if input_sentence[index] in pretrained_model:\n",
    "            temp = pretrained_model[input_sentence[index]]\n",
    "            temp = torch.tensor(temp, dtype=torch.long)\n",
    "            # print(temp.size())\n",
    "            idxs.append(temp)\n",
    "            tags.append(label_dict[input_tags[index]])\n",
    "\n",
    "    # Pad idxs and tags\n",
    "    if len(tags) < pad_len:\n",
    "        tags += [26] * (pad_len - len(tags))\n",
    "\n",
    "    # Pad the embeddings\n",
    "    if len(idxs) < pad_len:\n",
    "        idxs += [torch.zeros(300)] * (pad_len - len(idxs))\n",
    "\n",
    "    stacked_tensor = torch.stack([tensor.unsqueeze(0) for tensor in idxs], dim=0)\n",
    "    return stacked_tensor, torch.tensor(tags, dtype=torch.long)\n",
    "\n",
    "def prepare_sequence_t2(input_dict, pretrained_model, label_dict, pad_len=174):\n",
    "    input_sentence = input_dict[\"text\"].split()\n",
    "    input_tags = input_dict[\"labels\"]\n",
    "    idxs, tags = [], []\n",
    "\n",
    "    for index in range(len(input_sentence)):\n",
    "        if input_sentence[index] in pretrained_model:\n",
    "            temp = pretrained_model[input_sentence[index]]\n",
    "            temp = torch.tensor(temp, dtype=torch.long)\n",
    "            # print(temp.size())\n",
    "            idxs.append(temp)\n",
    "            tags.append(label_dict[input_tags[index]])\n",
    "\n",
    "    # Pad idxs and tags\n",
    "    if len(tags) < pad_len:\n",
    "        tags += [0] * (pad_len - len(tags))\n",
    "\n",
    "    # Pad the embeddings\n",
    "    if len(idxs) < pad_len:\n",
    "        idxs += [torch.zeros(300)] * (pad_len - len(idxs))\n",
    "\n",
    "    stacked_tensor = torch.stack([tensor.unsqueeze(0) for tensor in idxs], dim=0)\n",
    "    return stacked_tensor, torch.tensor(tags, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_t1(model, test_data, pretrained_embeddings, labels_dict):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "\n",
    "    test_preds, test_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in test_data:\n",
    "\n",
    "            # Prepare input and target sequences\n",
    "            input_indices, output_indices = prepare_sequence_t1(sample, pretrained_embeddings, labels_dict)\n",
    "            if input_indices.size()[0] == 0:\n",
    "                continue  # Skip if the input sequence is empty\n",
    "            \n",
    "            outputs = model(input_indices)\n",
    "            _, predicted = torch.max(outputs.data, 2)  # Get the predicted classes\n",
    "            \n",
    "            test_preds.extend(predicted.view(-1).numpy())  # Flatten and store predictions\n",
    "            test_true.extend(output_indices.view(-1).numpy()) \n",
    "\n",
    "    # Compute metrics\n",
    "    # Calculate the F1 score for the test set\n",
    "    test_f1 = f1_score(test_true, test_preds, average='macro')\n",
    "\n",
    "    # Calculate accuracy for the test set\n",
    "    test_accuracy = accuracy_score(test_true, test_preds)\n",
    "\n",
    "    # Print the F1 score and accuracy\n",
    "    print(f'Test F1 Score: {test_f1}')\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# test_model(model, test_data, word2vec_model, legaltag_label)\n",
    "    \n",
    "def test_model_t2(model, test_data, pretrained_embeddings, labels_dict):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "\n",
    "    test_preds, test_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in test_data:\n",
    "\n",
    "            # Prepare input and target sequences\n",
    "            input_indices, output_indices = prepare_sequence_t2(sample, pretrained_embeddings, labels_dict)\n",
    "            if input_indices.size()[0] == 0:\n",
    "                continue  # Skip if the input sequence is empty\n",
    "            \n",
    "            outputs = model(input_indices)\n",
    "            _, predicted = torch.max(outputs.data, 2)  # Get the predicted classes\n",
    "            \n",
    "            test_preds.extend(predicted.view(-1).numpy())  # Flatten and store predictions\n",
    "            test_true.extend(output_indices.view(-1).numpy()) \n",
    "\n",
    "    # Compute metrics\n",
    "    # Calculate the F1 score for the test set\n",
    "    test_f1 = f1_score(test_true, test_preds, average='macro')\n",
    "\n",
    "    # Calculate accuracy for the test set\n",
    "    test_accuracy = accuracy_score(test_true, test_preds)\n",
    "\n",
    "    # Print the F1 score and accuracy\n",
    "    print(f'Test F1 Score: {test_f1}')\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# test_model(model, test_data, word2vec_model, legaltag_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)  # Take the output of the last timestep\n",
    "        return out\n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)  # Take the output of the last timestep\n",
    "        return out\n",
    "    \n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out)  # Take the output of the last timestep\n",
    "        return out\n",
    "    \n",
    "# Add BiLSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model_class, pretrained_model_path):\n",
    "    # Load the pretrained model\n",
    "    pretrained_model = torch.load(pretrained_model_path)\n",
    "    \n",
    "    # Infer dimensions from the loaded model tensors\n",
    "    if \"RNN\" in pretrained_model_path:\n",
    "        input_dim = pretrained_model['rnn.weight_ih_l0'].shape[1]\n",
    "        hidden_dim = pretrained_model['rnn.weight_hh_l0'].shape[0]\n",
    "        output_dim = pretrained_model['fc.weight'].shape[0]\n",
    "        layers = 1\n",
    "        model = model_class(input_dim, hidden_dim, output_dim, layers)\n",
    "\n",
    "    elif \"GRU\" in pretrained_model_path:\n",
    "        input_dim = pretrained_model['gru.weight_ih_l0'].shape[1]\n",
    "        hidden_dim = pretrained_model['gru.weight_hh_l0'].shape[0]\n",
    "        output_dim = pretrained_model['fc.weight'].shape[0]\n",
    "        layers = 1\n",
    "        model = model_class(input_dim, hidden_dim, output_dim, layers)\n",
    "\n",
    "    elif \"LSTM\" in pretrained_model_path:\n",
    "        input_dim = pretrained_model['lstm.weight_ih_l0'].shape[1]\n",
    "        hidden_dim = pretrained_model['lstm.weight_hh_l0'].shape[0]\n",
    "        output_dim = pretrained_model['fc.weight'].shape[0]\n",
    "        layers = 1\n",
    "        model = model_class(input_dim, hidden_dim, output_dim, layers)\n",
    "    \n",
    "    # Load the entire pretrained model (including model architecture and state_dict)\n",
    "    model.load_state_dict(pretrained_model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Task1_RNN_word2vec = RNNModel(input_dim=300, hidden_dim=64, output_dim=27, num_layers=1)\n",
    "Task1_RNN_word2vec.load_state_dict(torch.load(\"Task1_RNN_word2vec.pth\"))\n",
    "Task1_RNN_GloVe = RNNModel(input_dim=300, hidden_dim=256, output_dim=27, num_layers=1)\n",
    "Task1_RNN_GloVe.load_state_dict(torch.load(\"Task1_RNN_GloVe.pth\"))\n",
    "# Task1_RNN_fasttext = RNNModel(input_dim=300, hidden_dim=256, output_dim=27, num_layers=1)\n",
    "# Task1_RNN_fasttext.load_state_dict(torch.load(\"models\\Task1_RNN_fasttext.pth\"))\n",
    "\n",
    "Task1_LSTM_word2vec = LSTMModel(input_dim=300, hidden_dim=64, output_dim=27, num_layers=1,dropout=0)\n",
    "Task1_LSTM_word2vec.load_state_dict(torch.load(\"Task1_LSTM_word2vec.pth\"))\n",
    "Task1_LSTM_GloVe = LSTMModel(input_dim=300, hidden_dim=256, output_dim=27, num_layers=1,dropout=0)\n",
    "Task1_LSTM_GloVe.load_state_dict(torch.load(\"Task1_LSTM_GloVe.pth\"))\n",
    "# Task1_LSTM_fasttext = LSTMModel(input_dim=300, hidden_dim=5, output_dim=27, num_layers=1,dropout=0)\n",
    "# Task1_LSTM_fasttext.load_state_dict(torch.load(\"Task1_LSTM_Fasttext.pth\"))\n",
    "\n",
    "Task1_GRU_word2vec = GRUModel(input_dim=300, hidden_dim=64, output_dim=27, num_layers=1)\n",
    "Task1_GRU_word2vec.load_state_dict(torch.load(\"Task1_GRU_word2vec.pth\"))\n",
    "Task1_GRU_GloVe = GRUModel(input_dim=300, hidden_dim=64, output_dim=27, num_layers=1)  \n",
    "Task1_GRU_GloVe.load_state_dict(torch.load(\"Task1_GRU_GloVe.pth\"))\n",
    "# Task1_GRU_fasttext = GRUModel(input_dim=300, hidden_dim=64, output_dim=27, num_layers=1)\n",
    "# Task1_GRU_fasttext.load_state_dict(torch.load(\"Task1_GRU_Fasttext.pth\"))\n",
    "\n",
    "# Add BiLSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Task2_RNN_word2vec = RNNModel(input_dim=300, hidden_dim=256, output_dim=3, num_layers=1)\n",
    "Task2_RNN_word2vec.load_state_dict(torch.load(\"Task2_RNN_word2vec.pth\"))\n",
    "Task2_RNN_GloVe = RNNModel(input_dim=300, hidden_dim=256, output_dim=3, num_layers=1)\n",
    "Task2_RNN_GloVe.load_state_dict(torch.load(\"Task2_RNN_GloVe.pth\"))\n",
    "# Task2_RNN_fasttext = RNNModel(input_dim=300, hidden_dim=256, output_dim=3, num_layers=1) \n",
    "# Task2_RNN_fasttext.load_state_dict(torch.load(\"Task2_RNN_Fasttext.pth\"))\n",
    "\n",
    "\n",
    "Task2_LSTM_word2vec = LSTMModel(input_dim=300, hidden_dim=256, output_dim=3, num_layers=1,dropout=0)\n",
    "Task2_LSTM_word2vec.load_state_dict(torch.load(\"Task2_LSTM_word2vec.pth\"))\n",
    "Task2_LSTM_GloVe = LSTMModel(input_dim=300, hidden_dim=256, output_dim=3, num_layers=1,dropout=0)\n",
    "Task2_LSTM_GloVe.load_state_dict(torch.load(\"Task2_LSTM_GloVe.pth\"))\n",
    "# Task2_LSTM_fasttext = LSTMModel(input_dim=300, hidden_dim=256, output_dim=3, num_layers=1,dropout=0)\n",
    "# Task2_LSTM_fasttext.load_state_dict(torch.load(\"Task2_LSTM_fasttext.pth\"))\n",
    "\n",
    "\n",
    "Task2_GRU_word2vec = GRUModel(input_dim=300, hidden_dim=64, output_dim=3, num_layers=1)\n",
    "Task2_GRU_word2vec.load_state_dict(torch.load(\"Task2_GRU_word2vec.pth\"))\n",
    "Task2_GRU_GloVe = GRUModel(input_dim=300, hidden_dim=256, output_dim=3, num_layers=1)\n",
    "Task2_GRU_GloVe.load_state_dict(torch.load(\"Task2_GRU_GloVe.pth\"))\n",
    "# Task2_GRU_fasttext = GRUModel(input_dim=300, hidden_dim=256, output_dim=3, num_layers=1) # Giving Error\n",
    "# Task2_GRU_fasttext.load_state_dict(torch.load(\"Task2_GRU_fasttext.pth\"))\n",
    "\n",
    "# Add BiLSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task-1 RNN Word2Vec\n",
      "Test F1 Score: 0.03665188258262209\n",
      "Test Accuracy: 0.979415718905563\n",
      "\n",
      "Task-1 RNN GloVe\n",
      "Test F1 Score: 0.13225549419595778\n",
      "Test Accuracy: 0.9903528214817775\n",
      "\n",
      "Task-1 LSTM Word2Vec\n",
      "Test F1 Score: 0.03665188258262209\n",
      "Test Accuracy: 0.979415718905563\n",
      "\n",
      "Task-1 LSTM GloVe\n",
      "Test F1 Score: 0.15805884841311724\n",
      "Test Accuracy: 0.9906556205564235\n",
      "\n",
      "Task-1 GRU Word2Vec\n",
      "Test F1 Score: 0.03665188258262209\n",
      "Test Accuracy: 0.979415718905563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Task-1 RNN Word2Vec\")\n",
    "test_model_t1(Task1_RNN_word2vec, test_data_list_task1, word2vec_model, legaltag_label)\n",
    "print()\n",
    "\n",
    "print(\"Task-1 RNN GloVe\")\n",
    "test_model_t1(Task1_RNN_GloVe, test_data_list_task1, GloVe_embeddings_dict, legaltag_label)\n",
    "print()\n",
    "\n",
    "# print(\"Task-1 RNN fasttext\")\n",
    "# test_model_t1(Task1_RNN_fasttext, test_data_list_task1, fasttext_dict, legaltag_label) # rename fasttext dict\n",
    "# print()\n",
    "\n",
    "\n",
    "print(\"Task-1 LSTM Word2Vec\")\n",
    "test_model_t1(Task1_LSTM_word2vec, test_data_list_task1, word2vec_model, legaltag_label)\n",
    "print()\n",
    "\n",
    "print(\"Task-1 LSTM GloVe\")\n",
    "test_model_t1(Task1_LSTM_GloVe, test_data_list_task1, GloVe_embeddings_dict, legaltag_label)\n",
    "print()\n",
    "\n",
    "# print(\"Task-1 LSTM fasttext\")\n",
    "# test_model_t1(Task1_LSTM_fasttext, test_data_list_task1, fasttext_dict, legaltag_label) # rename fasttext dict\n",
    "# print()\n",
    "\n",
    "\n",
    "print(\"Task-1 GRU Word2Vec\")\n",
    "test_model_t1(Task1_GRU_word2vec, test_data_list_task1, word2vec_model, legaltag_label)\n",
    "print()\n",
    "\n",
    "print(\"Task-1 GRU GloVe\")\n",
    "test_model_t1(Task1_GRU_GloVe, test_data_list_task1, GloVe_embeddings_dict, legaltag_label)\n",
    "print()\n",
    "\n",
    "# print(\"Task-1 GRU fasttext\")\n",
    "# test_model_t1(Task1_GRU_fasttext, test_data_list_task1, fasttext_dict, legaltag_label) # rename fasttext dict\n",
    "# print()\n",
    "\n",
    "\n",
    "# # Add BiLSTM CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task-2 RNN Word2Vec\n",
      "Test F1 Score: 0.3313179032476041\n",
      "Test Accuracy: 0.9879800953181945\n",
      "\n",
      "Task-2 RNN GloVe\n",
      "Test F1 Score: 0.5387312564922245\n",
      "Test Accuracy: 0.9916771797028315\n",
      "\n",
      "Task-2 LSTM Word2Vec\n",
      "Test F1 Score: 0.3313179032476041\n",
      "Test Accuracy: 0.9879800953181945\n",
      "\n",
      "Task-2 LSTM GloVe\n",
      "Test F1 Score: 0.5916985066094422\n",
      "Test Accuracy: 0.9921327446033081\n",
      "\n",
      "Task-2 GRU Word2Vec\n",
      "Test F1 Score: 0.3313179032476041\n",
      "Test Accuracy: 0.9879800953181945\n",
      "\n",
      "Task-2 GRU GloVe\n",
      "Test F1 Score: 0.5873960406533144\n",
      "Test Accuracy: 0.9920626576955425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Task-2 RNN Word2Vec\")\n",
    "test_model_t2(Task2_RNN_word2vec, test_data_list_task2, word2vec_model, task2_dict)\n",
    "print()\n",
    "\n",
    "print(\"Task-2 RNN GloVe\")\n",
    "test_model_t2(Task2_RNN_GloVe, test_data_list_task2, GloVe_embeddings_dict, task2_dict)\n",
    "print()\n",
    "\n",
    "# print(\"Task-2 RNN fasttext\")\n",
    "# # test_model_t2(Task2_RNN_fasttext, test_data_list_task2, word2vec_model, task2_dict) # rename fasttext dict\n",
    "# print()\n",
    "\n",
    "\n",
    "print(\"Task-2 LSTM Word2Vec\")\n",
    "test_model_t2(Task2_LSTM_word2vec, test_data_list_task2, word2vec_model, task2_dict)\n",
    "print()\n",
    "\n",
    "print(\"Task-2 LSTM GloVe\")\n",
    "test_model_t2(Task2_LSTM_GloVe, test_data_list_task2, GloVe_embeddings_dict, task2_dict)\n",
    "print()\n",
    "\n",
    "# print(\"Task-2 LSTM fasttext\")\n",
    "# # test_model_t2(Task2_LSTM_fasttext, test_data_list_task2, word2vec_model, task2_dict) # rename fasttext dict\n",
    "# print()\n",
    "\n",
    "\n",
    "print(\"Task-2 GRU Word2Vec\")\n",
    "test_model_t2(Task2_GRU_word2vec, test_data_list_task2, word2vec_model, task2_dict)\n",
    "print()\n",
    "\n",
    "print(\"Task-2 GRU GloVe\")\n",
    "test_model_t2(Task2_GRU_GloVe, test_data_list_task2, GloVe_embeddings_dict, task2_dict)\n",
    "print()\n",
    "\n",
    "# print(\"Task-2 GRU fasttext\")\n",
    "# # test_model_t2(Task2_GRU_fasttext, test_data_list_task2, word2vec_model, task2_dict) # rename fasttext dict\n",
    "# print()\n",
    "\n",
    "\n",
    "# Add BiLSTM CRF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
