{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36348f5ae5434756959c5fb975ed7241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_806509db4b2a45eb83521a8e4af07ca1",
              "IPY_MODEL_0910d5fe0daa4d4bad003c8e317163a5",
              "IPY_MODEL_7c94bd318b404a858300a5e40ba5133b"
            ],
            "layout": "IPY_MODEL_4798d635b9c8462d8b2d78f20fcd918a"
          }
        },
        "806509db4b2a45eb83521a8e4af07ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19e5bff2a6f7428194d74b1ed377510e",
            "placeholder": "​",
            "style": "IPY_MODEL_50fec253a4d14a26b28932cacb10085e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0910d5fe0daa4d4bad003c8e317163a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_168419f084754836a1188fd28386f7f2",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_367b2b8917a9449090f073ad892ed699",
            "value": 48
          }
        },
        "7c94bd318b404a858300a5e40ba5133b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f1a263dcb724369b035d566717f7a7c",
            "placeholder": "​",
            "style": "IPY_MODEL_69534e743baa4d2e931d7042ecbe36a8",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.64kB/s]"
          }
        },
        "4798d635b9c8462d8b2d78f20fcd918a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e5bff2a6f7428194d74b1ed377510e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50fec253a4d14a26b28932cacb10085e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "168419f084754836a1188fd28386f7f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "367b2b8917a9449090f073ad892ed699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f1a263dcb724369b035d566717f7a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69534e743baa4d2e931d7042ecbe36a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "519c45fe12d04acfb03b0b73d66c0cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1db66363cc2240d2a15a45bd4ac27e2c",
              "IPY_MODEL_4f725f88ce2d4aada6ca167b90aa6a8a",
              "IPY_MODEL_754c98d0102048dc959d81e395b9f4ef"
            ],
            "layout": "IPY_MODEL_8ec35ff52db14b8c809b86b1c5671569"
          }
        },
        "1db66363cc2240d2a15a45bd4ac27e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cdb6fe00d8a499b9cd0193413bd6708",
            "placeholder": "​",
            "style": "IPY_MODEL_3b5baff87a2142f78f12810631b75b39",
            "value": "vocab.txt: 100%"
          }
        },
        "4f725f88ce2d4aada6ca167b90aa6a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6bce4cee6934041bde08ba2d0786408",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_938c8c1394a84208a370fee0bb34e073",
            "value": 231508
          }
        },
        "754c98d0102048dc959d81e395b9f4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb5c2305bae429ab6bba1da32554e0f",
            "placeholder": "​",
            "style": "IPY_MODEL_f1f4863acbe94440ad5c03a3c303d673",
            "value": " 232k/232k [00:00&lt;00:00, 473kB/s]"
          }
        },
        "8ec35ff52db14b8c809b86b1c5671569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cdb6fe00d8a499b9cd0193413bd6708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5baff87a2142f78f12810631b75b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6bce4cee6934041bde08ba2d0786408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "938c8c1394a84208a370fee0bb34e073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdb5c2305bae429ab6bba1da32554e0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f4863acbe94440ad5c03a3c303d673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d757759bc2fe4b6a81cb63d661464fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bf6bc0e6fee4e499110c953489e89fa",
              "IPY_MODEL_0016cf3688de46ba8d375ccb170b5da0",
              "IPY_MODEL_a0e4e9696da2427e804af737c6fc57c5"
            ],
            "layout": "IPY_MODEL_b1e6f1a296e0403ebfc60e224d20e13f"
          }
        },
        "0bf6bc0e6fee4e499110c953489e89fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a127b1602df54111845d4bc59fe8cbc2",
            "placeholder": "​",
            "style": "IPY_MODEL_db55a4ff8e204e98afec257840bf48b0",
            "value": "tokenizer.json: 100%"
          }
        },
        "0016cf3688de46ba8d375ccb170b5da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c282da62c044b6395b38e438a8d9823",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d547bf251e74191a17ef3134216ee44",
            "value": 466062
          }
        },
        "a0e4e9696da2427e804af737c6fc57c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33e3e7af3b314073aea3713ac5a21e4c",
            "placeholder": "​",
            "style": "IPY_MODEL_0861c893fcf943ae9ec6e00d308978b2",
            "value": " 466k/466k [00:00&lt;00:00, 653kB/s]"
          }
        },
        "b1e6f1a296e0403ebfc60e224d20e13f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a127b1602df54111845d4bc59fe8cbc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db55a4ff8e204e98afec257840bf48b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c282da62c044b6395b38e438a8d9823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d547bf251e74191a17ef3134216ee44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33e3e7af3b314073aea3713ac5a21e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0861c893fcf943ae9ec6e00d308978b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de5ed084b1394d6c9651dd78df6b305e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97bf7c9b183143a598d77a21867bdd6c",
              "IPY_MODEL_dbf2801db2fa41f5b655c2cb224e1fb5",
              "IPY_MODEL_ff3fe8754bd340fd862ab6c94acd15c4"
            ],
            "layout": "IPY_MODEL_3129a11b8ef84dddb37f47e015606c7d"
          }
        },
        "97bf7c9b183143a598d77a21867bdd6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa61568e7422424e907c82ecccb1bd16",
            "placeholder": "​",
            "style": "IPY_MODEL_910012d7beb241f0a5ee7196297a74af",
            "value": "config.json: 100%"
          }
        },
        "dbf2801db2fa41f5b655c2cb224e1fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f283273ee8f4468b72cb25bb6b5d639",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c7fd887192d4d809803334af08ee371",
            "value": 570
          }
        },
        "ff3fe8754bd340fd862ab6c94acd15c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_542e0de2b62d4ed590608c8af1148c03",
            "placeholder": "​",
            "style": "IPY_MODEL_1ffe78a905cd4482b2f30c14586cb676",
            "value": " 570/570 [00:00&lt;00:00, 38.1kB/s]"
          }
        },
        "3129a11b8ef84dddb37f47e015606c7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa61568e7422424e907c82ecccb1bd16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "910012d7beb241f0a5ee7196297a74af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f283273ee8f4468b72cb25bb6b5d639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c7fd887192d4d809803334af08ee371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "542e0de2b62d4ed590608c8af1148c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffe78a905cd4482b2f30c14586cb676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d0cf8c53dfb43879d1cb50448538f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85bbb4088d3249a29fbb2354fcd241db",
              "IPY_MODEL_7be2f4fb8b214dca8946e6fa8627504b",
              "IPY_MODEL_39955bb67cfb4249a7b6dee933708dd4"
            ],
            "layout": "IPY_MODEL_ab8db9f63dec467691c686c1c07e50a8"
          }
        },
        "85bbb4088d3249a29fbb2354fcd241db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea12d8d8b7445b79a621d55d4497666",
            "placeholder": "​",
            "style": "IPY_MODEL_9e5fc970a6f34cfbace9aa9c802c2d57",
            "value": "model.safetensors: 100%"
          }
        },
        "7be2f4fb8b214dca8946e6fa8627504b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37e2983bfc974cf98a4c85b340ba2e7d",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff08481e565549dfa8085b49b6845712",
            "value": 440449768
          }
        },
        "39955bb67cfb4249a7b6dee933708dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f8434c41e154f74a242b6cb5d83b53e",
            "placeholder": "​",
            "style": "IPY_MODEL_2afe5be8ed2941b686769b111bfbd4c5",
            "value": " 440M/440M [00:01&lt;00:00, 297MB/s]"
          }
        },
        "ab8db9f63dec467691c686c1c07e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea12d8d8b7445b79a621d55d4497666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e5fc970a6f34cfbace9aa9c802c2d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37e2983bfc974cf98a4c85b340ba2e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff08481e565549dfa8085b49b6845712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f8434c41e154f74a242b6cb5d83b53e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2afe5be8ed2941b686769b111bfbd4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT STS Task"
      ],
      "metadata": {
        "id": "FC6WZdnG2TuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"train.csv\",sep=\"\\t\")\n",
        "val_data = pd.read_csv(\"dev.csv\",sep=\"\\t\")"
      ],
      "metadata": {
        "id": "129BYvNN6Cmv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dropna(inplace=True)\n",
        "val_data.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "tD43LhD66MhW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Y-o-Jfpd4Axm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare data (Assuming 'data' is defined)\n",
        "input_sentences = [(row[1][\"sentence1\"], row[1][\"sentence2\"]) for row in train_data.iterrows()]\n",
        "similarity_scores = list(train_data[\"score\"])  # Example similarity scores\n",
        "\n",
        "# Tokenize input sentences\n",
        "train_tokenized_inputs = tokenizer(input_sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "\n",
        "# Convert similarity scores to torch tensors\n",
        "train_labels = torch.tensor(similarity_scores, dtype=torch.float32).unsqueeze(1).to(device)  # Unsqueeze to match logits shape\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_dataset = TensorDataset(train_tokenized_inputs.input_ids, train_tokenized_inputs.token_type_ids, train_tokenized_inputs.attention_mask, train_labels)\n",
        "\n",
        "\n",
        "# Prepare data (Assuming 'data' is defined)\n",
        "input_sentences = [(row[1][\"setence1\"], row[1][\"sentence2\"]) for row in val_data.iterrows()]\n",
        "similarity_scores = list(val_data[\"score\"])  # Example similarity scores\n",
        "\n",
        "# Tokenize input sentences\n",
        "val_tokenized_inputs = tokenizer(input_sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "\n",
        "# Convert similarity scores to torch tensors\n",
        "val_labels = torch.tensor(similarity_scores, dtype=torch.float32).unsqueeze(1).to(device)  # Unsqueeze to match logits shape\n",
        "\n",
        "# Create a TensorDataset\n",
        "val_dataset = TensorDataset(val_tokenized_inputs.input_ids, val_tokenized_inputs.token_type_ids, val_tokenized_inputs.attention_mask, val_labels)\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "train_loss_epochs = []\n",
        "val_loss_epochs = []\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(5):\n",
        "    print(\"Training\")\n",
        "    print(\"Epoch Number:\", epoch)\n",
        "    batch_number = 1\n",
        "    # Training phase\n",
        "    train_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        print(\"Batch Number:\", batch_number)\n",
        "        input_ids, token_type_ids, attention_mask, targets = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits.squeeze()\n",
        "        targets = targets.squeeze()\n",
        "        # Use mean square error (MSE) loss\n",
        "        loss = torch.nn.functional.mse_loss(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(\"Loss:\",loss.item())\n",
        "        train_loss += loss.item()\n",
        "        batch_number += 1\n",
        "    # Calculate average training loss for the epoch\n",
        "    average_train_loss = train_loss / len(train_loader)\n",
        "    train_loss_epochs.append(average_train_loss)\n",
        "    # Validation phase\n",
        "    val_loss = 0.0\n",
        "    print(\"Validation\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, token_type_ids, attention_mask, targets = batch\n",
        "            outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits.squeeze()\n",
        "            targets = targets.squeeze()\n",
        "            # Calculate validation loss using MSE\n",
        "            loss = torch.nn.functional.mse_loss(logits, targets)\n",
        "            print(\"Validation Loss:\",loss.item())\n",
        "            val_loss += loss.item()\n",
        "    # Calculate average validation loss for the epoch\n",
        "    average_val_loss = val_loss / len(val_loader)\n",
        "    val_loss_epochs.append(average_val_loss)\n",
        "    # Print average training and validation loss for the epoch\n",
        "    print(\"Average Training Loss:\", average_train_loss)\n",
        "    print(\"Average Validation Loss:\", average_val_loss)\n",
        "\n",
        "    model.train()  # Set model back to training mode\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "36348f5ae5434756959c5fb975ed7241",
            "806509db4b2a45eb83521a8e4af07ca1",
            "0910d5fe0daa4d4bad003c8e317163a5",
            "7c94bd318b404a858300a5e40ba5133b",
            "4798d635b9c8462d8b2d78f20fcd918a",
            "19e5bff2a6f7428194d74b1ed377510e",
            "50fec253a4d14a26b28932cacb10085e",
            "168419f084754836a1188fd28386f7f2",
            "367b2b8917a9449090f073ad892ed699",
            "7f1a263dcb724369b035d566717f7a7c",
            "69534e743baa4d2e931d7042ecbe36a8",
            "519c45fe12d04acfb03b0b73d66c0cb7",
            "1db66363cc2240d2a15a45bd4ac27e2c",
            "4f725f88ce2d4aada6ca167b90aa6a8a",
            "754c98d0102048dc959d81e395b9f4ef",
            "8ec35ff52db14b8c809b86b1c5671569",
            "3cdb6fe00d8a499b9cd0193413bd6708",
            "3b5baff87a2142f78f12810631b75b39",
            "e6bce4cee6934041bde08ba2d0786408",
            "938c8c1394a84208a370fee0bb34e073",
            "bdb5c2305bae429ab6bba1da32554e0f",
            "f1f4863acbe94440ad5c03a3c303d673",
            "d757759bc2fe4b6a81cb63d661464fde",
            "0bf6bc0e6fee4e499110c953489e89fa",
            "0016cf3688de46ba8d375ccb170b5da0",
            "a0e4e9696da2427e804af737c6fc57c5",
            "b1e6f1a296e0403ebfc60e224d20e13f",
            "a127b1602df54111845d4bc59fe8cbc2",
            "db55a4ff8e204e98afec257840bf48b0",
            "4c282da62c044b6395b38e438a8d9823",
            "5d547bf251e74191a17ef3134216ee44",
            "33e3e7af3b314073aea3713ac5a21e4c",
            "0861c893fcf943ae9ec6e00d308978b2",
            "de5ed084b1394d6c9651dd78df6b305e",
            "97bf7c9b183143a598d77a21867bdd6c",
            "dbf2801db2fa41f5b655c2cb224e1fb5",
            "ff3fe8754bd340fd862ab6c94acd15c4",
            "3129a11b8ef84dddb37f47e015606c7d",
            "aa61568e7422424e907c82ecccb1bd16",
            "910012d7beb241f0a5ee7196297a74af",
            "5f283273ee8f4468b72cb25bb6b5d639",
            "9c7fd887192d4d809803334af08ee371",
            "542e0de2b62d4ed590608c8af1148c03",
            "1ffe78a905cd4482b2f30c14586cb676",
            "5d0cf8c53dfb43879d1cb50448538f06",
            "85bbb4088d3249a29fbb2354fcd241db",
            "7be2f4fb8b214dca8946e6fa8627504b",
            "39955bb67cfb4249a7b6dee933708dd4",
            "ab8db9f63dec467691c686c1c07e50a8",
            "7ea12d8d8b7445b79a621d55d4497666",
            "9e5fc970a6f34cfbace9aa9c802c2d57",
            "37e2983bfc974cf98a4c85b340ba2e7d",
            "ff08481e565549dfa8085b49b6845712",
            "8f8434c41e154f74a242b6cb5d83b53e",
            "2afe5be8ed2941b686769b111bfbd4c5"
          ]
        },
        "id": "Vaq3lu_fMeG8",
        "outputId": "4382804b-d762-40b8-9143-3d65efe59a3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36348f5ae5434756959c5fb975ed7241"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "519c45fe12d04acfb03b0b73d66c0cb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d757759bc2fe4b6a81cb63d661464fde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de5ed084b1394d6c9651dd78df6b305e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d0cf8c53dfb43879d1cb50448538f06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch Number: 83\n",
            "Loss: 0.36278581619262695\n",
            "Batch Number: 84\n",
            "Loss: 0.11571723222732544\n",
            "Batch Number: 85\n",
            "Loss: 0.283891499042511\n",
            "Batch Number: 86\n",
            "Loss: 0.31518012285232544\n",
            "Batch Number: 87\n",
            "Loss: 0.0801553800702095\n",
            "Batch Number: 88\n",
            "Loss: 0.2165229320526123\n",
            "Batch Number: 89\n",
            "Loss: 0.30846890807151794\n",
            "Batch Number: 90\n",
            "Loss: 0.09962596744298935\n",
            "Batch Number: 91\n",
            "Loss: 0.13391879200935364\n",
            "Batch Number: 92\n",
            "Loss: 0.9060458540916443\n",
            "Batch Number: 93\n",
            "Loss: 0.17851027846336365\n",
            "Batch Number: 94\n",
            "Loss: 0.3550623059272766\n",
            "Batch Number: 95\n",
            "Loss: 0.1896144300699234\n",
            "Batch Number: 96\n",
            "Loss: 1.1033766269683838\n",
            "Batch Number: 97\n",
            "Loss: 0.5774842500686646\n",
            "Batch Number: 98\n",
            "Loss: 0.6740557551383972\n",
            "Batch Number: 99\n",
            "Loss: 0.17639802396297455\n",
            "Batch Number: 100\n",
            "Loss: 0.008908665738999844\n",
            "Batch Number: 101\n",
            "Loss: 0.8876914978027344\n",
            "Batch Number: 102\n",
            "Loss: 0.7762570381164551\n",
            "Batch Number: 103\n",
            "Loss: 0.056075066328048706\n",
            "Batch Number: 104\n",
            "Loss: 0.22601734101772308\n",
            "Batch Number: 105\n",
            "Loss: 0.07711908966302872\n",
            "Batch Number: 106\n",
            "Loss: 0.6504406332969666\n",
            "Batch Number: 107\n",
            "Loss: 0.46657782793045044\n",
            "Batch Number: 108\n",
            "Loss: 0.5410224795341492\n",
            "Batch Number: 109\n",
            "Loss: 0.08355662226676941\n",
            "Batch Number: 110\n",
            "Loss: 0.1524098515510559\n",
            "Batch Number: 111\n",
            "Loss: 0.5378062129020691\n",
            "Batch Number: 112\n",
            "Loss: 0.2719385325908661\n",
            "Batch Number: 113\n",
            "Loss: 0.1537252962589264\n",
            "Batch Number: 114\n",
            "Loss: 0.3190782964229584\n",
            "Batch Number: 115\n",
            "Loss: 0.1316254884004593\n",
            "Batch Number: 116\n",
            "Loss: 0.17921124398708344\n",
            "Batch Number: 117\n",
            "Loss: 0.2649660110473633\n",
            "Batch Number: 118\n",
            "Loss: 0.16441942751407623\n",
            "Batch Number: 119\n",
            "Loss: 0.015857528895139694\n",
            "Batch Number: 120\n",
            "Loss: 0.08029620349407196\n",
            "Batch Number: 121\n",
            "Loss: 0.15776720643043518\n",
            "Batch Number: 122\n",
            "Loss: 0.08247184753417969\n",
            "Batch Number: 123\n",
            "Loss: 0.17610269784927368\n",
            "Batch Number: 124\n",
            "Loss: 0.1411275863647461\n",
            "Batch Number: 125\n",
            "Loss: 0.11065740883350372\n",
            "Batch Number: 126\n",
            "Loss: 0.8488203287124634\n",
            "Batch Number: 127\n",
            "Loss: 0.14418940246105194\n",
            "Batch Number: 128\n",
            "Loss: 0.6252520680427551\n",
            "Batch Number: 129\n",
            "Loss: 0.3174881041049957\n",
            "Batch Number: 130\n",
            "Loss: 0.21289817988872528\n",
            "Batch Number: 131\n",
            "Loss: 0.46173572540283203\n",
            "Batch Number: 132\n",
            "Loss: 0.4198068082332611\n",
            "Batch Number: 133\n",
            "Loss: 0.30715781450271606\n",
            "Batch Number: 134\n",
            "Loss: 0.16044369339942932\n",
            "Batch Number: 135\n",
            "Loss: 0.28296083211898804\n",
            "Batch Number: 136\n",
            "Loss: 0.20934395492076874\n",
            "Batch Number: 137\n",
            "Loss: 0.18656422197818756\n",
            "Batch Number: 138\n",
            "Loss: 0.44328615069389343\n",
            "Batch Number: 139\n",
            "Loss: 0.12403185665607452\n",
            "Batch Number: 140\n",
            "Loss: 0.18384911119937897\n",
            "Batch Number: 141\n",
            "Loss: 0.09949231147766113\n",
            "Batch Number: 142\n",
            "Loss: 0.055503200739622116\n",
            "Batch Number: 143\n",
            "Loss: 0.1287999451160431\n",
            "Batch Number: 144\n",
            "Loss: 0.18813861906528473\n",
            "Batch Number: 145\n",
            "Loss: 0.2355637103319168\n",
            "Batch Number: 146\n",
            "Loss: 0.43505439162254333\n",
            "Batch Number: 147\n",
            "Loss: 0.40961989760398865\n",
            "Batch Number: 148\n",
            "Loss: 0.5354159474372864\n",
            "Batch Number: 149\n",
            "Loss: 0.13942399621009827\n",
            "Batch Number: 150\n",
            "Loss: 0.1109543964266777\n",
            "Batch Number: 151\n",
            "Loss: 0.17597846686840057\n",
            "Batch Number: 152\n",
            "Loss: 0.33367013931274414\n",
            "Batch Number: 153\n",
            "Loss: 0.48616304993629456\n",
            "Batch Number: 154\n",
            "Loss: 0.08356409519910812\n",
            "Batch Number: 155\n",
            "Loss: 0.15497727692127228\n",
            "Batch Number: 156\n",
            "Loss: 0.31408292055130005\n",
            "Batch Number: 157\n",
            "Loss: 0.16279175877571106\n",
            "Batch Number: 158\n",
            "Loss: 0.009290358982980251\n",
            "Batch Number: 159\n",
            "Loss: 0.5032939910888672\n",
            "Batch Number: 160\n",
            "Loss: 0.09530013054609299\n",
            "Batch Number: 161\n",
            "Loss: 0.4163866937160492\n",
            "Batch Number: 162\n",
            "Loss: 0.02435261756181717\n",
            "Batch Number: 163\n",
            "Loss: 0.2679302394390106\n",
            "Batch Number: 164\n",
            "Loss: 0.2571859061717987\n",
            "Batch Number: 165\n",
            "Loss: 0.2883918881416321\n",
            "Batch Number: 166\n",
            "Loss: 0.36911851167678833\n",
            "Batch Number: 167\n",
            "Loss: 0.12415090948343277\n",
            "Batch Number: 168\n",
            "Loss: 0.3150269389152527\n",
            "Batch Number: 169\n",
            "Loss: 0.15356238186359406\n",
            "Batch Number: 170\n",
            "Loss: 0.2259010374546051\n",
            "Batch Number: 171\n",
            "Loss: 0.2110283225774765\n",
            "Batch Number: 172\n",
            "Loss: 0.41325831413269043\n",
            "Batch Number: 173\n",
            "Loss: 0.2037210762500763\n",
            "Batch Number: 174\n",
            "Loss: 0.5952261090278625\n",
            "Batch Number: 175\n",
            "Loss: 0.6532853245735168\n",
            "Batch Number: 176\n",
            "Loss: 0.24222436547279358\n",
            "Batch Number: 177\n",
            "Loss: 0.17392505705356598\n",
            "Batch Number: 178\n",
            "Loss: 0.22579602897167206\n",
            "Batch Number: 179\n",
            "Loss: 0.26157182455062866\n",
            "Batch Number: 180\n",
            "Loss: 0.27221354842185974\n",
            "Batch Number: 181\n",
            "Loss: 0.5330668687820435\n",
            "Batch Number: 182\n",
            "Loss: 0.21334539353847504\n",
            "Batch Number: 183\n",
            "Loss: 0.22912125289440155\n",
            "Batch Number: 184\n",
            "Loss: 0.37603631615638733\n",
            "Batch Number: 185\n",
            "Loss: 0.47936391830444336\n",
            "Batch Number: 186\n",
            "Loss: 0.32649144530296326\n",
            "Batch Number: 187\n",
            "Loss: 0.16648010909557343\n",
            "Batch Number: 188\n",
            "Loss: 0.6650879979133606\n",
            "Batch Number: 189\n",
            "Loss: 0.4090716540813446\n",
            "Batch Number: 190\n",
            "Loss: 0.14726366102695465\n",
            "Batch Number: 191\n",
            "Loss: 0.3124046325683594\n",
            "Batch Number: 192\n",
            "Loss: 0.276351660490036\n",
            "Batch Number: 193\n",
            "Loss: 0.3179570138454437\n",
            "Batch Number: 194\n",
            "Loss: 0.22355638444423676\n",
            "Batch Number: 195\n",
            "Loss: 0.5546606779098511\n",
            "Batch Number: 196\n",
            "Loss: 0.19336096942424774\n",
            "Batch Number: 197\n",
            "Loss: 0.5029735565185547\n",
            "Batch Number: 198\n",
            "Loss: 0.1802687644958496\n",
            "Batch Number: 199\n",
            "Loss: 0.4789189398288727\n",
            "Batch Number: 200\n",
            "Loss: 0.10077934712171555\n",
            "Batch Number: 201\n",
            "Loss: 0.4581405222415924\n",
            "Batch Number: 202\n",
            "Loss: 0.08639688789844513\n",
            "Batch Number: 203\n",
            "Loss: 1.3214203119277954\n",
            "Batch Number: 204\n",
            "Loss: 0.29972726106643677\n",
            "Batch Number: 205\n",
            "Loss: 0.4690193235874176\n",
            "Batch Number: 206\n",
            "Loss: 0.3846336901187897\n",
            "Batch Number: 207\n",
            "Loss: 0.12246152013540268\n",
            "Batch Number: 208\n",
            "Loss: 0.07743128389120102\n",
            "Batch Number: 209\n",
            "Loss: 0.4080381989479065\n",
            "Batch Number: 210\n",
            "Loss: 0.3131312429904938\n",
            "Batch Number: 211\n",
            "Loss: 0.5748651027679443\n",
            "Batch Number: 212\n",
            "Loss: 0.10004587471485138\n",
            "Batch Number: 213\n",
            "Loss: 0.08086065202951431\n",
            "Batch Number: 214\n",
            "Loss: 0.22159171104431152\n",
            "Batch Number: 215\n",
            "Loss: 0.7440547943115234\n",
            "Batch Number: 216\n",
            "Loss: 0.06063274294137955\n",
            "Batch Number: 217\n",
            "Loss: 0.2300560474395752\n",
            "Batch Number: 218\n",
            "Loss: 0.1703995317220688\n",
            "Batch Number: 219\n",
            "Loss: 0.2802022397518158\n",
            "Batch Number: 220\n",
            "Loss: 0.07010222971439362\n",
            "Batch Number: 221\n",
            "Loss: 0.24439166486263275\n",
            "Batch Number: 222\n",
            "Loss: 0.09886834025382996\n",
            "Batch Number: 223\n",
            "Loss: 0.2504574656486511\n",
            "Batch Number: 224\n",
            "Loss: 0.13710807263851166\n",
            "Batch Number: 225\n",
            "Loss: 0.36211004853248596\n",
            "Batch Number: 226\n",
            "Loss: 0.14657630026340485\n",
            "Batch Number: 227\n",
            "Loss: 0.19884581863880157\n",
            "Batch Number: 228\n",
            "Loss: 0.41475602984428406\n",
            "Batch Number: 229\n",
            "Loss: 0.5952624678611755\n",
            "Batch Number: 230\n",
            "Loss: 0.6436133980751038\n",
            "Batch Number: 231\n",
            "Loss: 0.10516315698623657\n",
            "Batch Number: 232\n",
            "Loss: 0.05449102073907852\n",
            "Batch Number: 233\n",
            "Loss: 0.20036473870277405\n",
            "Batch Number: 234\n",
            "Loss: 0.07704959809780121\n",
            "Batch Number: 235\n",
            "Loss: 0.11261539906263351\n",
            "Batch Number: 236\n",
            "Loss: 0.9127708673477173\n",
            "Batch Number: 237\n",
            "Loss: 0.01375509798526764\n",
            "Batch Number: 238\n",
            "Loss: 1.0212725400924683\n",
            "Batch Number: 239\n",
            "Loss: 0.21114325523376465\n",
            "Batch Number: 240\n",
            "Loss: 0.039885181933641434\n",
            "Batch Number: 241\n",
            "Loss: 0.24126091599464417\n",
            "Batch Number: 242\n",
            "Loss: 0.28828415274620056\n",
            "Batch Number: 243\n",
            "Loss: 0.06797011941671371\n",
            "Batch Number: 244\n",
            "Loss: 0.26227256655693054\n",
            "Batch Number: 245\n",
            "Loss: 0.4378969371318817\n",
            "Batch Number: 246\n",
            "Loss: 0.11674492806196213\n",
            "Batch Number: 247\n",
            "Loss: 0.20770595967769623\n",
            "Batch Number: 248\n",
            "Loss: 0.2630997598171234\n",
            "Batch Number: 249\n",
            "Loss: 0.41926804184913635\n",
            "Batch Number: 250\n",
            "Loss: 0.4543936848640442\n",
            "Batch Number: 251\n",
            "Loss: 0.1677779108285904\n",
            "Batch Number: 252\n",
            "Loss: 0.3105863034725189\n",
            "Batch Number: 253\n",
            "Loss: 0.22218556702136993\n",
            "Batch Number: 254\n",
            "Loss: 0.16671089828014374\n",
            "Batch Number: 255\n",
            "Loss: 0.5091599822044373\n",
            "Batch Number: 256\n",
            "Loss: 0.2185913324356079\n",
            "Batch Number: 257\n",
            "Loss: 0.5442618727684021\n",
            "Batch Number: 258\n",
            "Loss: 0.7112061381340027\n",
            "Batch Number: 259\n",
            "Loss: 0.10294251888990402\n",
            "Batch Number: 260\n",
            "Loss: 0.4577014446258545\n",
            "Batch Number: 261\n",
            "Loss: 0.20454004406929016\n",
            "Batch Number: 262\n",
            "Loss: 0.17928418517112732\n",
            "Batch Number: 263\n",
            "Loss: 0.19557194411754608\n",
            "Batch Number: 264\n",
            "Loss: 0.1817278414964676\n",
            "Batch Number: 265\n",
            "Loss: 0.5160134434700012\n",
            "Batch Number: 266\n",
            "Loss: 0.1526184231042862\n",
            "Batch Number: 267\n",
            "Loss: 0.11157752573490143\n",
            "Batch Number: 268\n",
            "Loss: 0.3507796823978424\n",
            "Batch Number: 269\n",
            "Loss: 0.2646772563457489\n",
            "Batch Number: 270\n",
            "Loss: 0.07344834506511688\n",
            "Batch Number: 271\n",
            "Loss: 0.31675174832344055\n",
            "Batch Number: 272\n",
            "Loss: 0.25148820877075195\n",
            "Batch Number: 273\n",
            "Loss: 0.3293977677822113\n",
            "Batch Number: 274\n",
            "Loss: 0.6480053067207336\n",
            "Batch Number: 275\n",
            "Loss: 0.170013889670372\n",
            "Batch Number: 276\n",
            "Loss: 0.537594735622406\n",
            "Batch Number: 277\n",
            "Loss: 0.18116892874240875\n",
            "Batch Number: 278\n",
            "Loss: 0.32521459460258484\n",
            "Batch Number: 279\n",
            "Loss: 0.5831213593482971\n",
            "Batch Number: 280\n",
            "Loss: 0.5202381610870361\n",
            "Batch Number: 281\n",
            "Loss: 0.20171737670898438\n",
            "Batch Number: 282\n",
            "Loss: 0.20999887585639954\n",
            "Batch Number: 283\n",
            "Loss: 0.22781561315059662\n",
            "Batch Number: 284\n",
            "Loss: 0.29130077362060547\n",
            "Batch Number: 285\n",
            "Loss: 0.3119352459907532\n",
            "Batch Number: 286\n",
            "Loss: 0.40535759925842285\n",
            "Batch Number: 287\n",
            "Loss: 0.04205111786723137\n",
            "Batch Number: 288\n",
            "Loss: 0.1772303581237793\n",
            "Batch Number: 289\n",
            "Loss: 0.587692379951477\n",
            "Batch Number: 290\n",
            "Loss: 0.19619058072566986\n",
            "Batch Number: 291\n",
            "Loss: 0.144156813621521\n",
            "Batch Number: 292\n",
            "Loss: 0.28259438276290894\n",
            "Batch Number: 293\n",
            "Loss: 0.021717626601457596\n",
            "Batch Number: 294\n",
            "Loss: 0.1604049801826477\n",
            "Batch Number: 295\n",
            "Loss: 0.1753341108560562\n",
            "Batch Number: 296\n",
            "Loss: 0.5323328971862793\n",
            "Batch Number: 297\n",
            "Loss: 0.10293927043676376\n",
            "Batch Number: 298\n",
            "Loss: 0.45632344484329224\n",
            "Batch Number: 299\n",
            "Loss: 0.29420310258865356\n",
            "Batch Number: 300\n",
            "Loss: 0.17520421743392944\n",
            "Batch Number: 301\n",
            "Loss: 0.16970179975032806\n",
            "Batch Number: 302\n",
            "Loss: 0.3616872727870941\n",
            "Batch Number: 303\n",
            "Loss: 0.570772111415863\n",
            "Batch Number: 304\n",
            "Loss: 0.12117595970630646\n",
            "Batch Number: 305\n",
            "Loss: 0.19950690865516663\n",
            "Batch Number: 306\n",
            "Loss: 0.6070765852928162\n",
            "Batch Number: 307\n",
            "Loss: 0.36120182275772095\n",
            "Batch Number: 308\n",
            "Loss: 0.43194708228111267\n",
            "Batch Number: 309\n",
            "Loss: 0.15170851349830627\n",
            "Batch Number: 310\n",
            "Loss: 0.543283998966217\n",
            "Batch Number: 311\n",
            "Loss: 0.22214937210083008\n",
            "Batch Number: 312\n",
            "Loss: 0.4939078986644745\n",
            "Batch Number: 313\n",
            "Loss: 0.0840652585029602\n",
            "Batch Number: 314\n",
            "Loss: 0.30173352360725403\n",
            "Batch Number: 315\n",
            "Loss: 0.49627256393432617\n",
            "Batch Number: 316\n",
            "Loss: 0.11560975760221481\n",
            "Batch Number: 317\n",
            "Loss: 0.5967752933502197\n",
            "Batch Number: 318\n",
            "Loss: 0.11698050796985626\n",
            "Batch Number: 319\n",
            "Loss: 0.21020963788032532\n",
            "Batch Number: 320\n",
            "Loss: 0.1435861587524414\n",
            "Batch Number: 321\n",
            "Loss: 0.3030312955379486\n",
            "Batch Number: 322\n",
            "Loss: 0.06191691383719444\n",
            "Batch Number: 323\n",
            "Loss: 0.057854533195495605\n",
            "Batch Number: 324\n",
            "Loss: 0.12118148803710938\n",
            "Batch Number: 325\n",
            "Loss: 0.4215002954006195\n",
            "Batch Number: 326\n",
            "Loss: 0.5454153418540955\n",
            "Batch Number: 327\n",
            "Loss: 0.6704885959625244\n",
            "Batch Number: 328\n",
            "Loss: 0.23829546570777893\n",
            "Batch Number: 329\n",
            "Loss: 0.3133563995361328\n",
            "Batch Number: 330\n",
            "Loss: 0.148217111825943\n",
            "Batch Number: 331\n",
            "Loss: 0.14546267688274384\n",
            "Batch Number: 332\n",
            "Loss: 0.21183383464813232\n",
            "Batch Number: 333\n",
            "Loss: 0.10806740820407867\n",
            "Batch Number: 334\n",
            "Loss: 0.12864913046360016\n",
            "Batch Number: 335\n",
            "Loss: 0.3887850046157837\n",
            "Batch Number: 336\n",
            "Loss: 0.9375562071800232\n",
            "Batch Number: 337\n",
            "Loss: 0.24904321134090424\n",
            "Batch Number: 338\n",
            "Loss: 0.6630454063415527\n",
            "Batch Number: 339\n",
            "Loss: 0.8136245012283325\n",
            "Batch Number: 340\n",
            "Loss: 0.3946661651134491\n",
            "Batch Number: 341\n",
            "Loss: 0.12566924095153809\n",
            "Batch Number: 342\n",
            "Loss: 0.12374065071344376\n",
            "Batch Number: 343\n",
            "Loss: 0.0884987935423851\n",
            "Batch Number: 344\n",
            "Loss: 0.12090754508972168\n",
            "Batch Number: 345\n",
            "Loss: 0.2780408263206482\n",
            "Batch Number: 346\n",
            "Loss: 0.19247578084468842\n",
            "Batch Number: 347\n",
            "Loss: 0.24553902447223663\n",
            "Batch Number: 348\n",
            "Loss: 0.20181484520435333\n",
            "Batch Number: 349\n",
            "Loss: 0.21542072296142578\n",
            "Batch Number: 350\n",
            "Loss: 0.13683488965034485\n",
            "Batch Number: 351\n",
            "Loss: 0.03721882775425911\n",
            "Batch Number: 352\n",
            "Loss: 0.1549626886844635\n",
            "Batch Number: 353\n",
            "Loss: 0.16942672431468964\n",
            "Batch Number: 354\n",
            "Loss: 0.23319435119628906\n",
            "Batch Number: 355\n",
            "Loss: 0.8544888496398926\n",
            "Batch Number: 356\n",
            "Loss: 0.39739546179771423\n",
            "Batch Number: 357\n",
            "Loss: 0.7877318859100342\n",
            "Batch Number: 358\n",
            "Loss: 0.031631771475076675\n",
            "Batch Number: 359\n",
            "Loss: 0.33485084772109985\n",
            "Batch Number: 360\n",
            "Loss: 0.2220165729522705\n",
            "Batch Number: 361\n",
            "Loss: 0.18097858130931854\n",
            "Batch Number: 362\n",
            "Loss: 0.21504059433937073\n",
            "Batch Number: 363\n",
            "Loss: 0.04850670322775841\n",
            "Batch Number: 364\n",
            "Loss: 0.06768321245908737\n",
            "Batch Number: 365\n",
            "Loss: 0.8071025013923645\n",
            "Batch Number: 366\n",
            "Loss: 0.15125195682048798\n",
            "Batch Number: 367\n",
            "Loss: 0.2859080731868744\n",
            "Batch Number: 368\n",
            "Loss: 1.4484678506851196\n",
            "Batch Number: 369\n",
            "Loss: 0.1457718461751938\n",
            "Batch Number: 370\n",
            "Loss: 1.84799325466156\n",
            "Batch Number: 371\n",
            "Loss: 0.16451923549175262\n",
            "Batch Number: 372\n",
            "Loss: 0.23005051910877228\n",
            "Batch Number: 373\n",
            "Loss: 0.5435377955436707\n",
            "Batch Number: 374\n",
            "Loss: 0.08698047697544098\n",
            "Batch Number: 375\n",
            "Loss: 0.55707186460495\n",
            "Batch Number: 376\n",
            "Loss: 0.056879546493291855\n",
            "Batch Number: 377\n",
            "Loss: 0.32264265418052673\n",
            "Batch Number: 378\n",
            "Loss: 0.45915690064430237\n",
            "Batch Number: 379\n",
            "Loss: 0.6106000542640686\n",
            "Batch Number: 380\n",
            "Loss: 0.13175983726978302\n",
            "Batch Number: 381\n",
            "Loss: 0.2850399613380432\n",
            "Batch Number: 382\n",
            "Loss: 0.184270977973938\n",
            "Batch Number: 383\n",
            "Loss: 0.2083641141653061\n",
            "Batch Number: 384\n",
            "Loss: 0.11109983175992966\n",
            "Batch Number: 385\n",
            "Loss: 0.31453937292099\n",
            "Batch Number: 386\n",
            "Loss: 0.3090342581272125\n",
            "Batch Number: 387\n",
            "Loss: 0.3378058075904846\n",
            "Batch Number: 388\n",
            "Loss: 0.25930488109588623\n",
            "Batch Number: 389\n",
            "Loss: 0.26254603266716003\n",
            "Batch Number: 390\n",
            "Loss: 0.13422521948814392\n",
            "Batch Number: 391\n",
            "Loss: 0.14141227304935455\n",
            "Batch Number: 392\n",
            "Loss: 0.20127907395362854\n",
            "Batch Number: 393\n",
            "Loss: 0.11050047725439072\n",
            "Batch Number: 394\n",
            "Loss: 0.17993441224098206\n",
            "Batch Number: 395\n",
            "Loss: 0.4133375585079193\n",
            "Batch Number: 396\n",
            "Loss: 0.09316658973693848\n",
            "Batch Number: 397\n",
            "Loss: 0.2633233666419983\n",
            "Batch Number: 398\n",
            "Loss: 0.27957698702812195\n",
            "Batch Number: 399\n",
            "Loss: 1.308648943901062\n",
            "Batch Number: 400\n",
            "Loss: 0.4471322000026703\n",
            "Batch Number: 401\n",
            "Loss: 0.19782708585262299\n",
            "Batch Number: 402\n",
            "Loss: 0.31270885467529297\n",
            "Batch Number: 403\n",
            "Loss: 0.4351511001586914\n",
            "Batch Number: 404\n",
            "Loss: 0.3169320225715637\n",
            "Batch Number: 405\n",
            "Loss: 0.521609902381897\n",
            "Batch Number: 406\n",
            "Loss: 0.20294205844402313\n",
            "Batch Number: 407\n",
            "Loss: 0.6901694536209106\n",
            "Batch Number: 408\n",
            "Loss: 0.508120059967041\n",
            "Batch Number: 409\n",
            "Loss: 0.08793693035840988\n",
            "Batch Number: 410\n",
            "Loss: 0.36950358748435974\n",
            "Batch Number: 411\n",
            "Loss: 0.2189261019229889\n",
            "Batch Number: 412\n",
            "Loss: 0.20505861937999725\n",
            "Batch Number: 413\n",
            "Loss: 0.09241451323032379\n",
            "Batch Number: 414\n",
            "Loss: 0.6103495955467224\n",
            "Batch Number: 415\n",
            "Loss: 0.16082890331745148\n",
            "Batch Number: 416\n",
            "Loss: 0.18608467280864716\n",
            "Batch Number: 417\n",
            "Loss: 0.6327563524246216\n",
            "Batch Number: 418\n",
            "Loss: 0.3800448179244995\n",
            "Batch Number: 419\n",
            "Loss: 0.4470252990722656\n",
            "Batch Number: 420\n",
            "Loss: 0.5331869125366211\n",
            "Batch Number: 421\n",
            "Loss: 0.31666845083236694\n",
            "Batch Number: 422\n",
            "Loss: 0.35327157378196716\n",
            "Batch Number: 423\n",
            "Loss: 0.30982646346092224\n",
            "Batch Number: 424\n",
            "Loss: 0.11101620644330978\n",
            "Batch Number: 425\n",
            "Loss: 0.8402165770530701\n",
            "Batch Number: 426\n",
            "Loss: 0.5974543690681458\n",
            "Batch Number: 427\n",
            "Loss: 0.16828233003616333\n",
            "Batch Number: 428\n",
            "Loss: 0.044421274214982986\n",
            "Batch Number: 429\n",
            "Loss: 0.0937563106417656\n",
            "Batch Number: 430\n",
            "Loss: 0.2590879201889038\n",
            "Batch Number: 431\n",
            "Loss: 0.2512767016887665\n",
            "Batch Number: 432\n",
            "Loss: 0.5677191019058228\n",
            "Batch Number: 433\n",
            "Loss: 0.295084148645401\n",
            "Batch Number: 434\n",
            "Loss: 1.1917839050292969\n",
            "Batch Number: 435\n",
            "Loss: 0.11602697521448135\n",
            "Batch Number: 436\n",
            "Loss: 0.14760223031044006\n",
            "Batch Number: 437\n",
            "Loss: 0.302702397108078\n",
            "Batch Number: 438\n",
            "Loss: 0.12697963416576385\n",
            "Batch Number: 439\n",
            "Loss: 0.17069236934185028\n",
            "Batch Number: 440\n",
            "Loss: 0.15479497611522675\n",
            "Batch Number: 441\n",
            "Loss: 0.08077389001846313\n",
            "Batch Number: 442\n",
            "Loss: 0.2720680236816406\n",
            "Batch Number: 443\n",
            "Loss: 0.15871061384677887\n",
            "Batch Number: 444\n",
            "Loss: 0.12287751585245132\n",
            "Batch Number: 445\n",
            "Loss: 0.15038810670375824\n",
            "Batch Number: 446\n",
            "Loss: 0.1531905233860016\n",
            "Batch Number: 447\n",
            "Loss: 0.35883280634880066\n",
            "Batch Number: 448\n",
            "Loss: 0.18056397140026093\n",
            "Batch Number: 449\n",
            "Loss: 0.1450652927160263\n",
            "Batch Number: 450\n",
            "Loss: 0.3258092403411865\n",
            "Batch Number: 451\n",
            "Loss: 0.17528335750102997\n",
            "Batch Number: 452\n",
            "Loss: 0.23849640786647797\n",
            "Batch Number: 453\n",
            "Loss: 0.19009040296077728\n",
            "Batch Number: 454\n",
            "Loss: 0.06335163116455078\n",
            "Batch Number: 455\n",
            "Loss: 0.03690123185515404\n",
            "Batch Number: 456\n",
            "Loss: 0.4959551990032196\n",
            "Batch Number: 457\n",
            "Loss: 0.13329647481441498\n",
            "Batch Number: 458\n",
            "Loss: 0.06527187675237656\n",
            "Batch Number: 459\n",
            "Loss: 0.33974549174308777\n",
            "Batch Number: 460\n",
            "Loss: 0.3500511646270752\n",
            "Batch Number: 461\n",
            "Loss: 0.22573792934417725\n",
            "Batch Number: 462\n",
            "Loss: 0.15890586376190186\n",
            "Batch Number: 463\n",
            "Loss: 0.10469191521406174\n",
            "Batch Number: 464\n",
            "Loss: 0.19436681270599365\n",
            "Batch Number: 465\n",
            "Loss: 0.5950469374656677\n",
            "Batch Number: 466\n",
            "Loss: 0.17791616916656494\n",
            "Batch Number: 467\n",
            "Loss: 0.19296778738498688\n",
            "Batch Number: 468\n",
            "Loss: 0.8464692234992981\n",
            "Batch Number: 469\n",
            "Loss: 0.15680518746376038\n",
            "Batch Number: 470\n",
            "Loss: 0.38749250769615173\n",
            "Batch Number: 471\n",
            "Loss: 0.10786475986242294\n",
            "Batch Number: 472\n",
            "Loss: 0.1368379443883896\n",
            "Batch Number: 473\n",
            "Loss: 0.6782587170600891\n",
            "Batch Number: 474\n",
            "Loss: 0.7621335387229919\n",
            "Batch Number: 475\n",
            "Loss: 0.12825219333171844\n",
            "Batch Number: 476\n",
            "Loss: 0.40114060044288635\n",
            "Batch Number: 477\n",
            "Loss: 0.1588273048400879\n",
            "Batch Number: 478\n",
            "Loss: 0.2643902897834778\n",
            "Batch Number: 479\n",
            "Loss: 0.10682155191898346\n",
            "Batch Number: 480\n",
            "Loss: 0.19932670891284943\n",
            "Batch Number: 481\n",
            "Loss: 0.3227543532848358\n",
            "Batch Number: 482\n",
            "Loss: 0.10220277309417725\n",
            "Batch Number: 483\n",
            "Loss: 0.15166455507278442\n",
            "Batch Number: 484\n",
            "Loss: 0.9313506484031677\n",
            "Batch Number: 485\n",
            "Loss: 0.5441162586212158\n",
            "Batch Number: 486\n",
            "Loss: 0.14057597517967224\n",
            "Batch Number: 487\n",
            "Loss: 0.09681838750839233\n",
            "Batch Number: 488\n",
            "Loss: 0.18793687224388123\n",
            "Batch Number: 489\n",
            "Loss: 0.2148076295852661\n",
            "Batch Number: 490\n",
            "Loss: 0.7946032881736755\n",
            "Batch Number: 491\n",
            "Loss: 0.3065536618232727\n",
            "Batch Number: 492\n",
            "Loss: 0.4418604373931885\n",
            "Batch Number: 493\n",
            "Loss: 0.2846713960170746\n",
            "Batch Number: 494\n",
            "Loss: 0.12377327680587769\n",
            "Batch Number: 495\n",
            "Loss: 0.42152053117752075\n",
            "Batch Number: 496\n",
            "Loss: 0.3360947370529175\n",
            "Batch Number: 497\n",
            "Loss: 0.16550903022289276\n",
            "Batch Number: 498\n",
            "Loss: 0.20296840369701385\n",
            "Batch Number: 499\n",
            "Loss: 0.2660054862499237\n",
            "Batch Number: 500\n",
            "Loss: 0.7560039162635803\n",
            "Batch Number: 501\n",
            "Loss: 0.4024617671966553\n",
            "Batch Number: 502\n",
            "Loss: 0.08719059824943542\n",
            "Batch Number: 503\n",
            "Loss: 0.2072523832321167\n",
            "Batch Number: 504\n",
            "Loss: 0.17831653356552124\n",
            "Batch Number: 505\n",
            "Loss: 0.44710221886634827\n",
            "Batch Number: 506\n",
            "Loss: 0.3696099817752838\n",
            "Batch Number: 507\n",
            "Loss: 0.28266286849975586\n",
            "Batch Number: 508\n",
            "Loss: 0.23625536262989044\n",
            "Batch Number: 509\n",
            "Loss: 0.15876533091068268\n",
            "Batch Number: 510\n",
            "Loss: 0.22180962562561035\n",
            "Batch Number: 511\n",
            "Loss: 0.1916661411523819\n",
            "Batch Number: 512\n",
            "Loss: 0.3037758767604828\n",
            "Batch Number: 513\n",
            "Loss: 0.37157759070396423\n",
            "Batch Number: 514\n",
            "Loss: 0.09668844193220139\n",
            "Batch Number: 515\n",
            "Loss: 0.18419726192951202\n",
            "Batch Number: 516\n",
            "Loss: 0.347048282623291\n",
            "Batch Number: 517\n",
            "Loss: 0.15848056972026825\n",
            "Batch Number: 518\n",
            "Loss: 0.2717892825603485\n",
            "Batch Number: 519\n",
            "Loss: 0.22845879197120667\n",
            "Batch Number: 520\n",
            "Loss: 0.3100205361843109\n",
            "Batch Number: 521\n",
            "Loss: 0.11960716545581818\n",
            "Batch Number: 522\n",
            "Loss: 0.19092874228954315\n",
            "Batch Number: 523\n",
            "Loss: 0.0751759335398674\n",
            "Batch Number: 524\n",
            "Loss: 0.5775799751281738\n",
            "Batch Number: 525\n",
            "Loss: 0.2953925132751465\n",
            "Batch Number: 526\n",
            "Loss: 0.19054631888866425\n",
            "Batch Number: 527\n",
            "Loss: 0.23043255507946014\n",
            "Batch Number: 528\n",
            "Loss: 0.9336754083633423\n",
            "Batch Number: 529\n",
            "Loss: 0.5428999662399292\n",
            "Batch Number: 530\n",
            "Loss: 0.3084735870361328\n",
            "Batch Number: 531\n",
            "Loss: 0.23410367965698242\n",
            "Batch Number: 532\n",
            "Loss: 0.18708379566669464\n",
            "Batch Number: 533\n",
            "Loss: 0.23743195831775665\n",
            "Batch Number: 534\n",
            "Loss: 0.20107212662696838\n",
            "Batch Number: 535\n",
            "Loss: 0.25518274307250977\n",
            "Batch Number: 536\n",
            "Loss: 0.39601942896842957\n",
            "Batch Number: 537\n",
            "Loss: 0.4662068784236908\n",
            "Batch Number: 538\n",
            "Loss: 0.5929408669471741\n",
            "Batch Number: 539\n",
            "Loss: 0.08615996688604355\n",
            "Batch Number: 540\n",
            "Loss: 0.464083731174469\n",
            "Batch Number: 541\n",
            "Loss: 0.42812228202819824\n",
            "Batch Number: 542\n",
            "Loss: 0.0775914192199707\n",
            "Batch Number: 543\n",
            "Loss: 0.19497446715831757\n",
            "Batch Number: 544\n",
            "Loss: 0.40005770325660706\n",
            "Batch Number: 545\n",
            "Loss: 0.4182611405849457\n",
            "Batch Number: 546\n",
            "Loss: 0.2600303590297699\n",
            "Batch Number: 547\n",
            "Loss: 0.12654924392700195\n",
            "Batch Number: 548\n",
            "Loss: 0.13961546123027802\n",
            "Batch Number: 549\n",
            "Loss: 0.27108269929885864\n",
            "Batch Number: 550\n",
            "Loss: 0.2616046071052551\n",
            "Batch Number: 551\n",
            "Loss: 0.07130139321088791\n",
            "Batch Number: 552\n",
            "Loss: 0.45604410767555237\n",
            "Batch Number: 553\n",
            "Loss: 0.09273425489664078\n",
            "Batch Number: 554\n",
            "Loss: 0.296487033367157\n",
            "Batch Number: 555\n",
            "Loss: 0.2710023820400238\n",
            "Batch Number: 556\n",
            "Loss: 0.48850709199905396\n",
            "Batch Number: 557\n",
            "Loss: 0.054277073591947556\n",
            "Batch Number: 558\n",
            "Loss: 0.17027446627616882\n",
            "Batch Number: 559\n",
            "Loss: 0.509036660194397\n",
            "Batch Number: 560\n",
            "Loss: 0.5105956792831421\n",
            "Batch Number: 561\n",
            "Loss: 0.34320303797721863\n",
            "Batch Number: 562\n",
            "Loss: 0.0865633636713028\n",
            "Batch Number: 563\n",
            "Loss: 0.5483722686767578\n",
            "Batch Number: 564\n",
            "Loss: 0.37164661288261414\n",
            "Batch Number: 565\n",
            "Loss: 0.287823349237442\n",
            "Batch Number: 566\n",
            "Loss: 0.10497289896011353\n",
            "Batch Number: 567\n",
            "Loss: 0.23840928077697754\n",
            "Batch Number: 568\n",
            "Loss: 0.6613450646400452\n",
            "Batch Number: 569\n",
            "Loss: 0.15322847664356232\n",
            "Batch Number: 570\n",
            "Loss: 0.5286656022071838\n",
            "Batch Number: 571\n",
            "Loss: 0.13973964750766754\n",
            "Batch Number: 572\n",
            "Loss: 0.1596662849187851\n",
            "Batch Number: 573\n",
            "Loss: 0.22126661241054535\n",
            "Batch Number: 574\n",
            "Loss: 0.11639123409986496\n",
            "Batch Number: 575\n",
            "Loss: 0.07741767168045044\n",
            "Batch Number: 576\n",
            "Loss: 0.5468589067459106\n",
            "Batch Number: 577\n",
            "Loss: 0.4023585915565491\n",
            "Batch Number: 578\n",
            "Loss: 0.11894774436950684\n",
            "Batch Number: 579\n",
            "Loss: 0.238091379404068\n",
            "Batch Number: 580\n",
            "Loss: 0.1834522932767868\n",
            "Batch Number: 581\n",
            "Loss: 0.2752231955528259\n",
            "Batch Number: 582\n",
            "Loss: 0.08896499127149582\n",
            "Batch Number: 583\n",
            "Loss: 0.860944926738739\n",
            "Batch Number: 584\n",
            "Loss: 0.8940777778625488\n",
            "Batch Number: 585\n",
            "Loss: 0.2053391933441162\n",
            "Batch Number: 586\n",
            "Loss: 0.17542488873004913\n",
            "Batch Number: 587\n",
            "Loss: 0.24701522290706635\n",
            "Batch Number: 588\n",
            "Loss: 0.2525714337825775\n",
            "Batch Number: 589\n",
            "Loss: 0.21926084160804749\n",
            "Batch Number: 590\n",
            "Loss: 0.8638491630554199\n",
            "Batch Number: 591\n",
            "Loss: 0.34916064143180847\n",
            "Batch Number: 592\n",
            "Loss: 0.412248432636261\n",
            "Batch Number: 593\n",
            "Loss: 0.1949271261692047\n",
            "Batch Number: 594\n",
            "Loss: 0.5291388034820557\n",
            "Batch Number: 595\n",
            "Loss: 0.29862305521965027\n",
            "Batch Number: 596\n",
            "Loss: 0.18917857110500336\n",
            "Batch Number: 597\n",
            "Loss: 0.37009474635124207\n",
            "Batch Number: 598\n",
            "Loss: 0.17365241050720215\n",
            "Batch Number: 599\n",
            "Loss: 0.4390999376773834\n",
            "Batch Number: 600\n",
            "Loss: 0.24197259545326233\n",
            "Batch Number: 601\n",
            "Loss: 0.3857283294200897\n",
            "Batch Number: 602\n",
            "Loss: 0.36374711990356445\n",
            "Batch Number: 603\n",
            "Loss: 0.31131258606910706\n",
            "Batch Number: 604\n",
            "Loss: 0.33716967701911926\n",
            "Batch Number: 605\n",
            "Loss: 0.49845290184020996\n",
            "Batch Number: 606\n",
            "Loss: 0.04367196559906006\n",
            "Batch Number: 607\n",
            "Loss: 0.03915462642908096\n",
            "Batch Number: 608\n",
            "Loss: 0.3820600211620331\n",
            "Batch Number: 609\n",
            "Loss: 0.05292782932519913\n",
            "Batch Number: 610\n",
            "Loss: 0.233279749751091\n",
            "Batch Number: 611\n",
            "Loss: 0.3014422357082367\n",
            "Batch Number: 612\n",
            "Loss: 0.15979771316051483\n",
            "Batch Number: 613\n",
            "Loss: 0.08877282589673996\n",
            "Batch Number: 614\n",
            "Loss: 0.10013376921415329\n",
            "Batch Number: 615\n",
            "Loss: 0.04606601223349571\n",
            "Batch Number: 616\n",
            "Loss: 0.24344997107982635\n",
            "Batch Number: 617\n",
            "Loss: 0.8169804811477661\n",
            "Batch Number: 618\n",
            "Loss: 0.31713488698005676\n",
            "Batch Number: 619\n",
            "Loss: 0.28699398040771484\n",
            "Batch Number: 620\n",
            "Loss: 0.2404564470052719\n",
            "Batch Number: 621\n",
            "Loss: 0.21570058166980743\n",
            "Batch Number: 622\n",
            "Loss: 0.19208423793315887\n",
            "Batch Number: 623\n",
            "Loss: 0.3900158405303955\n",
            "Batch Number: 624\n",
            "Loss: 0.14905166625976562\n",
            "Batch Number: 625\n",
            "Loss: 0.5771171450614929\n",
            "Batch Number: 626\n",
            "Loss: 0.46964141726493835\n",
            "Batch Number: 627\n",
            "Loss: 0.26806172728538513\n",
            "Batch Number: 628\n",
            "Loss: 0.1507011502981186\n",
            "Batch Number: 629\n",
            "Loss: 0.5549700856208801\n",
            "Batch Number: 630\n",
            "Loss: 0.41482654213905334\n",
            "Batch Number: 631\n",
            "Loss: 0.49702009558677673\n",
            "Batch Number: 632\n",
            "Loss: 0.32072916626930237\n",
            "Batch Number: 633\n",
            "Loss: 0.26140937209129333\n",
            "Batch Number: 634\n",
            "Loss: 0.27289193868637085\n",
            "Batch Number: 635\n",
            "Loss: 0.42031440138816833\n",
            "Batch Number: 636\n",
            "Loss: 0.15665952861309052\n",
            "Batch Number: 637\n",
            "Loss: 0.09414099901914597\n",
            "Batch Number: 638\n",
            "Loss: 0.20927441120147705\n",
            "Batch Number: 639\n",
            "Loss: 0.43576526641845703\n",
            "Batch Number: 640\n",
            "Loss: 0.28503701090812683\n",
            "Batch Number: 641\n",
            "Loss: 0.06922125816345215\n",
            "Batch Number: 642\n",
            "Loss: 0.8098526000976562\n",
            "Batch Number: 643\n",
            "Loss: 0.8501443266868591\n",
            "Batch Number: 644\n",
            "Loss: 0.2839019000530243\n",
            "Batch Number: 645\n",
            "Loss: 0.15306782722473145\n",
            "Batch Number: 646\n",
            "Loss: 0.8303738832473755\n",
            "Batch Number: 647\n",
            "Loss: 0.21617424488067627\n",
            "Batch Number: 648\n",
            "Loss: 0.26037895679473877\n",
            "Batch Number: 649\n",
            "Loss: 0.24580851197242737\n",
            "Batch Number: 650\n",
            "Loss: 0.3881077170372009\n",
            "Batch Number: 651\n",
            "Loss: 0.068011574447155\n",
            "Batch Number: 652\n",
            "Loss: 0.15973196923732758\n",
            "Batch Number: 653\n",
            "Loss: 1.323853850364685\n",
            "Batch Number: 654\n",
            "Loss: 0.9333118796348572\n",
            "Batch Number: 655\n",
            "Loss: 0.5073748826980591\n",
            "Batch Number: 656\n",
            "Loss: 0.6646062731742859\n",
            "Batch Number: 657\n",
            "Loss: 0.060773517936468124\n",
            "Batch Number: 658\n",
            "Loss: 0.29084524512290955\n",
            "Batch Number: 659\n",
            "Loss: 0.13623785972595215\n",
            "Batch Number: 660\n",
            "Loss: 0.22352610528469086\n",
            "Batch Number: 661\n",
            "Loss: 0.3864358067512512\n",
            "Batch Number: 662\n",
            "Loss: 0.5722508430480957\n",
            "Batch Number: 663\n",
            "Loss: 0.2546974718570709\n",
            "Batch Number: 664\n",
            "Loss: 0.13464172184467316\n",
            "Batch Number: 665\n",
            "Loss: 0.09627493470907211\n",
            "Batch Number: 666\n",
            "Loss: 0.07893671840429306\n",
            "Batch Number: 667\n",
            "Loss: 0.19426527619361877\n",
            "Batch Number: 668\n",
            "Loss: 0.0882263109087944\n",
            "Batch Number: 669\n",
            "Loss: 0.19594447314739227\n",
            "Batch Number: 670\n",
            "Loss: 0.13629627227783203\n",
            "Batch Number: 671\n",
            "Loss: 0.20425383746623993\n",
            "Batch Number: 672\n",
            "Loss: 0.18696606159210205\n",
            "Batch Number: 673\n",
            "Loss: 0.3466806411743164\n",
            "Batch Number: 674\n",
            "Loss: 0.19719675183296204\n",
            "Batch Number: 675\n",
            "Loss: 0.28303632140159607\n",
            "Batch Number: 676\n",
            "Loss: 0.2187235802412033\n",
            "Batch Number: 677\n",
            "Loss: 0.24289195239543915\n",
            "Batch Number: 678\n",
            "Loss: 0.26657652854919434\n",
            "Batch Number: 679\n",
            "Loss: 0.08728822320699692\n",
            "Batch Number: 680\n",
            "Loss: 0.16496789455413818\n",
            "Batch Number: 681\n",
            "Loss: 0.2445220947265625\n",
            "Batch Number: 682\n",
            "Loss: 0.17014078795909882\n",
            "Batch Number: 683\n",
            "Loss: 0.19588696956634521\n",
            "Batch Number: 684\n",
            "Loss: 0.08008725941181183\n",
            "Batch Number: 685\n",
            "Loss: 0.07850971072912216\n",
            "Batch Number: 686\n",
            "Loss: 0.24663305282592773\n",
            "Batch Number: 687\n",
            "Loss: 0.4527941644191742\n",
            "Batch Number: 688\n",
            "Loss: 0.09503010660409927\n",
            "Batch Number: 689\n",
            "Loss: 0.08837401866912842\n",
            "Batch Number: 690\n",
            "Loss: 0.18730199337005615\n",
            "Batch Number: 691\n",
            "Loss: 0.09933996200561523\n",
            "Batch Number: 692\n",
            "Loss: 0.3430345952510834\n",
            "Batch Number: 693\n",
            "Loss: 0.15308797359466553\n",
            "Batch Number: 694\n",
            "Loss: 0.24121348559856415\n",
            "Batch Number: 695\n",
            "Loss: 0.11629978567361832\n",
            "Batch Number: 696\n",
            "Loss: 0.32256948947906494\n",
            "Batch Number: 697\n",
            "Loss: 0.18214403092861176\n",
            "Batch Number: 698\n",
            "Loss: 0.19550395011901855\n",
            "Batch Number: 699\n",
            "Loss: 0.4899648129940033\n",
            "Batch Number: 700\n",
            "Loss: 0.15175262093544006\n",
            "Batch Number: 701\n",
            "Loss: 0.8131856322288513\n",
            "Batch Number: 702\n",
            "Loss: 0.16827556490898132\n",
            "Batch Number: 703\n",
            "Loss: 0.06409566849470139\n",
            "Batch Number: 704\n",
            "Loss: 0.5804268717765808\n",
            "Batch Number: 705\n",
            "Loss: 0.2857162654399872\n",
            "Batch Number: 706\n",
            "Loss: 0.08885063230991364\n",
            "Batch Number: 707\n",
            "Loss: 0.2568073272705078\n",
            "Batch Number: 708\n",
            "Loss: 0.32152217626571655\n",
            "Batch Number: 709\n",
            "Loss: 0.06835225969552994\n",
            "Batch Number: 710\n",
            "Loss: 0.23309651017189026\n",
            "Batch Number: 711\n",
            "Loss: 0.15842276811599731\n",
            "Batch Number: 712\n",
            "Loss: 0.0719958245754242\n",
            "Batch Number: 713\n",
            "Loss: 0.11057527363300323\n",
            "Batch Number: 714\n",
            "Loss: 0.08155059069395065\n",
            "Batch Number: 715\n",
            "Loss: 0.16166892647743225\n",
            "Batch Number: 716\n",
            "Loss: 0.2303561419248581\n",
            "Batch Number: 717\n",
            "Loss: 0.22245685756206512\n",
            "Batch Number: 718\n",
            "Loss: 0.18233537673950195\n",
            "Batch Number: 719\n",
            "Loss: 0.1170140728354454\n",
            "Batch Number: 720\n",
            "Loss: 0.4042772352695465\n",
            "Batch Number: 721\n",
            "Loss: 0.07151588052511215\n",
            "Batch Number: 722\n",
            "Loss: 0.46345630288124084\n",
            "Batch Number: 723\n",
            "Loss: 0.2585671842098236\n",
            "Batch Number: 724\n",
            "Loss: 0.18072573840618134\n",
            "Batch Number: 725\n",
            "Loss: 0.46613645553588867\n",
            "Batch Number: 726\n",
            "Loss: 0.20602171123027802\n",
            "Batch Number: 727\n",
            "Loss: 0.5004999041557312\n",
            "Batch Number: 728\n",
            "Loss: 0.11990880966186523\n",
            "Batch Number: 729\n",
            "Loss: 0.11923708766698837\n",
            "Batch Number: 730\n",
            "Loss: 0.217631533741951\n",
            "Batch Number: 731\n",
            "Loss: 0.03278191760182381\n",
            "Batch Number: 732\n",
            "Loss: 0.4723888337612152\n",
            "Batch Number: 733\n",
            "Loss: 1.1655147075653076\n",
            "Batch Number: 734\n",
            "Loss: 0.6438540816307068\n",
            "Batch Number: 735\n",
            "Loss: 0.11697705090045929\n",
            "Batch Number: 736\n",
            "Loss: 0.17804419994354248\n",
            "Batch Number: 737\n",
            "Loss: 0.6610335111618042\n",
            "Batch Number: 738\n",
            "Loss: 0.5028833746910095\n",
            "Batch Number: 739\n",
            "Loss: 0.213955357670784\n",
            "Batch Number: 740\n",
            "Loss: 0.22558951377868652\n",
            "Batch Number: 741\n",
            "Loss: 0.11715944856405258\n",
            "Batch Number: 742\n",
            "Loss: 0.30370330810546875\n",
            "Batch Number: 743\n",
            "Loss: 0.1646420806646347\n",
            "Batch Number: 744\n",
            "Loss: 0.23747482895851135\n",
            "Batch Number: 745\n",
            "Loss: 0.6990545988082886\n",
            "Batch Number: 746\n",
            "Loss: 0.5840580463409424\n",
            "Batch Number: 747\n",
            "Loss: 0.23206643760204315\n",
            "Batch Number: 748\n",
            "Loss: 0.22824595868587494\n",
            "Batch Number: 749\n",
            "Loss: 0.3280852735042572\n",
            "Batch Number: 750\n",
            "Loss: 0.10143037140369415\n",
            "Batch Number: 751\n",
            "Loss: 0.40687069296836853\n",
            "Batch Number: 752\n",
            "Loss: 0.9483299255371094\n",
            "Batch Number: 753\n",
            "Loss: 0.8396248817443848\n",
            "Batch Number: 754\n",
            "Loss: 0.20648665726184845\n",
            "Batch Number: 755\n",
            "Loss: 0.16183769702911377\n",
            "Batch Number: 756\n",
            "Loss: 0.03513948991894722\n",
            "Batch Number: 757\n",
            "Loss: 0.2350929230451584\n",
            "Batch Number: 758\n",
            "Loss: 0.6241764426231384\n",
            "Batch Number: 759\n",
            "Loss: 0.07635819911956787\n",
            "Batch Number: 760\n",
            "Loss: 0.22086215019226074\n",
            "Batch Number: 761\n",
            "Loss: 0.3645154535770416\n",
            "Batch Number: 762\n",
            "Loss: 0.32018613815307617\n",
            "Batch Number: 763\n",
            "Loss: 0.35156169533729553\n",
            "Batch Number: 764\n",
            "Loss: 0.543765127658844\n",
            "Batch Number: 765\n",
            "Loss: 0.6020454168319702\n",
            "Batch Number: 766\n",
            "Loss: 0.7537927627563477\n",
            "Batch Number: 767\n",
            "Loss: 0.395433634519577\n",
            "Batch Number: 768\n",
            "Loss: 0.27770039439201355\n",
            "Batch Number: 769\n",
            "Loss: 0.13296975195407867\n",
            "Batch Number: 770\n",
            "Loss: 0.45213016867637634\n",
            "Batch Number: 771\n",
            "Loss: 0.44936472177505493\n",
            "Batch Number: 772\n",
            "Loss: 0.19313663244247437\n",
            "Batch Number: 773\n",
            "Loss: 0.11622798442840576\n",
            "Batch Number: 774\n",
            "Loss: 0.4136747419834137\n",
            "Batch Number: 775\n",
            "Loss: 0.3536636531352997\n",
            "Batch Number: 776\n",
            "Loss: 0.6171495914459229\n",
            "Batch Number: 777\n",
            "Loss: 0.3652038276195526\n",
            "Batch Number: 778\n",
            "Loss: 0.5520073175430298\n",
            "Batch Number: 779\n",
            "Loss: 0.2742908298969269\n",
            "Batch Number: 780\n",
            "Loss: 0.10313288122415543\n",
            "Batch Number: 781\n",
            "Loss: 0.2028127759695053\n",
            "Batch Number: 782\n",
            "Loss: 0.17213991284370422\n",
            "Batch Number: 783\n",
            "Loss: 0.12863241136074066\n",
            "Batch Number: 784\n",
            "Loss: 0.44773396849632263\n",
            "Batch Number: 785\n",
            "Loss: 0.3485604226589203\n",
            "Batch Number: 786\n",
            "Loss: 0.23791085183620453\n",
            "Batch Number: 787\n",
            "Loss: 0.1415414810180664\n",
            "Batch Number: 788\n",
            "Loss: 0.1502256542444229\n",
            "Batch Number: 789\n",
            "Loss: 0.40154871344566345\n",
            "Batch Number: 790\n",
            "Loss: 0.0678449496626854\n",
            "Batch Number: 791\n",
            "Loss: 0.22905492782592773\n",
            "Batch Number: 792\n",
            "Loss: 0.5204280018806458\n",
            "Batch Number: 793\n",
            "Loss: 0.3340906500816345\n",
            "Batch Number: 794\n",
            "Loss: 0.264136403799057\n",
            "Batch Number: 795\n",
            "Loss: 0.23379682004451752\n",
            "Batch Number: 796\n",
            "Loss: 0.22399497032165527\n",
            "Batch Number: 797\n",
            "Loss: 0.2926728427410126\n",
            "Batch Number: 798\n",
            "Loss: 0.263272225856781\n",
            "Batch Number: 799\n",
            "Loss: 0.2822069227695465\n",
            "Batch Number: 800\n",
            "Loss: 0.37825918197631836\n",
            "Batch Number: 801\n",
            "Loss: 0.15261781215667725\n",
            "Batch Number: 802\n",
            "Loss: 0.39904260635375977\n",
            "Batch Number: 803\n",
            "Loss: 0.1539948433637619\n",
            "Batch Number: 804\n",
            "Loss: 0.5947038531303406\n",
            "Batch Number: 805\n",
            "Loss: 0.29254674911499023\n",
            "Batch Number: 806\n",
            "Loss: 0.23633995652198792\n",
            "Batch Number: 807\n",
            "Loss: 0.7358385920524597\n",
            "Batch Number: 808\n",
            "Loss: 0.541268527507782\n",
            "Batch Number: 809\n",
            "Loss: 0.7302339673042297\n",
            "Batch Number: 810\n",
            "Loss: 0.2722812294960022\n",
            "Batch Number: 811\n",
            "Loss: 0.14587338268756866\n",
            "Batch Number: 812\n",
            "Loss: 0.17023859918117523\n",
            "Batch Number: 813\n",
            "Loss: 0.3275929391384125\n",
            "Batch Number: 814\n",
            "Loss: 0.13257095217704773\n",
            "Batch Number: 815\n",
            "Loss: 0.21560366451740265\n",
            "Batch Number: 816\n",
            "Loss: 0.24454255402088165\n",
            "Batch Number: 817\n",
            "Loss: 0.12029045075178146\n",
            "Batch Number: 818\n",
            "Loss: 0.1967228800058365\n",
            "Batch Number: 819\n",
            "Loss: 0.25433751940727234\n",
            "Batch Number: 820\n",
            "Loss: 0.11372127383947372\n",
            "Batch Number: 821\n",
            "Loss: 0.22685424983501434\n",
            "Batch Number: 822\n",
            "Loss: 0.2181750386953354\n",
            "Batch Number: 823\n",
            "Loss: 0.1345670223236084\n",
            "Batch Number: 824\n",
            "Loss: 0.07211821526288986\n",
            "Batch Number: 825\n",
            "Loss: 0.2879917025566101\n",
            "Batch Number: 826\n",
            "Loss: 0.13479216396808624\n",
            "Batch Number: 827\n",
            "Loss: 0.26486942172050476\n",
            "Batch Number: 828\n",
            "Loss: 0.2944154441356659\n",
            "Batch Number: 829\n",
            "Loss: 0.02407091110944748\n",
            "Batch Number: 830\n",
            "Loss: 0.16688838601112366\n",
            "Batch Number: 831\n",
            "Loss: 0.22827044129371643\n",
            "Batch Number: 832\n",
            "Loss: 0.1707281768321991\n",
            "Batch Number: 833\n",
            "Loss: 0.14751267433166504\n",
            "Batch Number: 834\n",
            "Loss: 0.6777216196060181\n",
            "Batch Number: 835\n",
            "Loss: 0.10558052361011505\n",
            "Batch Number: 836\n",
            "Loss: 0.33734557032585144\n",
            "Batch Number: 837\n",
            "Loss: 0.3508390486240387\n",
            "Batch Number: 838\n",
            "Loss: 0.3682621419429779\n",
            "Batch Number: 839\n",
            "Loss: 0.11557632684707642\n",
            "Batch Number: 840\n",
            "Loss: 0.5389960408210754\n",
            "Batch Number: 841\n",
            "Loss: 0.26197972893714905\n",
            "Batch Number: 842\n",
            "Loss: 0.04308280348777771\n",
            "Batch Number: 843\n",
            "Loss: 0.12329532206058502\n",
            "Batch Number: 844\n",
            "Loss: 0.10096319764852524\n",
            "Batch Number: 845\n",
            "Loss: 0.37039607763290405\n",
            "Batch Number: 846\n",
            "Loss: 0.5801678895950317\n",
            "Batch Number: 847\n",
            "Loss: 0.5920581817626953\n",
            "Batch Number: 848\n",
            "Loss: 0.06916327774524689\n",
            "Batch Number: 849\n",
            "Loss: 0.2016465663909912\n",
            "Batch Number: 850\n",
            "Loss: 0.5397889018058777\n",
            "Batch Number: 851\n",
            "Loss: 0.08355889469385147\n",
            "Batch Number: 852\n",
            "Loss: 0.5447317957878113\n",
            "Batch Number: 853\n",
            "Loss: 0.09961452335119247\n",
            "Batch Number: 854\n",
            "Loss: 0.600113034248352\n",
            "Batch Number: 855\n",
            "Loss: 0.04736032709479332\n",
            "Batch Number: 856\n",
            "Loss: 0.24332177639007568\n",
            "Batch Number: 857\n",
            "Loss: 0.7037269473075867\n",
            "Batch Number: 858\n",
            "Loss: 0.24190537631511688\n",
            "Batch Number: 859\n",
            "Loss: 0.29009905457496643\n",
            "Batch Number: 860\n",
            "Loss: 0.5709788799285889\n",
            "Batch Number: 861\n",
            "Loss: 0.5191158652305603\n",
            "Batch Number: 862\n",
            "Loss: 0.055912502110004425\n",
            "Batch Number: 863\n",
            "Loss: 0.4501596987247467\n",
            "Batch Number: 864\n",
            "Loss: 0.6215847730636597\n",
            "Batch Number: 865\n",
            "Loss: 0.2661629319190979\n",
            "Batch Number: 866\n",
            "Loss: 0.2379646748304367\n",
            "Batch Number: 867\n",
            "Loss: 0.5213077664375305\n",
            "Batch Number: 868\n",
            "Loss: 0.26855722069740295\n",
            "Batch Number: 869\n",
            "Loss: 0.19574980437755585\n",
            "Batch Number: 870\n",
            "Loss: 0.22154788672924042\n",
            "Batch Number: 871\n",
            "Loss: 0.3754110336303711\n",
            "Batch Number: 872\n",
            "Loss: 0.21017234027385712\n",
            "Batch Number: 873\n",
            "Loss: 0.0683036670088768\n",
            "Batch Number: 874\n",
            "Loss: 0.26286396384239197\n",
            "Batch Number: 875\n",
            "Loss: 0.3975307047367096\n",
            "Batch Number: 876\n",
            "Loss: 0.5132735371589661\n",
            "Batch Number: 877\n",
            "Loss: 0.12588107585906982\n",
            "Batch Number: 878\n",
            "Loss: 0.2660893201828003\n",
            "Batch Number: 879\n",
            "Loss: 0.4389022886753082\n",
            "Batch Number: 880\n",
            "Loss: 0.3135957419872284\n",
            "Batch Number: 881\n",
            "Loss: 0.37343689799308777\n",
            "Batch Number: 882\n",
            "Loss: 0.17277811467647552\n",
            "Batch Number: 883\n",
            "Loss: 0.4137519896030426\n",
            "Batch Number: 884\n",
            "Loss: 0.7705100178718567\n",
            "Batch Number: 885\n",
            "Loss: 0.1904049962759018\n",
            "Batch Number: 886\n",
            "Loss: 0.4180257320404053\n",
            "Batch Number: 887\n",
            "Loss: 0.7043491005897522\n",
            "Batch Number: 888\n",
            "Loss: 0.5109320282936096\n",
            "Batch Number: 889\n",
            "Loss: 0.11295828968286514\n",
            "Batch Number: 890\n",
            "Loss: 0.26007959246635437\n",
            "Batch Number: 891\n",
            "Loss: 0.10092299431562424\n",
            "Batch Number: 892\n",
            "Loss: 0.09571286290884018\n",
            "Batch Number: 893\n",
            "Loss: 0.22672688961029053\n",
            "Batch Number: 894\n",
            "Loss: 0.5076972246170044\n",
            "Batch Number: 895\n",
            "Loss: 0.26208972930908203\n",
            "Batch Number: 896\n",
            "Loss: 1.1380447149276733\n",
            "Batch Number: 897\n",
            "Loss: 0.09256671369075775\n",
            "Batch Number: 898\n",
            "Loss: 0.29871419072151184\n",
            "Batch Number: 899\n",
            "Loss: 0.4653208255767822\n",
            "Batch Number: 900\n",
            "Loss: 0.2643035650253296\n",
            "Batch Number: 901\n",
            "Loss: 0.028667325153946877\n",
            "Batch Number: 902\n",
            "Loss: 0.030382877215743065\n",
            "Batch Number: 903\n",
            "Loss: 0.45450279116630554\n",
            "Batch Number: 904\n",
            "Loss: 0.03718340024352074\n",
            "Batch Number: 905\n",
            "Loss: 0.5084020495414734\n",
            "Batch Number: 906\n",
            "Loss: 0.10659857094287872\n",
            "Batch Number: 907\n",
            "Loss: 0.10947567224502563\n",
            "Batch Number: 908\n",
            "Loss: 0.06776347011327744\n",
            "Batch Number: 909\n",
            "Loss: 0.050704475492239\n",
            "Batch Number: 910\n",
            "Loss: 0.1096380352973938\n",
            "Batch Number: 911\n",
            "Loss: 0.18543122708797455\n",
            "Batch Number: 912\n",
            "Loss: 0.6788935661315918\n",
            "Batch Number: 913\n",
            "Loss: 0.32586461305618286\n",
            "Batch Number: 914\n",
            "Loss: 0.4523206353187561\n",
            "Batch Number: 915\n",
            "Loss: 0.5536376237869263\n",
            "Batch Number: 916\n",
            "Loss: 0.6632196307182312\n",
            "Batch Number: 917\n",
            "Loss: 0.20695219933986664\n",
            "Batch Number: 918\n",
            "Loss: 0.33729657530784607\n",
            "Batch Number: 919\n",
            "Loss: 0.22100578248500824\n",
            "Batch Number: 920\n",
            "Loss: 0.24919572472572327\n",
            "Batch Number: 921\n",
            "Loss: 0.14173585176467896\n",
            "Batch Number: 922\n",
            "Loss: 0.07142384350299835\n",
            "Batch Number: 923\n",
            "Loss: 0.36340007185935974\n",
            "Batch Number: 924\n",
            "Loss: 0.14628978073596954\n",
            "Batch Number: 925\n",
            "Loss: 0.4218134582042694\n",
            "Batch Number: 926\n",
            "Loss: 0.23829558491706848\n",
            "Batch Number: 927\n",
            "Loss: 0.5352506041526794\n",
            "Batch Number: 928\n",
            "Loss: 0.06939131021499634\n",
            "Batch Number: 929\n",
            "Loss: 0.07243408262729645\n",
            "Batch Number: 930\n",
            "Loss: 0.7188330888748169\n",
            "Batch Number: 931\n",
            "Loss: 0.20827023684978485\n",
            "Batch Number: 932\n",
            "Loss: 0.18256251513957977\n",
            "Batch Number: 933\n",
            "Loss: 0.6099684834480286\n",
            "Batch Number: 934\n",
            "Loss: 0.05389366298913956\n",
            "Batch Number: 935\n",
            "Loss: 0.35782286524772644\n",
            "Batch Number: 936\n",
            "Loss: 0.30144888162612915\n",
            "Batch Number: 937\n",
            "Loss: 0.28210124373435974\n",
            "Batch Number: 938\n",
            "Loss: 0.191167950630188\n",
            "Batch Number: 939\n",
            "Loss: 0.6906557083129883\n",
            "Batch Number: 940\n",
            "Loss: 0.15461477637290955\n",
            "Batch Number: 941\n",
            "Loss: 0.5754257440567017\n",
            "Batch Number: 942\n",
            "Loss: 0.5269598364830017\n",
            "Batch Number: 943\n",
            "Loss: 1.0633318424224854\n",
            "Batch Number: 944\n",
            "Loss: 0.4039618670940399\n",
            "Batch Number: 945\n",
            "Loss: 0.24101097881793976\n",
            "Batch Number: 946\n",
            "Loss: 0.12395431101322174\n",
            "Batch Number: 947\n",
            "Loss: 0.28580331802368164\n",
            "Batch Number: 948\n",
            "Loss: 0.04470844194293022\n",
            "Batch Number: 949\n",
            "Loss: 1.9570093154907227\n",
            "Batch Number: 950\n",
            "Loss: 0.9252261519432068\n",
            "Batch Number: 951\n",
            "Loss: 0.3811047673225403\n",
            "Batch Number: 952\n",
            "Loss: 0.17272774875164032\n",
            "Batch Number: 953\n",
            "Loss: 0.5249860882759094\n",
            "Batch Number: 954\n",
            "Loss: 0.16140925884246826\n",
            "Batch Number: 955\n",
            "Loss: 0.2876609265804291\n",
            "Batch Number: 956\n",
            "Loss: 0.4160853326320648\n",
            "Batch Number: 957\n",
            "Loss: 0.6679062247276306\n",
            "Batch Number: 958\n",
            "Loss: 0.197780579328537\n",
            "Batch Number: 959\n",
            "Loss: 0.4656601846218109\n",
            "Batch Number: 960\n",
            "Loss: 0.31870660185813904\n",
            "Batch Number: 961\n",
            "Loss: 0.40320220589637756\n",
            "Batch Number: 962\n",
            "Loss: 0.30670806765556335\n",
            "Batch Number: 963\n",
            "Loss: 0.7625905871391296\n",
            "Batch Number: 964\n",
            "Loss: 0.2973719537258148\n",
            "Batch Number: 965\n",
            "Loss: 0.3301120698451996\n",
            "Batch Number: 966\n",
            "Loss: 0.1472303420305252\n",
            "Batch Number: 967\n",
            "Loss: 0.30289551615715027\n",
            "Batch Number: 968\n",
            "Loss: 0.6478276252746582\n",
            "Batch Number: 969\n",
            "Loss: 0.04660070687532425\n",
            "Batch Number: 970\n",
            "Loss: 0.35043153166770935\n",
            "Batch Number: 971\n",
            "Loss: 0.4164346754550934\n",
            "Batch Number: 972\n",
            "Loss: 0.35960909724235535\n",
            "Batch Number: 973\n",
            "Loss: 0.2341800183057785\n",
            "Batch Number: 974\n",
            "Loss: 0.10188191384077072\n",
            "Batch Number: 975\n",
            "Loss: 0.0918363556265831\n",
            "Batch Number: 976\n",
            "Loss: 0.33484330773353577\n",
            "Batch Number: 977\n",
            "Loss: 0.19560770690441132\n",
            "Batch Number: 978\n",
            "Loss: 0.020288672298192978\n",
            "Batch Number: 979\n",
            "Loss: 0.15679645538330078\n",
            "Batch Number: 980\n",
            "Loss: 0.29663708806037903\n",
            "Batch Number: 981\n",
            "Loss: 0.09056627750396729\n",
            "Batch Number: 982\n",
            "Loss: 0.3769851624965668\n",
            "Batch Number: 983\n",
            "Loss: 0.055631544440984726\n",
            "Batch Number: 984\n",
            "Loss: 0.17347808182239532\n",
            "Batch Number: 985\n",
            "Loss: 0.4516759514808655\n",
            "Batch Number: 986\n",
            "Loss: 0.1823553591966629\n",
            "Batch Number: 987\n",
            "Loss: 0.22357086837291718\n",
            "Batch Number: 988\n",
            "Loss: 0.49647149443626404\n",
            "Batch Number: 989\n",
            "Loss: 0.2945323884487152\n",
            "Batch Number: 990\n",
            "Loss: 0.14798937737941742\n",
            "Batch Number: 991\n",
            "Loss: 0.26598820090293884\n",
            "Batch Number: 992\n",
            "Loss: 0.5535525679588318\n",
            "Batch Number: 993\n",
            "Loss: 0.04920819774270058\n",
            "Batch Number: 994\n",
            "Loss: 0.3495725095272064\n",
            "Batch Number: 995\n",
            "Loss: 0.22192196547985077\n",
            "Batch Number: 996\n",
            "Loss: 0.11289634555578232\n",
            "Batch Number: 997\n",
            "Loss: 0.27006667852401733\n",
            "Batch Number: 998\n",
            "Loss: 0.4598200023174286\n",
            "Batch Number: 999\n",
            "Loss: 0.16104093194007874\n",
            "Batch Number: 1000\n",
            "Loss: 0.16789528727531433\n",
            "Batch Number: 1001\n",
            "Loss: 0.1592625081539154\n",
            "Batch Number: 1002\n",
            "Loss: 0.10962412506341934\n",
            "Batch Number: 1003\n",
            "Loss: 0.373426616191864\n",
            "Batch Number: 1004\n",
            "Loss: 0.35240647196769714\n",
            "Batch Number: 1005\n",
            "Loss: 0.2355896532535553\n",
            "Batch Number: 1006\n",
            "Loss: 0.06542859971523285\n",
            "Batch Number: 1007\n",
            "Loss: 0.24855279922485352\n",
            "Batch Number: 1008\n",
            "Loss: 0.10885864496231079\n",
            "Batch Number: 1009\n",
            "Loss: 0.5154779553413391\n",
            "Batch Number: 1010\n",
            "Loss: 0.4686770439147949\n",
            "Batch Number: 1011\n",
            "Loss: 0.5415472984313965\n",
            "Batch Number: 1012\n",
            "Loss: 0.2739430367946625\n",
            "Batch Number: 1013\n",
            "Loss: 0.6404733657836914\n",
            "Batch Number: 1014\n",
            "Loss: 0.23502062261104584\n",
            "Batch Number: 1015\n",
            "Loss: 0.877604603767395\n",
            "Batch Number: 1016\n",
            "Loss: 0.25955477356910706\n",
            "Batch Number: 1017\n",
            "Loss: 0.04952194169163704\n",
            "Batch Number: 1018\n",
            "Loss: 0.1574690341949463\n",
            "Batch Number: 1019\n",
            "Loss: 0.20788338780403137\n",
            "Batch Number: 1020\n",
            "Loss: 0.8060576319694519\n",
            "Batch Number: 1021\n",
            "Loss: 0.17077769339084625\n",
            "Batch Number: 1022\n",
            "Loss: 0.3603103756904602\n",
            "Batch Number: 1023\n",
            "Loss: 0.33699870109558105\n",
            "Batch Number: 1024\n",
            "Loss: 0.031211627647280693\n",
            "Batch Number: 1025\n",
            "Loss: 0.07746477425098419\n",
            "Batch Number: 1026\n",
            "Loss: 0.24689190089702606\n",
            "Batch Number: 1027\n",
            "Loss: 0.5108925104141235\n",
            "Batch Number: 1028\n",
            "Loss: 0.18817979097366333\n",
            "Batch Number: 1029\n",
            "Loss: 0.1863032430410385\n",
            "Batch Number: 1030\n",
            "Loss: 0.41047605872154236\n",
            "Batch Number: 1031\n",
            "Loss: 0.10647008568048477\n",
            "Batch Number: 1032\n",
            "Loss: 0.49660634994506836\n",
            "Batch Number: 1033\n",
            "Loss: 0.09840861707925797\n",
            "Batch Number: 1034\n",
            "Loss: 0.0904550775885582\n",
            "Batch Number: 1035\n",
            "Loss: 0.12256350368261337\n",
            "Batch Number: 1036\n",
            "Loss: 0.13716882467269897\n",
            "Batch Number: 1037\n",
            "Loss: 0.12312673777341843\n",
            "Batch Number: 1038\n",
            "Loss: 0.0753425657749176\n",
            "Batch Number: 1039\n",
            "Loss: 0.037499621510505676\n",
            "Batch Number: 1040\n",
            "Loss: 0.20374427735805511\n",
            "Batch Number: 1041\n",
            "Loss: 0.24837303161621094\n",
            "Batch Number: 1042\n",
            "Loss: 0.21709631383419037\n",
            "Batch Number: 1043\n",
            "Loss: 0.5115036964416504\n",
            "Batch Number: 1044\n",
            "Loss: 0.08808331191539764\n",
            "Batch Number: 1045\n",
            "Loss: 0.49218660593032837\n",
            "Batch Number: 1046\n",
            "Loss: 0.37883830070495605\n",
            "Batch Number: 1047\n",
            "Loss: 0.06341206282377243\n",
            "Batch Number: 1048\n",
            "Loss: 0.7925030589103699\n",
            "Batch Number: 1049\n",
            "Loss: 0.060816001147031784\n",
            "Batch Number: 1050\n",
            "Loss: 0.27657046914100647\n",
            "Batch Number: 1051\n",
            "Loss: 0.33365944027900696\n",
            "Batch Number: 1052\n",
            "Loss: 0.33576223254203796\n",
            "Batch Number: 1053\n",
            "Loss: 0.33692702651023865\n",
            "Batch Number: 1054\n",
            "Loss: 0.05708283931016922\n",
            "Batch Number: 1055\n",
            "Loss: 0.29913443326950073\n",
            "Batch Number: 1056\n",
            "Loss: 0.3754649758338928\n",
            "Batch Number: 1057\n",
            "Loss: 0.05342375114560127\n",
            "Batch Number: 1058\n",
            "Loss: 0.31043827533721924\n",
            "Batch Number: 1059\n",
            "Loss: 0.14437514543533325\n",
            "Batch Number: 1060\n",
            "Loss: 0.13773857057094574\n",
            "Batch Number: 1061\n",
            "Loss: 0.2281656712293625\n",
            "Batch Number: 1062\n",
            "Loss: 0.23261991143226624\n",
            "Batch Number: 1063\n",
            "Loss: 0.04758952930569649\n",
            "Batch Number: 1064\n",
            "Loss: 0.23116996884346008\n",
            "Batch Number: 1065\n",
            "Loss: 0.3225732445716858\n",
            "Batch Number: 1066\n",
            "Loss: 0.14450311660766602\n",
            "Batch Number: 1067\n",
            "Loss: 0.05316975712776184\n",
            "Batch Number: 1068\n",
            "Loss: 0.20423753559589386\n",
            "Batch Number: 1069\n",
            "Loss: 0.20178551971912384\n",
            "Batch Number: 1070\n",
            "Loss: 0.4724302291870117\n",
            "Batch Number: 1071\n",
            "Loss: 0.6269972920417786\n",
            "Batch Number: 1072\n",
            "Loss: 0.8476986289024353\n",
            "Batch Number: 1073\n",
            "Loss: 0.2510809004306793\n",
            "Batch Number: 1074\n",
            "Loss: 0.03642474114894867\n",
            "Batch Number: 1075\n",
            "Loss: 0.09036984294652939\n",
            "Batch Number: 1076\n",
            "Loss: 0.06928006559610367\n",
            "Batch Number: 1077\n",
            "Loss: 0.34852924942970276\n",
            "Batch Number: 1078\n",
            "Loss: 0.2973700165748596\n",
            "Batch Number: 1079\n",
            "Loss: 0.20528335869312286\n",
            "Batch Number: 1080\n",
            "Loss: 0.2315530776977539\n",
            "Batch Number: 1081\n",
            "Loss: 0.2187846153974533\n",
            "Batch Number: 1082\n",
            "Loss: 0.6647517085075378\n",
            "Batch Number: 1083\n",
            "Loss: 0.5156072378158569\n",
            "Batch Number: 1084\n",
            "Loss: 0.01688791625201702\n",
            "Batch Number: 1085\n",
            "Loss: 0.7256483435630798\n",
            "Batch Number: 1086\n",
            "Loss: 0.3915817439556122\n",
            "Batch Number: 1087\n",
            "Loss: 0.07624917477369308\n",
            "Batch Number: 1088\n",
            "Loss: 0.16121573746204376\n",
            "Batch Number: 1089\n",
            "Loss: 0.16909384727478027\n",
            "Batch Number: 1090\n",
            "Loss: 0.0738774910569191\n",
            "Batch Number: 1091\n",
            "Loss: 0.268595814704895\n",
            "Batch Number: 1092\n",
            "Loss: 0.22506482899188995\n",
            "Batch Number: 1093\n",
            "Loss: 0.14851497113704681\n",
            "Batch Number: 1094\n",
            "Loss: 0.5353521704673767\n",
            "Batch Number: 1095\n",
            "Loss: 0.6963655352592468\n",
            "Batch Number: 1096\n",
            "Loss: 0.37575945258140564\n",
            "Batch Number: 1097\n",
            "Loss: 0.2334175854921341\n",
            "Batch Number: 1098\n",
            "Loss: 0.25558704137802124\n",
            "Batch Number: 1099\n",
            "Loss: 0.34261149168014526\n",
            "Batch Number: 1100\n",
            "Loss: 0.05096587538719177\n",
            "Batch Number: 1101\n",
            "Loss: 0.25317972898483276\n",
            "Batch Number: 1102\n",
            "Loss: 0.17422640323638916\n",
            "Batch Number: 1103\n",
            "Loss: 0.12604853510856628\n",
            "Batch Number: 1104\n",
            "Loss: 0.14350397884845734\n",
            "Batch Number: 1105\n",
            "Loss: 0.06828200817108154\n",
            "Batch Number: 1106\n",
            "Loss: 0.4619893729686737\n",
            "Batch Number: 1107\n",
            "Loss: 0.221380814909935\n",
            "Batch Number: 1108\n",
            "Loss: 1.1733042001724243\n",
            "Batch Number: 1109\n",
            "Loss: 0.10544884204864502\n",
            "Batch Number: 1110\n",
            "Loss: 0.0785093680024147\n",
            "Batch Number: 1111\n",
            "Loss: 0.23532310128211975\n",
            "Batch Number: 1112\n",
            "Loss: 0.45510998368263245\n",
            "Batch Number: 1113\n",
            "Loss: 0.2604089379310608\n",
            "Batch Number: 1114\n",
            "Loss: 0.2914063632488251\n",
            "Batch Number: 1115\n",
            "Loss: 0.20250879228115082\n",
            "Batch Number: 1116\n",
            "Loss: 0.16697722673416138\n",
            "Batch Number: 1117\n",
            "Loss: 0.19505329430103302\n",
            "Batch Number: 1118\n",
            "Loss: 0.27765846252441406\n",
            "Batch Number: 1119\n",
            "Loss: 0.1771543323993683\n",
            "Batch Number: 1120\n",
            "Loss: 0.09092569351196289\n",
            "Batch Number: 1121\n",
            "Loss: 0.7139241099357605\n",
            "Batch Number: 1122\n",
            "Loss: 0.4133571684360504\n",
            "Batch Number: 1123\n",
            "Loss: 0.6445493102073669\n",
            "Batch Number: 1124\n",
            "Loss: 0.07626251131296158\n",
            "Batch Number: 1125\n",
            "Loss: 0.17036549746990204\n",
            "Batch Number: 1126\n",
            "Loss: 0.04638321325182915\n",
            "Batch Number: 1127\n",
            "Loss: 0.2917157709598541\n",
            "Batch Number: 1128\n",
            "Loss: 0.13451559841632843\n",
            "Batch Number: 1129\n",
            "Loss: 0.19841346144676208\n",
            "Batch Number: 1130\n",
            "Loss: 0.49119406938552856\n",
            "Batch Number: 1131\n",
            "Loss: 0.2108166217803955\n",
            "Batch Number: 1132\n",
            "Loss: 1.010472059249878\n",
            "Batch Number: 1133\n",
            "Loss: 0.4656948745250702\n",
            "Batch Number: 1134\n",
            "Loss: 0.27657732367515564\n",
            "Batch Number: 1135\n",
            "Loss: 0.5272294878959656\n",
            "Batch Number: 1136\n",
            "Loss: 0.1782938688993454\n",
            "Batch Number: 1137\n",
            "Loss: 0.2970236539840698\n",
            "Batch Number: 1138\n",
            "Loss: 0.13296425342559814\n",
            "Batch Number: 1139\n",
            "Loss: 0.2707831561565399\n",
            "Batch Number: 1140\n",
            "Loss: 0.20555846393108368\n",
            "Batch Number: 1141\n",
            "Loss: 0.21780554950237274\n",
            "Batch Number: 1142\n",
            "Loss: 0.7314920425415039\n",
            "Validation\n",
            "Validation Loss: 0.1617661863565445\n",
            "Validation Loss: 0.24724216759204865\n",
            "Validation Loss: 0.11355932801961899\n",
            "Validation Loss: 0.2221996784210205\n",
            "Validation Loss: 0.09518702328205109\n",
            "Validation Loss: 0.2945951521396637\n",
            "Validation Loss: 0.3393220603466034\n",
            "Validation Loss: 0.1505318433046341\n",
            "Validation Loss: 0.4351515769958496\n",
            "Validation Loss: 0.5566754341125488\n",
            "Validation Loss: 0.39805999398231506\n",
            "Validation Loss: 0.5103143453598022\n",
            "Validation Loss: 0.5165511965751648\n",
            "Validation Loss: 0.3946668803691864\n",
            "Validation Loss: 0.9515045285224915\n",
            "Validation Loss: 0.4873017966747284\n",
            "Validation Loss: 0.1293514221906662\n",
            "Validation Loss: 0.4855623245239258\n",
            "Validation Loss: 0.49330994486808777\n",
            "Validation Loss: 0.4131165146827698\n",
            "Validation Loss: 0.1477610170841217\n",
            "Validation Loss: 0.41351309418678284\n",
            "Validation Loss: 0.14803370833396912\n",
            "Validation Loss: 0.6606755256652832\n",
            "Validation Loss: 0.25248953700065613\n",
            "Validation Loss: 0.6972584128379822\n",
            "Validation Loss: 0.09673251956701279\n",
            "Validation Loss: 0.15790043771266937\n",
            "Validation Loss: 0.18538646399974823\n",
            "Validation Loss: 0.09447341412305832\n",
            "Validation Loss: 0.5510278344154358\n",
            "Validation Loss: 0.5728543996810913\n",
            "Validation Loss: 0.14939932525157928\n",
            "Validation Loss: 0.3272877037525177\n",
            "Validation Loss: 1.2076772451400757\n",
            "Validation Loss: 0.4491097629070282\n",
            "Validation Loss: 0.24025888741016388\n",
            "Validation Loss: 0.30196282267570496\n",
            "Validation Loss: 0.2834635078907013\n",
            "Validation Loss: 0.21208210289478302\n",
            "Validation Loss: 0.3876080811023712\n",
            "Validation Loss: 1.1114768981933594\n",
            "Validation Loss: 0.12464547157287598\n",
            "Validation Loss: 0.4609852731227875\n",
            "Validation Loss: 0.1849655658006668\n",
            "Validation Loss: 0.15756003558635712\n",
            "Validation Loss: 0.10371648520231247\n",
            "Validation Loss: 0.2615077495574951\n",
            "Validation Loss: 0.3157183825969696\n",
            "Validation Loss: 0.10904143005609512\n",
            "Validation Loss: 0.24184678494930267\n",
            "Validation Loss: 0.5071694254875183\n",
            "Validation Loss: 0.1461460292339325\n",
            "Validation Loss: 0.3932969272136688\n",
            "Validation Loss: 0.6750137209892273\n",
            "Validation Loss: 0.38576826453208923\n",
            "Validation Loss: 0.7105008363723755\n",
            "Validation Loss: 0.48954057693481445\n",
            "Validation Loss: 0.3216828405857086\n",
            "Validation Loss: 0.09634997695684433\n",
            "Validation Loss: 0.3959568440914154\n",
            "Validation Loss: 0.2185051292181015\n",
            "Validation Loss: 1.518680453300476\n",
            "Validation Loss: 0.101593017578125\n",
            "Validation Loss: 0.37230512499809265\n",
            "Validation Loss: 0.4061654508113861\n",
            "Validation Loss: 0.35873401165008545\n",
            "Validation Loss: 0.716035008430481\n",
            "Validation Loss: 0.044205453246831894\n",
            "Validation Loss: 0.6988316178321838\n",
            "Validation Loss: 0.30115053057670593\n",
            "Validation Loss: 0.37255552411079407\n",
            "Validation Loss: 0.037115663290023804\n",
            "Validation Loss: 0.21436329185962677\n",
            "Validation Loss: 0.27373206615448\n",
            "Validation Loss: 0.301302045583725\n",
            "Validation Loss: 0.20234791934490204\n",
            "Validation Loss: 0.6451094746589661\n",
            "Validation Loss: 1.0445283651351929\n",
            "Validation Loss: 0.27240225672721863\n",
            "Validation Loss: 0.44905996322631836\n",
            "Validation Loss: 0.49943313002586365\n",
            "Validation Loss: 0.36274075508117676\n",
            "Validation Loss: 0.45144030451774597\n",
            "Validation Loss: 0.14065058529376984\n",
            "Validation Loss: 0.33849000930786133\n",
            "Validation Loss: 0.2840747833251953\n",
            "Validation Loss: 0.20219774544239044\n",
            "Validation Loss: 0.13157515227794647\n",
            "Validation Loss: 0.07021275907754898\n",
            "Validation Loss: 0.9175392985343933\n",
            "Validation Loss: 0.6167809963226318\n",
            "Validation Loss: 0.13619457185268402\n",
            "Validation Loss: 0.9041566848754883\n",
            "Validation Loss: 0.46824055910110474\n",
            "Validation Loss: 0.10795285552740097\n",
            "Validation Loss: 0.2018163651227951\n",
            "Validation Loss: 0.3661459982395172\n",
            "Validation Loss: 1.079956293106079\n",
            "Validation Loss: 0.9218552708625793\n",
            "Validation Loss: 0.5424670577049255\n",
            "Validation Loss: 1.9495775699615479\n",
            "Validation Loss: 0.23173823952674866\n",
            "Validation Loss: 1.5468920469284058\n",
            "Validation Loss: 0.7822291851043701\n",
            "Validation Loss: 0.08080708235502243\n",
            "Validation Loss: 0.691301167011261\n",
            "Validation Loss: 0.1501963585615158\n",
            "Validation Loss: 0.15684866905212402\n",
            "Validation Loss: 0.2105255126953125\n",
            "Validation Loss: 0.9446396231651306\n",
            "Validation Loss: 0.22894449532032013\n",
            "Validation Loss: 0.6240912079811096\n",
            "Validation Loss: 0.1070495992898941\n",
            "Validation Loss: 0.556239664554596\n",
            "Validation Loss: 2.6366240978240967\n",
            "Validation Loss: 0.10963060706853867\n",
            "Validation Loss: 0.22898827493190765\n",
            "Validation Loss: 0.2567397654056549\n",
            "Validation Loss: 0.587371826171875\n",
            "Validation Loss: 0.8718817830085754\n",
            "Validation Loss: 0.01820293627679348\n",
            "Validation Loss: 0.10039906948804855\n",
            "Validation Loss: 0.5339102745056152\n",
            "Validation Loss: 0.7174615859985352\n",
            "Validation Loss: 0.25646528601646423\n",
            "Validation Loss: 1.7841135263442993\n",
            "Validation Loss: 0.36565515398979187\n",
            "Validation Loss: 1.427854299545288\n",
            "Validation Loss: 0.7352321743965149\n",
            "Validation Loss: 0.18394306302070618\n",
            "Validation Loss: 0.8960652351379395\n",
            "Validation Loss: 1.74752938747406\n",
            "Validation Loss: 0.9081966280937195\n",
            "Validation Loss: 0.27377215027809143\n",
            "Validation Loss: 0.36660072207450867\n",
            "Validation Loss: 0.5895275473594666\n",
            "Validation Loss: 1.0309170484542847\n",
            "Validation Loss: 0.10413533449172974\n",
            "Validation Loss: 2.160651922225952\n",
            "Validation Loss: 0.2561575472354889\n",
            "Validation Loss: 0.40959349274635315\n",
            "Validation Loss: 0.4173804819583893\n",
            "Validation Loss: 1.4260104894638062\n",
            "Validation Loss: 0.556123673915863\n",
            "Validation Loss: 0.662810206413269\n",
            "Validation Loss: 0.49386951327323914\n",
            "Validation Loss: 0.22734485566616058\n",
            "Validation Loss: 0.4306325614452362\n",
            "Validation Loss: 0.18277715146541595\n",
            "Validation Loss: 1.5290061235427856\n",
            "Validation Loss: 0.2904324233531952\n",
            "Validation Loss: 0.36172762513160706\n",
            "Validation Loss: 0.18864209949970245\n",
            "Validation Loss: 0.5235166549682617\n",
            "Validation Loss: 0.1460980325937271\n",
            "Validation Loss: 0.9632081985473633\n",
            "Validation Loss: 0.2906516492366791\n",
            "Validation Loss: 1.647547960281372\n",
            "Validation Loss: 0.7436237335205078\n",
            "Validation Loss: 0.06783422082662582\n",
            "Validation Loss: 0.5786497592926025\n",
            "Validation Loss: 0.6732982993125916\n",
            "Validation Loss: 0.5359709858894348\n",
            "Validation Loss: 0.39768528938293457\n",
            "Validation Loss: 0.5098854899406433\n",
            "Validation Loss: 0.650227963924408\n",
            "Validation Loss: 0.9505354166030884\n",
            "Validation Loss: 0.26819461584091187\n",
            "Validation Loss: 1.1421335935592651\n",
            "Validation Loss: 0.5879144072532654\n",
            "Validation Loss: 0.6231481432914734\n",
            "Validation Loss: 0.1976313591003418\n",
            "Validation Loss: 0.6483123898506165\n",
            "Validation Loss: 0.41231584548950195\n",
            "Validation Loss: 0.16826584935188293\n",
            "Validation Loss: 1.2383660078048706\n",
            "Validation Loss: 0.8578504920005798\n",
            "Validation Loss: 0.6986886262893677\n",
            "Validation Loss: 0.7839406728744507\n",
            "Validation Loss: 0.6803960204124451\n",
            "Validation Loss: 0.22405369579792023\n",
            "Validation Loss: 0.42213860154151917\n",
            "Validation Loss: 0.8664482235908508\n",
            "Validation Loss: 0.6108662486076355\n",
            "Validation Loss: 0.6127020716667175\n",
            "Validation Loss: 0.3291369378566742\n",
            "Validation Loss: 0.23663043975830078\n",
            "Validation Loss: 1.6669262647628784\n",
            "Validation Loss: 0.28650471568107605\n",
            "Validation Loss: 0.748241126537323\n",
            "Validation Loss: 1.101253628730774\n",
            "Validation Loss: 0.30291369557380676\n",
            "Validation Loss: 0.14445118606090546\n",
            "Validation Loss: 0.5230710506439209\n",
            "Validation Loss: 1.1780282258987427\n",
            "Validation Loss: 1.3209060430526733\n",
            "Validation Loss: 0.2146008461713791\n",
            "Validation Loss: 0.8151933550834656\n",
            "Validation Loss: 0.8590086102485657\n",
            "Validation Loss: 0.6605129241943359\n",
            "Validation Loss: 0.23246757686138153\n",
            "Validation Loss: 0.11776204407215118\n",
            "Validation Loss: 0.1858547180891037\n",
            "Validation Loss: 0.07226873934268951\n",
            "Validation Loss: 0.41582170128822327\n",
            "Validation Loss: 0.25351202487945557\n",
            "Validation Loss: 0.11296834796667099\n",
            "Validation Loss: 0.5655922293663025\n",
            "Validation Loss: 0.3418343663215637\n",
            "Validation Loss: 0.19695234298706055\n",
            "Validation Loss: 0.16188518702983856\n",
            "Validation Loss: 0.03520488366484642\n",
            "Validation Loss: 0.12670652568340302\n",
            "Validation Loss: 0.3972911536693573\n",
            "Validation Loss: 0.1833888739347458\n",
            "Validation Loss: 0.38725805282592773\n",
            "Validation Loss: 0.45425650477409363\n",
            "Validation Loss: 0.183932825922966\n",
            "Validation Loss: 0.242256760597229\n",
            "Validation Loss: 1.0917394161224365\n",
            "Validation Loss: 0.20242832601070404\n",
            "Validation Loss: 0.13921275734901428\n",
            "Validation Loss: 0.5325020551681519\n",
            "Validation Loss: 0.05882307514548302\n",
            "Validation Loss: 0.17866800725460052\n",
            "Validation Loss: 0.29428574442863464\n",
            "Validation Loss: 0.23348169028759003\n",
            "Validation Loss: 0.03552314639091492\n",
            "Validation Loss: 0.3134385049343109\n",
            "Validation Loss: 0.12100231647491455\n",
            "Validation Loss: 0.3821024000644684\n",
            "Validation Loss: 0.19505338370800018\n",
            "Validation Loss: 0.2252422422170639\n",
            "Validation Loss: 0.6015767455101013\n",
            "Validation Loss: 0.10872983932495117\n",
            "Validation Loss: 0.41021424531936646\n",
            "Validation Loss: 0.21823513507843018\n",
            "Validation Loss: 0.35046982765197754\n",
            "Validation Loss: 0.1856643408536911\n",
            "Validation Loss: 0.300383061170578\n",
            "Validation Loss: 0.17986050248146057\n",
            "Validation Loss: 0.2810026705265045\n",
            "Validation Loss: 0.03666257485747337\n",
            "Validation Loss: 1.1786986589431763\n",
            "Validation Loss: 0.16775284707546234\n",
            "Validation Loss: 1.254477858543396\n",
            "Validation Loss: 0.4560811519622803\n",
            "Validation Loss: 1.3854354619979858\n",
            "Validation Loss: 0.633821427822113\n",
            "Validation Loss: 0.3367287814617157\n",
            "Validation Loss: 0.3280978798866272\n",
            "Validation Loss: 0.34535250067710876\n",
            "Validation Loss: 0.2118852138519287\n",
            "Validation Loss: 0.20833683013916016\n",
            "Validation Loss: 0.38357457518577576\n",
            "Validation Loss: 0.5904514789581299\n",
            "Validation Loss: 0.3790828287601471\n",
            "Validation Loss: 0.5642322897911072\n",
            "Validation Loss: 0.30530136823654175\n",
            "Validation Loss: 1.0406734943389893\n",
            "Validation Loss: 1.9623231887817383\n",
            "Validation Loss: 2.1266562938690186\n",
            "Validation Loss: 0.7297548651695251\n",
            "Validation Loss: 0.19717979431152344\n",
            "Validation Loss: 0.35273274779319763\n",
            "Validation Loss: 0.18060575425624847\n",
            "Validation Loss: 0.7997451424598694\n",
            "Validation Loss: 0.5552668571472168\n",
            "Validation Loss: 0.3400501608848572\n",
            "Validation Loss: 0.18104539811611176\n",
            "Validation Loss: 0.5369247794151306\n",
            "Validation Loss: 0.40736842155456543\n",
            "Validation Loss: 1.1913822889328003\n",
            "Validation Loss: 0.09442894905805588\n",
            "Validation Loss: 0.28616371750831604\n",
            "Validation Loss: 1.3682596683502197\n",
            "Validation Loss: 0.9487072229385376\n",
            "Validation Loss: 0.4761013090610504\n",
            "Validation Loss: 0.13617126643657684\n",
            "Validation Loss: 0.28081250190734863\n",
            "Validation Loss: 0.34370312094688416\n",
            "Validation Loss: 0.6072329878807068\n",
            "Validation Loss: 0.2992340326309204\n",
            "Validation Loss: 0.3530697524547577\n",
            "Validation Loss: 0.7193249464035034\n",
            "Validation Loss: 0.8676218390464783\n",
            "Validation Loss: 0.3281981647014618\n",
            "Validation Loss: 0.015415306203067303\n",
            "Validation Loss: 0.6409054398536682\n",
            "Validation Loss: 0.5575612187385559\n",
            "Validation Loss: 1.0409940481185913\n",
            "Validation Loss: 1.191472053527832\n",
            "Validation Loss: 0.48414838314056396\n",
            "Average Training Loss: 0.29993169020450366\n",
            "Average Validation Loss: 0.4960117664498587\n",
            "Training\n",
            "Epoch Number: 4\n",
            "Batch Number: 1\n",
            "Loss: 0.09259694814682007\n",
            "Batch Number: 2\n",
            "Loss: 0.1971135437488556\n",
            "Batch Number: 3\n",
            "Loss: 0.35990193486213684\n",
            "Batch Number: 4\n",
            "Loss: 0.3421514630317688\n",
            "Batch Number: 5\n",
            "Loss: 0.022755196318030357\n",
            "Batch Number: 6\n",
            "Loss: 0.07912274450063705\n",
            "Batch Number: 7\n",
            "Loss: 0.04125005006790161\n",
            "Batch Number: 8\n",
            "Loss: 0.18464195728302002\n",
            "Batch Number: 9\n",
            "Loss: 0.09407949447631836\n",
            "Batch Number: 10\n",
            "Loss: 0.17253538966178894\n",
            "Batch Number: 11\n",
            "Loss: 0.20188723504543304\n",
            "Batch Number: 12\n",
            "Loss: 0.05437720566987991\n",
            "Batch Number: 13\n",
            "Loss: 0.07118963450193405\n",
            "Batch Number: 14\n",
            "Loss: 0.1146112009882927\n",
            "Batch Number: 15\n",
            "Loss: 0.3745790719985962\n",
            "Batch Number: 16\n",
            "Loss: 0.7053185701370239\n",
            "Batch Number: 17\n",
            "Loss: 0.09695424139499664\n",
            "Batch Number: 18\n",
            "Loss: 0.0419168658554554\n",
            "Batch Number: 19\n",
            "Loss: 0.09683948755264282\n",
            "Batch Number: 20\n",
            "Loss: 0.14920657873153687\n",
            "Batch Number: 21\n",
            "Loss: 0.2404012680053711\n",
            "Batch Number: 22\n",
            "Loss: 0.1591370403766632\n",
            "Batch Number: 23\n",
            "Loss: 0.04202502965927124\n",
            "Batch Number: 24\n",
            "Loss: 0.13150762021541595\n",
            "Batch Number: 25\n",
            "Loss: 0.18093019723892212\n",
            "Batch Number: 26\n",
            "Loss: 0.1577947586774826\n",
            "Batch Number: 27\n",
            "Loss: 0.09413822740316391\n",
            "Batch Number: 28\n",
            "Loss: 0.3644367754459381\n",
            "Batch Number: 29\n",
            "Loss: 0.08829295635223389\n",
            "Batch Number: 30\n",
            "Loss: 0.05622521787881851\n",
            "Batch Number: 31\n",
            "Loss: 0.05959112569689751\n",
            "Batch Number: 32\n",
            "Loss: 0.15436019003391266\n",
            "Batch Number: 33\n",
            "Loss: 0.35315126180648804\n",
            "Batch Number: 34\n",
            "Loss: 0.08348023146390915\n",
            "Batch Number: 35\n",
            "Loss: 0.1525801718235016\n",
            "Batch Number: 36\n",
            "Loss: 0.1557459682226181\n",
            "Batch Number: 37\n",
            "Loss: 0.1469225138425827\n",
            "Batch Number: 38\n",
            "Loss: 0.20188851654529572\n",
            "Batch Number: 39\n",
            "Loss: 0.27309128642082214\n",
            "Batch Number: 40\n",
            "Loss: 0.541972279548645\n",
            "Batch Number: 41\n",
            "Loss: 0.17670093476772308\n",
            "Batch Number: 42\n",
            "Loss: 0.20070628821849823\n",
            "Batch Number: 43\n",
            "Loss: 0.10479667037725449\n",
            "Batch Number: 44\n",
            "Loss: 0.15468476712703705\n",
            "Batch Number: 45\n",
            "Loss: 0.4481429159641266\n",
            "Batch Number: 46\n",
            "Loss: 0.7373082637786865\n",
            "Batch Number: 47\n",
            "Loss: 0.06500707566738129\n",
            "Batch Number: 48\n",
            "Loss: 0.0985381156206131\n",
            "Batch Number: 49\n",
            "Loss: 0.2213737964630127\n",
            "Batch Number: 50\n",
            "Loss: 0.08033305406570435\n",
            "Batch Number: 51\n",
            "Loss: 0.09212671965360641\n",
            "Batch Number: 52\n",
            "Loss: 0.0506458654999733\n",
            "Batch Number: 53\n",
            "Loss: 0.11757972091436386\n",
            "Batch Number: 54\n",
            "Loss: 0.15557599067687988\n",
            "Batch Number: 55\n",
            "Loss: 0.11826654523611069\n",
            "Batch Number: 56\n",
            "Loss: 0.3206024169921875\n",
            "Batch Number: 57\n",
            "Loss: 0.05027155950665474\n",
            "Batch Number: 58\n",
            "Loss: 0.2575235068798065\n",
            "Batch Number: 59\n",
            "Loss: 0.22514191269874573\n",
            "Batch Number: 60\n",
            "Loss: 0.3748403787612915\n",
            "Batch Number: 61\n",
            "Loss: 0.06234623119235039\n",
            "Batch Number: 62\n",
            "Loss: 0.23567834496498108\n",
            "Batch Number: 63\n",
            "Loss: 0.3229100704193115\n",
            "Batch Number: 64\n",
            "Loss: 0.12969891726970673\n",
            "Batch Number: 65\n",
            "Loss: 0.13149891793727875\n",
            "Batch Number: 66\n",
            "Loss: 0.17998038232326508\n",
            "Batch Number: 67\n",
            "Loss: 0.26248010993003845\n",
            "Batch Number: 68\n",
            "Loss: 0.27816081047058105\n",
            "Batch Number: 69\n",
            "Loss: 0.3038414418697357\n",
            "Batch Number: 70\n",
            "Loss: 0.34852585196495056\n",
            "Batch Number: 71\n",
            "Loss: 0.2777528762817383\n",
            "Batch Number: 72\n",
            "Loss: 0.06991278380155563\n",
            "Batch Number: 73\n",
            "Loss: 0.28654301166534424\n",
            "Batch Number: 74\n",
            "Loss: 0.08793919533491135\n",
            "Batch Number: 75\n",
            "Loss: 0.04366892948746681\n",
            "Batch Number: 76\n",
            "Loss: 0.110902801156044\n",
            "Batch Number: 77\n",
            "Loss: 0.35149046778678894\n",
            "Batch Number: 78\n",
            "Loss: 0.1883082240819931\n",
            "Batch Number: 79\n",
            "Loss: 0.3940184712409973\n",
            "Batch Number: 80\n",
            "Loss: 0.19129596650600433\n",
            "Batch Number: 81\n",
            "Loss: 0.2248431295156479\n",
            "Batch Number: 82\n",
            "Loss: 0.21493588387966156\n",
            "Batch Number: 83\n",
            "Loss: 0.10165371000766754\n",
            "Batch Number: 84\n",
            "Loss: 0.19047211110591888\n",
            "Batch Number: 85\n",
            "Loss: 0.11556272953748703\n",
            "Batch Number: 86\n",
            "Loss: 0.21714220941066742\n",
            "Batch Number: 87\n",
            "Loss: 0.2892810106277466\n",
            "Batch Number: 88\n",
            "Loss: 0.3903813064098358\n",
            "Batch Number: 89\n",
            "Loss: 0.14097540080547333\n",
            "Batch Number: 90\n",
            "Loss: 0.08287461847066879\n",
            "Batch Number: 91\n",
            "Loss: 0.2634837329387665\n",
            "Batch Number: 92\n",
            "Loss: 0.2576233446598053\n",
            "Batch Number: 93\n",
            "Loss: 0.16800932586193085\n",
            "Batch Number: 94\n",
            "Loss: 0.4187416136264801\n",
            "Batch Number: 95\n",
            "Loss: 0.07092445343732834\n",
            "Batch Number: 96\n",
            "Loss: 0.178026482462883\n",
            "Batch Number: 97\n",
            "Loss: 0.5798482894897461\n",
            "Batch Number: 98\n",
            "Loss: 0.06717067211866379\n",
            "Batch Number: 99\n",
            "Loss: 0.053567517548799515\n",
            "Batch Number: 100\n",
            "Loss: 0.26473864912986755\n",
            "Batch Number: 101\n",
            "Loss: 0.14109361171722412\n",
            "Batch Number: 102\n",
            "Loss: 0.09127869457006454\n",
            "Batch Number: 103\n",
            "Loss: 0.07967814058065414\n",
            "Batch Number: 104\n",
            "Loss: 0.3649747371673584\n",
            "Batch Number: 105\n",
            "Loss: 0.25962382555007935\n",
            "Batch Number: 106\n",
            "Loss: 0.13499577343463898\n",
            "Batch Number: 107\n",
            "Loss: 0.04926157742738724\n",
            "Batch Number: 108\n",
            "Loss: 0.2610936164855957\n",
            "Batch Number: 109\n",
            "Loss: 0.08912628144025803\n",
            "Batch Number: 110\n",
            "Loss: 0.16490547358989716\n",
            "Batch Number: 111\n",
            "Loss: 0.04397780820727348\n",
            "Batch Number: 112\n",
            "Loss: 0.33632972836494446\n",
            "Batch Number: 113\n",
            "Loss: 0.20122110843658447\n",
            "Batch Number: 114\n",
            "Loss: 0.47735854983329773\n",
            "Batch Number: 115\n",
            "Loss: 0.2576422393321991\n",
            "Batch Number: 116\n",
            "Loss: 0.1849059909582138\n",
            "Batch Number: 117\n",
            "Loss: 0.0030624293722212315\n",
            "Batch Number: 118\n",
            "Loss: 0.01972181536257267\n",
            "Batch Number: 119\n",
            "Loss: 0.24285995960235596\n",
            "Batch Number: 120\n",
            "Loss: 0.14634351432323456\n",
            "Batch Number: 121\n",
            "Loss: 0.09895594418048859\n",
            "Batch Number: 122\n",
            "Loss: 0.09525094926357269\n",
            "Batch Number: 123\n",
            "Loss: 0.05737384781241417\n",
            "Batch Number: 124\n",
            "Loss: 0.05878530815243721\n",
            "Batch Number: 125\n",
            "Loss: 0.48392683267593384\n",
            "Batch Number: 126\n",
            "Loss: 0.3723120391368866\n",
            "Batch Number: 127\n",
            "Loss: 0.12225919216871262\n",
            "Batch Number: 128\n",
            "Loss: 0.2761533558368683\n",
            "Batch Number: 129\n",
            "Loss: 0.4992867410182953\n",
            "Batch Number: 130\n",
            "Loss: 0.04413604736328125\n",
            "Batch Number: 131\n",
            "Loss: 0.08449944853782654\n",
            "Batch Number: 132\n",
            "Loss: 0.33394867181777954\n",
            "Batch Number: 133\n",
            "Loss: 0.05978887155652046\n",
            "Batch Number: 134\n",
            "Loss: 0.08395062386989594\n",
            "Batch Number: 135\n",
            "Loss: 0.18691372871398926\n",
            "Batch Number: 136\n",
            "Loss: 0.08717335015535355\n",
            "Batch Number: 137\n",
            "Loss: 0.3156924247741699\n",
            "Batch Number: 138\n",
            "Loss: 0.14701803028583527\n",
            "Batch Number: 139\n",
            "Loss: 0.031850822269916534\n",
            "Batch Number: 140\n",
            "Loss: 0.07474298775196075\n",
            "Batch Number: 141\n",
            "Loss: 0.1526821106672287\n",
            "Batch Number: 142\n",
            "Loss: 0.13344918191432953\n",
            "Batch Number: 143\n",
            "Loss: 0.32434341311454773\n",
            "Batch Number: 144\n",
            "Loss: 0.10547387599945068\n",
            "Batch Number: 145\n",
            "Loss: 0.09167786687612534\n",
            "Batch Number: 146\n",
            "Loss: 0.07919548451900482\n",
            "Batch Number: 147\n",
            "Loss: 0.6963061690330505\n",
            "Batch Number: 148\n",
            "Loss: 0.045008789747953415\n",
            "Batch Number: 149\n",
            "Loss: 0.22335496544837952\n",
            "Batch Number: 150\n",
            "Loss: 0.1707731932401657\n",
            "Batch Number: 151\n",
            "Loss: 0.13513587415218353\n",
            "Batch Number: 152\n",
            "Loss: 0.05996055528521538\n",
            "Batch Number: 153\n",
            "Loss: 0.13512681424617767\n",
            "Batch Number: 154\n",
            "Loss: 0.22209183871746063\n",
            "Batch Number: 155\n",
            "Loss: 0.300896018743515\n",
            "Batch Number: 156\n",
            "Loss: 0.15990807116031647\n",
            "Batch Number: 157\n",
            "Loss: 0.06365703791379929\n",
            "Batch Number: 158\n",
            "Loss: 0.0799500122666359\n",
            "Batch Number: 159\n",
            "Loss: 0.24266386032104492\n",
            "Batch Number: 160\n",
            "Loss: 0.5578460097312927\n",
            "Batch Number: 161\n",
            "Loss: 0.02012220025062561\n",
            "Batch Number: 162\n",
            "Loss: 0.2396843433380127\n",
            "Batch Number: 163\n",
            "Loss: 0.30801260471343994\n",
            "Batch Number: 164\n",
            "Loss: 0.035667773336172104\n",
            "Batch Number: 165\n",
            "Loss: 0.1498798429965973\n",
            "Batch Number: 166\n",
            "Loss: 0.30305907130241394\n",
            "Batch Number: 167\n",
            "Loss: 0.39113113284111023\n",
            "Batch Number: 168\n",
            "Loss: 0.3648103177547455\n",
            "Batch Number: 169\n",
            "Loss: 0.1754596084356308\n",
            "Batch Number: 170\n",
            "Loss: 0.0825144350528717\n",
            "Batch Number: 171\n",
            "Loss: 0.412186861038208\n",
            "Batch Number: 172\n",
            "Loss: 0.49002763628959656\n",
            "Batch Number: 173\n",
            "Loss: 0.2448304444551468\n",
            "Batch Number: 174\n",
            "Loss: 0.751305878162384\n",
            "Batch Number: 175\n",
            "Loss: 0.2112226039171219\n",
            "Batch Number: 176\n",
            "Loss: 0.21389590203762054\n",
            "Batch Number: 177\n",
            "Loss: 0.08544912189245224\n",
            "Batch Number: 178\n",
            "Loss: 0.36820903420448303\n",
            "Batch Number: 179\n",
            "Loss: 0.22977565228939056\n",
            "Batch Number: 180\n",
            "Loss: 0.27325570583343506\n",
            "Batch Number: 181\n",
            "Loss: 0.20788703858852386\n",
            "Batch Number: 182\n",
            "Loss: 0.5347965955734253\n",
            "Batch Number: 183\n",
            "Loss: 0.07875453680753708\n",
            "Batch Number: 184\n",
            "Loss: 0.2377728670835495\n",
            "Batch Number: 185\n",
            "Loss: 0.19361071288585663\n",
            "Batch Number: 186\n",
            "Loss: 0.14797475934028625\n",
            "Batch Number: 187\n",
            "Loss: 0.17522074282169342\n",
            "Batch Number: 188\n",
            "Loss: 0.10120397061109543\n",
            "Batch Number: 189\n",
            "Loss: 0.16502821445465088\n",
            "Batch Number: 190\n",
            "Loss: 0.24465589225292206\n",
            "Batch Number: 191\n",
            "Loss: 0.28011736273765564\n",
            "Batch Number: 192\n",
            "Loss: 0.15200446546077728\n",
            "Batch Number: 193\n",
            "Loss: 0.51575767993927\n",
            "Batch Number: 194\n",
            "Loss: 0.325111448764801\n",
            "Batch Number: 195\n",
            "Loss: 0.0881819874048233\n",
            "Batch Number: 196\n",
            "Loss: 0.1144556999206543\n",
            "Batch Number: 197\n",
            "Loss: 0.12750329077243805\n",
            "Batch Number: 198\n",
            "Loss: 0.18974585831165314\n",
            "Batch Number: 199\n",
            "Loss: 0.16620080173015594\n",
            "Batch Number: 200\n",
            "Loss: 0.1193334087729454\n",
            "Batch Number: 201\n",
            "Loss: 0.40312790870666504\n",
            "Batch Number: 202\n",
            "Loss: 0.2248024195432663\n",
            "Batch Number: 203\n",
            "Loss: 0.6655322909355164\n",
            "Batch Number: 204\n",
            "Loss: 0.007594902068376541\n",
            "Batch Number: 205\n",
            "Loss: 0.3878338038921356\n",
            "Batch Number: 206\n",
            "Loss: 0.11894048750400543\n",
            "Batch Number: 207\n",
            "Loss: 0.11301308125257492\n",
            "Batch Number: 208\n",
            "Loss: 0.04039284214377403\n",
            "Batch Number: 209\n",
            "Loss: 0.6906093955039978\n",
            "Batch Number: 210\n",
            "Loss: 0.07132729142904282\n",
            "Batch Number: 211\n",
            "Loss: 0.11840202659368515\n",
            "Batch Number: 212\n",
            "Loss: 0.22067666053771973\n",
            "Batch Number: 213\n",
            "Loss: 0.09368101507425308\n",
            "Batch Number: 214\n",
            "Loss: 0.10479556769132614\n",
            "Batch Number: 215\n",
            "Loss: 0.24119816720485687\n",
            "Batch Number: 216\n",
            "Loss: 0.12199127674102783\n",
            "Batch Number: 217\n",
            "Loss: 0.16270141303539276\n",
            "Batch Number: 218\n",
            "Loss: 0.22030441462993622\n",
            "Batch Number: 219\n",
            "Loss: 0.16657400131225586\n",
            "Batch Number: 220\n",
            "Loss: 0.5944026708602905\n",
            "Batch Number: 221\n",
            "Loss: 0.10080855339765549\n",
            "Batch Number: 222\n",
            "Loss: 0.379162460565567\n",
            "Batch Number: 223\n",
            "Loss: 0.3916129767894745\n",
            "Batch Number: 224\n",
            "Loss: 0.14566199481487274\n",
            "Batch Number: 225\n",
            "Loss: 0.20292054116725922\n",
            "Batch Number: 226\n",
            "Loss: 0.2883220911026001\n",
            "Batch Number: 227\n",
            "Loss: 0.04718850925564766\n",
            "Batch Number: 228\n",
            "Loss: 0.22801518440246582\n",
            "Batch Number: 229\n",
            "Loss: 0.10815892368555069\n",
            "Batch Number: 230\n",
            "Loss: 0.16686539351940155\n",
            "Batch Number: 231\n",
            "Loss: 0.04709623008966446\n",
            "Batch Number: 232\n",
            "Loss: 0.081293486058712\n",
            "Batch Number: 233\n",
            "Loss: 0.17222777009010315\n",
            "Batch Number: 234\n",
            "Loss: 0.10341506451368332\n",
            "Batch Number: 235\n",
            "Loss: 0.5772550702095032\n",
            "Batch Number: 236\n",
            "Loss: 0.21286308765411377\n",
            "Batch Number: 237\n",
            "Loss: 0.27717432379722595\n",
            "Batch Number: 238\n",
            "Loss: 0.14881478250026703\n",
            "Batch Number: 239\n",
            "Loss: 0.11031433194875717\n",
            "Batch Number: 240\n",
            "Loss: 0.331286758184433\n",
            "Batch Number: 241\n",
            "Loss: 0.3126268982887268\n",
            "Batch Number: 242\n",
            "Loss: 0.06495831161737442\n",
            "Batch Number: 243\n",
            "Loss: 0.2600369453430176\n",
            "Batch Number: 244\n",
            "Loss: 0.22689440846443176\n",
            "Batch Number: 245\n",
            "Loss: 0.2687624990940094\n",
            "Batch Number: 246\n",
            "Loss: 0.14530520141124725\n",
            "Batch Number: 247\n",
            "Loss: 0.10987742245197296\n",
            "Batch Number: 248\n",
            "Loss: 0.12709441781044006\n",
            "Batch Number: 249\n",
            "Loss: 0.4484822750091553\n",
            "Batch Number: 250\n",
            "Loss: 0.10680293291807175\n",
            "Batch Number: 251\n",
            "Loss: 0.7002191543579102\n",
            "Batch Number: 252\n",
            "Loss: 0.2070850431919098\n",
            "Batch Number: 253\n",
            "Loss: 0.25553861260414124\n",
            "Batch Number: 254\n",
            "Loss: 0.46839800477027893\n",
            "Batch Number: 255\n",
            "Loss: 0.2551608681678772\n",
            "Batch Number: 256\n",
            "Loss: 0.1624288111925125\n",
            "Batch Number: 257\n",
            "Loss: 0.7115287780761719\n",
            "Batch Number: 258\n",
            "Loss: 0.3852304518222809\n",
            "Batch Number: 259\n",
            "Loss: 0.17330363392829895\n",
            "Batch Number: 260\n",
            "Loss: 0.2582140564918518\n",
            "Batch Number: 261\n",
            "Loss: 0.4936010539531708\n",
            "Batch Number: 262\n",
            "Loss: 0.2593955993652344\n",
            "Batch Number: 263\n",
            "Loss: 0.20526249706745148\n",
            "Batch Number: 264\n",
            "Loss: 0.3915136754512787\n",
            "Batch Number: 265\n",
            "Loss: 0.37374061346054077\n",
            "Batch Number: 266\n",
            "Loss: 0.1033666655421257\n",
            "Batch Number: 267\n",
            "Loss: 0.2261795997619629\n",
            "Batch Number: 268\n",
            "Loss: 0.1360749453306198\n",
            "Batch Number: 269\n",
            "Loss: 0.15505880117416382\n",
            "Batch Number: 270\n",
            "Loss: 0.22572286427021027\n",
            "Batch Number: 271\n",
            "Loss: 0.27431559562683105\n",
            "Batch Number: 272\n",
            "Loss: 0.4171912670135498\n",
            "Batch Number: 273\n",
            "Loss: 0.17955230176448822\n",
            "Batch Number: 274\n",
            "Loss: 0.051494300365448\n",
            "Batch Number: 275\n",
            "Loss: 0.26253587007522583\n",
            "Batch Number: 276\n",
            "Loss: 0.16459394991397858\n",
            "Batch Number: 277\n",
            "Loss: 0.19704698026180267\n",
            "Batch Number: 278\n",
            "Loss: 0.10248618572950363\n",
            "Batch Number: 279\n",
            "Loss: 0.5303356647491455\n",
            "Batch Number: 280\n",
            "Loss: 0.4306928813457489\n",
            "Batch Number: 281\n",
            "Loss: 0.06672603636980057\n",
            "Batch Number: 282\n",
            "Loss: 0.5064988136291504\n",
            "Batch Number: 283\n",
            "Loss: 0.2841828465461731\n",
            "Batch Number: 284\n",
            "Loss: 0.10533151775598526\n",
            "Batch Number: 285\n",
            "Loss: 0.726860523223877\n",
            "Batch Number: 286\n",
            "Loss: 0.11543288081884384\n",
            "Batch Number: 287\n",
            "Loss: 0.04424326494336128\n",
            "Batch Number: 288\n",
            "Loss: 0.14533691108226776\n",
            "Batch Number: 289\n",
            "Loss: 0.043825481086969376\n",
            "Batch Number: 290\n",
            "Loss: 0.14800070226192474\n",
            "Batch Number: 291\n",
            "Loss: 0.19225168228149414\n",
            "Batch Number: 292\n",
            "Loss: 0.17105215787887573\n",
            "Batch Number: 293\n",
            "Loss: 0.521786630153656\n",
            "Batch Number: 294\n",
            "Loss: 0.1359413117170334\n",
            "Batch Number: 295\n",
            "Loss: 0.2399725764989853\n",
            "Batch Number: 296\n",
            "Loss: 0.26818177103996277\n",
            "Batch Number: 297\n",
            "Loss: 0.4376846253871918\n",
            "Batch Number: 298\n",
            "Loss: 0.14521972835063934\n",
            "Batch Number: 299\n",
            "Loss: 0.1631753295660019\n",
            "Batch Number: 300\n",
            "Loss: 0.0624518059194088\n",
            "Batch Number: 301\n",
            "Loss: 0.30559054017066956\n",
            "Batch Number: 302\n",
            "Loss: 0.11022237688302994\n",
            "Batch Number: 303\n",
            "Loss: 0.183061882853508\n",
            "Batch Number: 304\n",
            "Loss: 0.16821500658988953\n",
            "Batch Number: 305\n",
            "Loss: 0.1299336850643158\n",
            "Batch Number: 306\n",
            "Loss: 0.15079115331172943\n",
            "Batch Number: 307\n",
            "Loss: 0.2869276702404022\n",
            "Batch Number: 308\n",
            "Loss: 0.1023046150803566\n",
            "Batch Number: 309\n",
            "Loss: 0.17739343643188477\n",
            "Batch Number: 310\n",
            "Loss: 0.1594814509153366\n",
            "Batch Number: 311\n",
            "Loss: 0.1260809749364853\n",
            "Batch Number: 312\n",
            "Loss: 0.12593404948711395\n",
            "Batch Number: 313\n",
            "Loss: 0.21266570687294006\n",
            "Batch Number: 314\n",
            "Loss: 0.024161381646990776\n",
            "Batch Number: 315\n",
            "Loss: 0.048059817403554916\n",
            "Batch Number: 316\n",
            "Loss: 0.13305044174194336\n",
            "Batch Number: 317\n",
            "Loss: 0.33462902903556824\n",
            "Batch Number: 318\n",
            "Loss: 0.2820809781551361\n",
            "Batch Number: 319\n",
            "Loss: 0.0893888771533966\n",
            "Batch Number: 320\n",
            "Loss: 0.12833422422409058\n",
            "Batch Number: 321\n",
            "Loss: 0.43871423602104187\n",
            "Batch Number: 322\n",
            "Loss: 0.15111589431762695\n",
            "Batch Number: 323\n",
            "Loss: 0.0548185370862484\n",
            "Batch Number: 324\n",
            "Loss: 0.04435037449002266\n",
            "Batch Number: 325\n",
            "Loss: 0.49828144907951355\n",
            "Batch Number: 326\n",
            "Loss: 0.08342987298965454\n",
            "Batch Number: 327\n",
            "Loss: 0.012440478429198265\n",
            "Batch Number: 328\n",
            "Loss: 0.6057259440422058\n",
            "Batch Number: 329\n",
            "Loss: 0.10906519740819931\n",
            "Batch Number: 330\n",
            "Loss: 0.3331201374530792\n",
            "Batch Number: 331\n",
            "Loss: 0.10275644063949585\n",
            "Batch Number: 332\n",
            "Loss: 0.12967084348201752\n",
            "Batch Number: 333\n",
            "Loss: 0.8462573289871216\n",
            "Batch Number: 334\n",
            "Loss: 0.14800076186656952\n",
            "Batch Number: 335\n",
            "Loss: 0.3613329827785492\n",
            "Batch Number: 336\n",
            "Loss: 0.37205734848976135\n",
            "Batch Number: 337\n",
            "Loss: 0.535343587398529\n",
            "Batch Number: 338\n",
            "Loss: 0.02212885022163391\n",
            "Batch Number: 339\n",
            "Loss: 0.5094515681266785\n",
            "Batch Number: 340\n",
            "Loss: 0.1067742109298706\n",
            "Batch Number: 341\n",
            "Loss: 0.3151338994503021\n",
            "Batch Number: 342\n",
            "Loss: 0.2468685358762741\n",
            "Batch Number: 343\n",
            "Loss: 0.28961238265037537\n",
            "Batch Number: 344\n",
            "Loss: 0.42225170135498047\n",
            "Batch Number: 345\n",
            "Loss: 0.1526447981595993\n",
            "Batch Number: 346\n",
            "Loss: 0.10238325595855713\n",
            "Batch Number: 347\n",
            "Loss: 0.16496604681015015\n",
            "Batch Number: 348\n",
            "Loss: 0.24301496148109436\n",
            "Batch Number: 349\n",
            "Loss: 0.08470391482114792\n",
            "Batch Number: 350\n",
            "Loss: 0.2088838815689087\n",
            "Batch Number: 351\n",
            "Loss: 0.23495249450206757\n",
            "Batch Number: 352\n",
            "Loss: 0.6688822507858276\n",
            "Batch Number: 353\n",
            "Loss: 0.22308015823364258\n",
            "Batch Number: 354\n",
            "Loss: 0.24536409974098206\n",
            "Batch Number: 355\n",
            "Loss: 0.10069229453802109\n",
            "Batch Number: 356\n",
            "Loss: 0.03446924313902855\n",
            "Batch Number: 357\n",
            "Loss: 0.2766522765159607\n",
            "Batch Number: 358\n",
            "Loss: 0.11876001209020615\n",
            "Batch Number: 359\n",
            "Loss: 0.38811787962913513\n",
            "Batch Number: 360\n",
            "Loss: 0.1498022824525833\n",
            "Batch Number: 361\n",
            "Loss: 0.173286572098732\n",
            "Batch Number: 362\n",
            "Loss: 0.2675624191761017\n",
            "Batch Number: 363\n",
            "Loss: 0.24438682198524475\n",
            "Batch Number: 364\n",
            "Loss: 0.15322177112102509\n",
            "Batch Number: 365\n",
            "Loss: 0.1932753175497055\n",
            "Batch Number: 366\n",
            "Loss: 0.11765159666538239\n",
            "Batch Number: 367\n",
            "Loss: 0.3766704201698303\n",
            "Batch Number: 368\n",
            "Loss: 0.9593579173088074\n",
            "Batch Number: 369\n",
            "Loss: 0.1044740080833435\n",
            "Batch Number: 370\n",
            "Loss: 0.06192290410399437\n",
            "Batch Number: 371\n",
            "Loss: 0.09725852310657501\n",
            "Batch Number: 372\n",
            "Loss: 0.2040034383535385\n",
            "Batch Number: 373\n",
            "Loss: 0.05569228157401085\n",
            "Batch Number: 374\n",
            "Loss: 0.2028580904006958\n",
            "Batch Number: 375\n",
            "Loss: 0.12504051625728607\n",
            "Batch Number: 376\n",
            "Loss: 0.14775395393371582\n",
            "Batch Number: 377\n",
            "Loss: 0.17422008514404297\n",
            "Batch Number: 378\n",
            "Loss: 0.05198536068201065\n",
            "Batch Number: 379\n",
            "Loss: 0.22813621163368225\n",
            "Batch Number: 380\n",
            "Loss: 0.14053188264369965\n",
            "Batch Number: 381\n",
            "Loss: 0.13424882292747498\n",
            "Batch Number: 382\n",
            "Loss: 0.36858686804771423\n",
            "Batch Number: 383\n",
            "Loss: 0.2249937802553177\n",
            "Batch Number: 384\n",
            "Loss: 0.2939912676811218\n",
            "Batch Number: 385\n",
            "Loss: 0.4404637813568115\n",
            "Batch Number: 386\n",
            "Loss: 0.1131802350282669\n",
            "Batch Number: 387\n",
            "Loss: 0.2870330512523651\n",
            "Batch Number: 388\n",
            "Loss: 0.7465466856956482\n",
            "Batch Number: 389\n",
            "Loss: 0.16583172976970673\n",
            "Batch Number: 390\n",
            "Loss: 0.2717916965484619\n",
            "Batch Number: 391\n",
            "Loss: 0.35815533995628357\n",
            "Batch Number: 392\n",
            "Loss: 0.22767095267772675\n",
            "Batch Number: 393\n",
            "Loss: 0.49620699882507324\n",
            "Batch Number: 394\n",
            "Loss: 0.2311558723449707\n",
            "Batch Number: 395\n",
            "Loss: 0.2361464500427246\n",
            "Batch Number: 396\n",
            "Loss: 0.07103413343429565\n",
            "Batch Number: 397\n",
            "Loss: 0.5053879618644714\n",
            "Batch Number: 398\n",
            "Loss: 0.11588845402002335\n",
            "Batch Number: 399\n",
            "Loss: 0.4084394872188568\n",
            "Batch Number: 400\n",
            "Loss: 0.23727481067180634\n",
            "Batch Number: 401\n",
            "Loss: 0.10912766307592392\n",
            "Batch Number: 402\n",
            "Loss: 0.3856368958950043\n",
            "Batch Number: 403\n",
            "Loss: 0.14403095841407776\n",
            "Batch Number: 404\n",
            "Loss: 0.40308982133865356\n",
            "Batch Number: 405\n",
            "Loss: 0.30150744318962097\n",
            "Batch Number: 406\n",
            "Loss: 0.11246366798877716\n",
            "Batch Number: 407\n",
            "Loss: 0.10634437948465347\n",
            "Batch Number: 408\n",
            "Loss: 0.11546261608600616\n",
            "Batch Number: 409\n",
            "Loss: 0.06817708164453506\n",
            "Batch Number: 410\n",
            "Loss: 0.10117029398679733\n",
            "Batch Number: 411\n",
            "Loss: 0.1371968537569046\n",
            "Batch Number: 412\n",
            "Loss: 0.2504975199699402\n",
            "Batch Number: 413\n",
            "Loss: 0.26180145144462585\n",
            "Batch Number: 414\n",
            "Loss: 0.07530056685209274\n",
            "Batch Number: 415\n",
            "Loss: 0.12640857696533203\n",
            "Batch Number: 416\n",
            "Loss: 0.12549550831317902\n",
            "Batch Number: 417\n",
            "Loss: 0.03448792174458504\n",
            "Batch Number: 418\n",
            "Loss: 0.11902689188718796\n",
            "Batch Number: 419\n",
            "Loss: 0.45105716586112976\n",
            "Batch Number: 420\n",
            "Loss: 0.05666208267211914\n",
            "Batch Number: 421\n",
            "Loss: 0.10392671078443527\n",
            "Batch Number: 422\n",
            "Loss: 0.09468210488557816\n",
            "Batch Number: 423\n",
            "Loss: 0.17060326039791107\n",
            "Batch Number: 424\n",
            "Loss: 0.22169910371303558\n",
            "Batch Number: 425\n",
            "Loss: 0.35542646050453186\n",
            "Batch Number: 426\n",
            "Loss: 0.250463604927063\n",
            "Batch Number: 427\n",
            "Loss: 0.1585208922624588\n",
            "Batch Number: 428\n",
            "Loss: 0.14129455387592316\n",
            "Batch Number: 429\n",
            "Loss: 0.2662836015224457\n",
            "Batch Number: 430\n",
            "Loss: 0.160470113158226\n",
            "Batch Number: 431\n",
            "Loss: 0.4098125398159027\n",
            "Batch Number: 432\n",
            "Loss: 0.21617189049720764\n",
            "Batch Number: 433\n",
            "Loss: 0.3348598778247833\n",
            "Batch Number: 434\n",
            "Loss: 0.4613766372203827\n",
            "Batch Number: 435\n",
            "Loss: 0.7400504946708679\n",
            "Batch Number: 436\n",
            "Loss: 0.06727534532546997\n",
            "Batch Number: 437\n",
            "Loss: 0.08471488207578659\n",
            "Batch Number: 438\n",
            "Loss: 0.17798767983913422\n",
            "Batch Number: 439\n",
            "Loss: 0.10429048538208008\n",
            "Batch Number: 440\n",
            "Loss: 0.03444116935133934\n",
            "Batch Number: 441\n",
            "Loss: 0.27131977677345276\n",
            "Batch Number: 442\n",
            "Loss: 0.18764565885066986\n",
            "Batch Number: 443\n",
            "Loss: 0.5521398186683655\n",
            "Batch Number: 444\n",
            "Loss: 0.12257330864667892\n",
            "Batch Number: 445\n",
            "Loss: 0.054353732615709305\n",
            "Batch Number: 446\n",
            "Loss: 0.04842689260840416\n",
            "Batch Number: 447\n",
            "Loss: 0.06934044510126114\n",
            "Batch Number: 448\n",
            "Loss: 0.19898462295532227\n",
            "Batch Number: 449\n",
            "Loss: 0.08539112657308578\n",
            "Batch Number: 450\n",
            "Loss: 0.15724776685237885\n",
            "Batch Number: 451\n",
            "Loss: 0.020376306027173996\n",
            "Batch Number: 452\n",
            "Loss: 0.18672047555446625\n",
            "Batch Number: 453\n",
            "Loss: 0.1872975379228592\n",
            "Batch Number: 454\n",
            "Loss: 0.26265498995780945\n",
            "Batch Number: 455\n",
            "Loss: 0.39787036180496216\n",
            "Batch Number: 456\n",
            "Loss: 0.7201567888259888\n",
            "Batch Number: 457\n",
            "Loss: 0.16798977553844452\n",
            "Batch Number: 458\n",
            "Loss: 0.11672776937484741\n",
            "Batch Number: 459\n",
            "Loss: 0.23819947242736816\n",
            "Batch Number: 460\n",
            "Loss: 0.0449875183403492\n",
            "Batch Number: 461\n",
            "Loss: 0.2596859335899353\n",
            "Batch Number: 462\n",
            "Loss: 0.21481327712535858\n",
            "Batch Number: 463\n",
            "Loss: 0.12460990250110626\n",
            "Batch Number: 464\n",
            "Loss: 0.09059249609708786\n",
            "Batch Number: 465\n",
            "Loss: 0.15441691875457764\n",
            "Batch Number: 466\n",
            "Loss: 0.16189168393611908\n",
            "Batch Number: 467\n",
            "Loss: 0.24854397773742676\n",
            "Batch Number: 468\n",
            "Loss: 0.03985392674803734\n",
            "Batch Number: 469\n",
            "Loss: 0.1147349625825882\n",
            "Batch Number: 470\n",
            "Loss: 0.08430089056491852\n",
            "Batch Number: 471\n",
            "Loss: 0.13904228806495667\n",
            "Batch Number: 472\n",
            "Loss: 0.15200400352478027\n",
            "Batch Number: 473\n",
            "Loss: 0.2621530592441559\n",
            "Batch Number: 474\n",
            "Loss: 0.1254822015762329\n",
            "Batch Number: 475\n",
            "Loss: 0.2777236998081207\n",
            "Batch Number: 476\n",
            "Loss: 0.10737814009189606\n",
            "Batch Number: 477\n",
            "Loss: 0.09757592529058456\n",
            "Batch Number: 478\n",
            "Loss: 0.3859942853450775\n",
            "Batch Number: 479\n",
            "Loss: 0.30085691809654236\n",
            "Batch Number: 480\n",
            "Loss: 0.1488984078168869\n",
            "Batch Number: 481\n",
            "Loss: 0.36954349279403687\n",
            "Batch Number: 482\n",
            "Loss: 0.07069331407546997\n",
            "Batch Number: 483\n",
            "Loss: 0.15419653058052063\n",
            "Batch Number: 484\n",
            "Loss: 0.07198083400726318\n",
            "Batch Number: 485\n",
            "Loss: 0.15316234529018402\n",
            "Batch Number: 486\n",
            "Loss: 0.07090651243925095\n",
            "Batch Number: 487\n",
            "Loss: 0.18046467006206512\n",
            "Batch Number: 488\n",
            "Loss: 0.297321081161499\n",
            "Batch Number: 489\n",
            "Loss: 0.20280824601650238\n",
            "Batch Number: 490\n",
            "Loss: 0.28246262669563293\n",
            "Batch Number: 491\n",
            "Loss: 0.15838603675365448\n",
            "Batch Number: 492\n",
            "Loss: 0.1024744063615799\n",
            "Batch Number: 493\n",
            "Loss: 0.1686316579580307\n",
            "Batch Number: 494\n",
            "Loss: 0.20646491646766663\n",
            "Batch Number: 495\n",
            "Loss: 0.0992354303598404\n",
            "Batch Number: 496\n",
            "Loss: 0.6579693555831909\n",
            "Batch Number: 497\n",
            "Loss: 0.25727778673171997\n",
            "Batch Number: 498\n",
            "Loss: 0.5991812944412231\n",
            "Batch Number: 499\n",
            "Loss: 0.10951801389455795\n",
            "Batch Number: 500\n",
            "Loss: 0.47672319412231445\n",
            "Batch Number: 501\n",
            "Loss: 0.31260940432548523\n",
            "Batch Number: 502\n",
            "Loss: 0.1852601319551468\n",
            "Batch Number: 503\n",
            "Loss: 0.21364764869213104\n",
            "Batch Number: 504\n",
            "Loss: 0.21750429272651672\n",
            "Batch Number: 505\n",
            "Loss: 0.03986207768321037\n",
            "Batch Number: 506\n",
            "Loss: 0.40810224413871765\n",
            "Batch Number: 507\n",
            "Loss: 0.6145159602165222\n",
            "Batch Number: 508\n",
            "Loss: 0.17724193632602692\n",
            "Batch Number: 509\n",
            "Loss: 0.22689305245876312\n",
            "Batch Number: 510\n",
            "Loss: 0.3971101939678192\n",
            "Batch Number: 511\n",
            "Loss: 0.13098666071891785\n",
            "Batch Number: 512\n",
            "Loss: 0.13287682831287384\n",
            "Batch Number: 513\n",
            "Loss: 0.7141571044921875\n",
            "Batch Number: 514\n",
            "Loss: 0.3516409993171692\n",
            "Batch Number: 515\n",
            "Loss: 0.13946793973445892\n",
            "Batch Number: 516\n",
            "Loss: 0.15646982192993164\n",
            "Batch Number: 517\n",
            "Loss: 0.8553823828697205\n",
            "Batch Number: 518\n",
            "Loss: 0.12513850629329681\n",
            "Batch Number: 519\n",
            "Loss: 0.03973955661058426\n",
            "Batch Number: 520\n",
            "Loss: 0.13045132160186768\n",
            "Batch Number: 521\n",
            "Loss: 0.6798175573348999\n",
            "Batch Number: 522\n",
            "Loss: 0.2657340466976166\n",
            "Batch Number: 523\n",
            "Loss: 0.09705514460802078\n",
            "Batch Number: 524\n",
            "Loss: 0.1311313956975937\n",
            "Batch Number: 525\n",
            "Loss: 0.1981591284275055\n",
            "Batch Number: 526\n",
            "Loss: 0.5690569877624512\n",
            "Batch Number: 527\n",
            "Loss: 0.18939439952373505\n",
            "Batch Number: 528\n",
            "Loss: 0.47344931960105896\n",
            "Batch Number: 529\n",
            "Loss: 0.05922708660364151\n",
            "Batch Number: 530\n",
            "Loss: 0.2870524227619171\n",
            "Batch Number: 531\n",
            "Loss: 0.16531531512737274\n",
            "Batch Number: 532\n",
            "Loss: 0.23954415321350098\n",
            "Batch Number: 533\n",
            "Loss: 0.4493642747402191\n",
            "Batch Number: 534\n",
            "Loss: 0.24239063262939453\n",
            "Batch Number: 535\n",
            "Loss: 0.4549271762371063\n",
            "Batch Number: 536\n",
            "Loss: 0.12241766601800919\n",
            "Batch Number: 537\n",
            "Loss: 0.1534394472837448\n",
            "Batch Number: 538\n",
            "Loss: 1.6776756048202515\n",
            "Batch Number: 539\n",
            "Loss: 0.33651310205459595\n",
            "Batch Number: 540\n",
            "Loss: 0.6089154481887817\n",
            "Batch Number: 541\n",
            "Loss: 0.5298088192939758\n",
            "Batch Number: 542\n",
            "Loss: 0.19227388501167297\n",
            "Batch Number: 543\n",
            "Loss: 0.08480560779571533\n",
            "Batch Number: 544\n",
            "Loss: 0.24181799590587616\n",
            "Batch Number: 545\n",
            "Loss: 0.13609196245670319\n",
            "Batch Number: 546\n",
            "Loss: 0.07979028671979904\n",
            "Batch Number: 547\n",
            "Loss: 0.09605822712182999\n",
            "Batch Number: 548\n",
            "Loss: 0.1617179960012436\n",
            "Batch Number: 549\n",
            "Loss: 0.1859586536884308\n",
            "Batch Number: 550\n",
            "Loss: 0.4629022777080536\n",
            "Batch Number: 551\n",
            "Loss: 0.3549308776855469\n",
            "Batch Number: 552\n",
            "Loss: 0.27113214135169983\n",
            "Batch Number: 553\n",
            "Loss: 0.20515410602092743\n",
            "Batch Number: 554\n",
            "Loss: 0.4128182828426361\n",
            "Batch Number: 555\n",
            "Loss: 0.5085598826408386\n",
            "Batch Number: 556\n",
            "Loss: 0.04711194708943367\n",
            "Batch Number: 557\n",
            "Loss: 0.15117020905017853\n",
            "Batch Number: 558\n",
            "Loss: 0.3136323392391205\n",
            "Batch Number: 559\n",
            "Loss: 0.3541010320186615\n",
            "Batch Number: 560\n",
            "Loss: 0.12527529895305634\n",
            "Batch Number: 561\n",
            "Loss: 0.1355356127023697\n",
            "Batch Number: 562\n",
            "Loss: 0.1949189007282257\n",
            "Batch Number: 563\n",
            "Loss: 0.11778030544519424\n",
            "Batch Number: 564\n",
            "Loss: 0.46675798296928406\n",
            "Batch Number: 565\n",
            "Loss: 0.06114625558257103\n",
            "Batch Number: 566\n",
            "Loss: 0.33518725633621216\n",
            "Batch Number: 567\n",
            "Loss: 0.29315856099128723\n",
            "Batch Number: 568\n",
            "Loss: 0.15433792769908905\n",
            "Batch Number: 569\n",
            "Loss: 0.36017295718193054\n",
            "Batch Number: 570\n",
            "Loss: 0.24050433933734894\n",
            "Batch Number: 571\n",
            "Loss: 1.2394245862960815\n",
            "Batch Number: 572\n",
            "Loss: 0.06625787913799286\n",
            "Batch Number: 573\n",
            "Loss: 0.33479103446006775\n",
            "Batch Number: 574\n",
            "Loss: 0.3866310715675354\n",
            "Batch Number: 575\n",
            "Loss: 0.049481432884931564\n",
            "Batch Number: 576\n",
            "Loss: 0.03004624880850315\n",
            "Batch Number: 577\n",
            "Loss: 0.03742968663573265\n",
            "Batch Number: 578\n",
            "Loss: 0.28384658694267273\n",
            "Batch Number: 579\n",
            "Loss: 0.16310229897499084\n",
            "Batch Number: 580\n",
            "Loss: 0.5968727469444275\n",
            "Batch Number: 581\n",
            "Loss: 0.20616154372692108\n",
            "Batch Number: 582\n",
            "Loss: 0.2022506296634674\n",
            "Batch Number: 583\n",
            "Loss: 0.34573984146118164\n",
            "Batch Number: 584\n",
            "Loss: 0.17876844108104706\n",
            "Batch Number: 585\n",
            "Loss: 0.04756031930446625\n",
            "Batch Number: 586\n",
            "Loss: 0.2957450747489929\n",
            "Batch Number: 587\n",
            "Loss: 0.3387162387371063\n",
            "Batch Number: 588\n",
            "Loss: 0.2504943907260895\n",
            "Batch Number: 589\n",
            "Loss: 0.6817895174026489\n",
            "Batch Number: 590\n",
            "Loss: 0.37980300188064575\n",
            "Batch Number: 591\n",
            "Loss: 0.4749418795108795\n",
            "Batch Number: 592\n",
            "Loss: 0.18631529808044434\n",
            "Batch Number: 593\n",
            "Loss: 0.09286652505397797\n",
            "Batch Number: 594\n",
            "Loss: 0.04244902729988098\n",
            "Batch Number: 595\n",
            "Loss: 0.11472036689519882\n",
            "Batch Number: 596\n",
            "Loss: 0.10084092617034912\n",
            "Batch Number: 597\n",
            "Loss: 0.4315864145755768\n",
            "Batch Number: 598\n",
            "Loss: 0.35862573981285095\n",
            "Batch Number: 599\n",
            "Loss: 0.35396286845207214\n",
            "Batch Number: 600\n",
            "Loss: 0.475544273853302\n",
            "Batch Number: 601\n",
            "Loss: 0.09089018404483795\n",
            "Batch Number: 602\n",
            "Loss: 0.13339653611183167\n",
            "Batch Number: 603\n",
            "Loss: 0.07145001739263535\n",
            "Batch Number: 604\n",
            "Loss: 0.3376738131046295\n",
            "Batch Number: 605\n",
            "Loss: 0.5705760717391968\n",
            "Batch Number: 606\n",
            "Loss: 0.15059316158294678\n",
            "Batch Number: 607\n",
            "Loss: 0.15477408468723297\n",
            "Batch Number: 608\n",
            "Loss: 0.11458911746740341\n",
            "Batch Number: 609\n",
            "Loss: 0.5033665895462036\n",
            "Batch Number: 610\n",
            "Loss: 0.27844753861427307\n",
            "Batch Number: 611\n",
            "Loss: 0.3078201115131378\n",
            "Batch Number: 612\n",
            "Loss: 0.027559330686926842\n",
            "Batch Number: 613\n",
            "Loss: 0.1298707276582718\n",
            "Batch Number: 614\n",
            "Loss: 0.19012194871902466\n",
            "Batch Number: 615\n",
            "Loss: 0.16148047149181366\n",
            "Batch Number: 616\n",
            "Loss: 0.1461818963289261\n",
            "Batch Number: 617\n",
            "Loss: 0.3361564874649048\n",
            "Batch Number: 618\n",
            "Loss: 0.35446226596832275\n",
            "Batch Number: 619\n",
            "Loss: 0.03924066200852394\n",
            "Batch Number: 620\n",
            "Loss: 0.4111814498901367\n",
            "Batch Number: 621\n",
            "Loss: 0.17743146419525146\n",
            "Batch Number: 622\n",
            "Loss: 0.20428235828876495\n",
            "Batch Number: 623\n",
            "Loss: 0.2561739385128021\n",
            "Batch Number: 624\n",
            "Loss: 0.3596342206001282\n",
            "Batch Number: 625\n",
            "Loss: 0.35211801528930664\n",
            "Batch Number: 626\n",
            "Loss: 0.1330224722623825\n",
            "Batch Number: 627\n",
            "Loss: 0.396127313375473\n",
            "Batch Number: 628\n",
            "Loss: 0.06935512274503708\n",
            "Batch Number: 629\n",
            "Loss: 0.197965607047081\n",
            "Batch Number: 630\n",
            "Loss: 0.9233617782592773\n",
            "Batch Number: 631\n",
            "Loss: 0.1933671087026596\n",
            "Batch Number: 632\n",
            "Loss: 0.4635816216468811\n",
            "Batch Number: 633\n",
            "Loss: 0.24415825307369232\n",
            "Batch Number: 634\n",
            "Loss: 0.4681929051876068\n",
            "Batch Number: 635\n",
            "Loss: 0.7633237838745117\n",
            "Batch Number: 636\n",
            "Loss: 0.20528243482112885\n",
            "Batch Number: 637\n",
            "Loss: 0.1334124356508255\n",
            "Batch Number: 638\n",
            "Loss: 0.4309549033641815\n",
            "Batch Number: 639\n",
            "Loss: 0.2263367623090744\n",
            "Batch Number: 640\n",
            "Loss: 0.2742440402507782\n",
            "Batch Number: 641\n",
            "Loss: 0.13967739045619965\n",
            "Batch Number: 642\n",
            "Loss: 0.158410906791687\n",
            "Batch Number: 643\n",
            "Loss: 0.22543683648109436\n",
            "Batch Number: 644\n",
            "Loss: 0.04515719413757324\n",
            "Batch Number: 645\n",
            "Loss: 0.15975718200206757\n",
            "Batch Number: 646\n",
            "Loss: 0.07926017791032791\n",
            "Batch Number: 647\n",
            "Loss: 0.06702881306409836\n",
            "Batch Number: 648\n",
            "Loss: 0.4756603240966797\n",
            "Batch Number: 649\n",
            "Loss: 0.6372930407524109\n",
            "Batch Number: 650\n",
            "Loss: 0.31778427958488464\n",
            "Batch Number: 651\n",
            "Loss: 0.07195601612329483\n",
            "Batch Number: 652\n",
            "Loss: 0.14956136047840118\n",
            "Batch Number: 653\n",
            "Loss: 0.16422465443611145\n",
            "Batch Number: 654\n",
            "Loss: 0.18472644686698914\n",
            "Batch Number: 655\n",
            "Loss: 0.14372259378433228\n",
            "Batch Number: 656\n",
            "Loss: 0.16116349399089813\n",
            "Batch Number: 657\n",
            "Loss: 0.17822642624378204\n",
            "Batch Number: 658\n",
            "Loss: 0.15873238444328308\n",
            "Batch Number: 659\n",
            "Loss: 0.26784902811050415\n",
            "Batch Number: 660\n",
            "Loss: 0.24709156155586243\n",
            "Batch Number: 661\n",
            "Loss: 0.2804632782936096\n",
            "Batch Number: 662\n",
            "Loss: 0.16955511271953583\n",
            "Batch Number: 663\n",
            "Loss: 0.22171227633953094\n",
            "Batch Number: 664\n",
            "Loss: 0.49918919801712036\n",
            "Batch Number: 665\n",
            "Loss: 0.4268035888671875\n",
            "Batch Number: 666\n",
            "Loss: 0.09881345182657242\n",
            "Batch Number: 667\n",
            "Loss: 0.23609113693237305\n",
            "Batch Number: 668\n",
            "Loss: 0.08321361243724823\n",
            "Batch Number: 669\n",
            "Loss: 0.366569846868515\n",
            "Batch Number: 670\n",
            "Loss: 0.21161845326423645\n",
            "Batch Number: 671\n",
            "Loss: 0.32708555459976196\n",
            "Batch Number: 672\n",
            "Loss: 0.2246900051832199\n",
            "Batch Number: 673\n",
            "Loss: 0.5776116251945496\n",
            "Batch Number: 674\n",
            "Loss: 0.423030287027359\n",
            "Batch Number: 675\n",
            "Loss: 0.055536895990371704\n",
            "Batch Number: 676\n",
            "Loss: 0.113888218998909\n",
            "Batch Number: 677\n",
            "Loss: 0.4742411673069\n",
            "Batch Number: 678\n",
            "Loss: 0.10366097837686539\n",
            "Batch Number: 679\n",
            "Loss: 0.24080215394496918\n",
            "Batch Number: 680\n",
            "Loss: 0.11813964694738388\n",
            "Batch Number: 681\n",
            "Loss: 0.09428361058235168\n",
            "Batch Number: 682\n",
            "Loss: 0.1544174998998642\n",
            "Batch Number: 683\n",
            "Loss: 0.2628777027130127\n",
            "Batch Number: 684\n",
            "Loss: 0.12039337307214737\n",
            "Batch Number: 685\n",
            "Loss: 0.10281892120838165\n",
            "Batch Number: 686\n",
            "Loss: 0.21366766095161438\n",
            "Batch Number: 687\n",
            "Loss: 0.07079587131738663\n",
            "Batch Number: 688\n",
            "Loss: 0.3458070755004883\n",
            "Batch Number: 689\n",
            "Loss: 0.20382718741893768\n",
            "Batch Number: 690\n",
            "Loss: 0.31811386346817017\n",
            "Batch Number: 691\n",
            "Loss: 0.08480652421712875\n",
            "Batch Number: 692\n",
            "Loss: 0.10748735815286636\n",
            "Batch Number: 693\n",
            "Loss: 0.15578018128871918\n",
            "Batch Number: 694\n",
            "Loss: 0.04783570021390915\n",
            "Batch Number: 695\n",
            "Loss: 0.31515398621559143\n",
            "Batch Number: 696\n",
            "Loss: 0.1995784342288971\n",
            "Batch Number: 697\n",
            "Loss: 0.2799052894115448\n",
            "Batch Number: 698\n",
            "Loss: 0.17608791589736938\n",
            "Batch Number: 699\n",
            "Loss: 0.05472825840115547\n",
            "Batch Number: 700\n",
            "Loss: 0.09023388475179672\n",
            "Batch Number: 701\n",
            "Loss: 0.12313895672559738\n",
            "Batch Number: 702\n",
            "Loss: 0.2319279909133911\n",
            "Batch Number: 703\n",
            "Loss: 0.34642401337623596\n",
            "Batch Number: 704\n",
            "Loss: 0.15646030008792877\n",
            "Batch Number: 705\n",
            "Loss: 0.1316537857055664\n",
            "Batch Number: 706\n",
            "Loss: 0.09731506556272507\n",
            "Batch Number: 707\n",
            "Loss: 0.6757296919822693\n",
            "Batch Number: 708\n",
            "Loss: 0.2695661187171936\n",
            "Batch Number: 709\n",
            "Loss: 0.07425867766141891\n",
            "Batch Number: 710\n",
            "Loss: 0.24142014980316162\n",
            "Batch Number: 711\n",
            "Loss: 0.05120779201388359\n",
            "Batch Number: 712\n",
            "Loss: 0.12680684030056\n",
            "Batch Number: 713\n",
            "Loss: 0.28627705574035645\n",
            "Batch Number: 714\n",
            "Loss: 0.15689870715141296\n",
            "Batch Number: 715\n",
            "Loss: 0.18285241723060608\n",
            "Batch Number: 716\n",
            "Loss: 0.2553844153881073\n",
            "Batch Number: 717\n",
            "Loss: 0.14643484354019165\n",
            "Batch Number: 718\n",
            "Loss: 0.03609466552734375\n",
            "Batch Number: 719\n",
            "Loss: 0.21900713443756104\n",
            "Batch Number: 720\n",
            "Loss: 0.3168269097805023\n",
            "Batch Number: 721\n",
            "Loss: 0.013748827390372753\n",
            "Batch Number: 722\n",
            "Loss: 0.2982693612575531\n",
            "Batch Number: 723\n",
            "Loss: 0.07100214809179306\n",
            "Batch Number: 724\n",
            "Loss: 0.08496508747339249\n",
            "Batch Number: 725\n",
            "Loss: 0.0599512942135334\n",
            "Batch Number: 726\n",
            "Loss: 0.17552262544631958\n",
            "Batch Number: 727\n",
            "Loss: 0.5544894337654114\n",
            "Batch Number: 728\n",
            "Loss: 0.07928507030010223\n",
            "Batch Number: 729\n",
            "Loss: 0.24168069660663605\n",
            "Batch Number: 730\n",
            "Loss: 0.2534821331501007\n",
            "Batch Number: 731\n",
            "Loss: 0.1461239904165268\n",
            "Batch Number: 732\n",
            "Loss: 0.2535657584667206\n",
            "Batch Number: 733\n",
            "Loss: 0.12463271617889404\n",
            "Batch Number: 734\n",
            "Loss: 0.2475482076406479\n",
            "Batch Number: 735\n",
            "Loss: 0.2298317402601242\n",
            "Batch Number: 736\n",
            "Loss: 0.4547230899333954\n",
            "Batch Number: 737\n",
            "Loss: 0.1767726093530655\n",
            "Batch Number: 738\n",
            "Loss: 0.20067544281482697\n",
            "Batch Number: 739\n",
            "Loss: 0.3613445460796356\n",
            "Batch Number: 740\n",
            "Loss: 0.7650993466377258\n",
            "Batch Number: 741\n",
            "Loss: 1.1872892379760742\n",
            "Batch Number: 742\n",
            "Loss: 0.257417231798172\n",
            "Batch Number: 743\n",
            "Loss: 0.23596982657909393\n",
            "Batch Number: 744\n",
            "Loss: 0.2551088035106659\n",
            "Batch Number: 745\n",
            "Loss: 0.23762579262256622\n",
            "Batch Number: 746\n",
            "Loss: 0.06512212008237839\n",
            "Batch Number: 747\n",
            "Loss: 0.3280600607395172\n",
            "Batch Number: 748\n",
            "Loss: 0.19631783664226532\n",
            "Batch Number: 749\n",
            "Loss: 0.10725774616003036\n",
            "Batch Number: 750\n",
            "Loss: 0.29679033160209656\n",
            "Batch Number: 751\n",
            "Loss: 0.2267170250415802\n",
            "Batch Number: 752\n",
            "Loss: 0.2515753507614136\n",
            "Batch Number: 753\n",
            "Loss: 0.8194875717163086\n",
            "Batch Number: 754\n",
            "Loss: 0.30474475026130676\n",
            "Batch Number: 755\n",
            "Loss: 0.16646133363246918\n",
            "Batch Number: 756\n",
            "Loss: 0.08595581352710724\n",
            "Batch Number: 757\n",
            "Loss: 0.7059723734855652\n",
            "Batch Number: 758\n",
            "Loss: 0.2344897836446762\n",
            "Batch Number: 759\n",
            "Loss: 0.214178204536438\n",
            "Batch Number: 760\n",
            "Loss: 0.04416397586464882\n",
            "Batch Number: 761\n",
            "Loss: 0.22034092247486115\n",
            "Batch Number: 762\n",
            "Loss: 0.1421346813440323\n",
            "Batch Number: 763\n",
            "Loss: 0.32017049193382263\n",
            "Batch Number: 764\n",
            "Loss: 0.19996778666973114\n",
            "Batch Number: 765\n",
            "Loss: 0.1505475789308548\n",
            "Batch Number: 766\n",
            "Loss: 0.08941584825515747\n",
            "Batch Number: 767\n",
            "Loss: 0.1886625736951828\n",
            "Batch Number: 768\n",
            "Loss: 0.31024304032325745\n",
            "Batch Number: 769\n",
            "Loss: 0.11723583191633224\n",
            "Batch Number: 770\n",
            "Loss: 0.47641420364379883\n",
            "Batch Number: 771\n",
            "Loss: 0.06995663791894913\n",
            "Batch Number: 772\n",
            "Loss: 0.024465348571538925\n",
            "Batch Number: 773\n",
            "Loss: 0.5104403495788574\n",
            "Batch Number: 774\n",
            "Loss: 0.47645092010498047\n",
            "Batch Number: 775\n",
            "Loss: 0.5245217084884644\n",
            "Batch Number: 776\n",
            "Loss: 0.3495655953884125\n",
            "Batch Number: 777\n",
            "Loss: 0.12675149738788605\n",
            "Batch Number: 778\n",
            "Loss: 0.10160082578659058\n",
            "Batch Number: 779\n",
            "Loss: 0.06006277725100517\n",
            "Batch Number: 780\n",
            "Loss: 0.1851392537355423\n",
            "Batch Number: 781\n",
            "Loss: 0.6068124175071716\n",
            "Batch Number: 782\n",
            "Loss: 0.1939116269350052\n",
            "Batch Number: 783\n",
            "Loss: 0.01730806939303875\n",
            "Batch Number: 784\n",
            "Loss: 0.2637290060520172\n",
            "Batch Number: 785\n",
            "Loss: 0.07385826110839844\n",
            "Batch Number: 786\n",
            "Loss: 0.15338338911533356\n",
            "Batch Number: 787\n",
            "Loss: 0.20613475143909454\n",
            "Batch Number: 788\n",
            "Loss: 0.3316105306148529\n",
            "Batch Number: 789\n",
            "Loss: 0.03733556345105171\n",
            "Batch Number: 790\n",
            "Loss: 0.18345831334590912\n",
            "Batch Number: 791\n",
            "Loss: 0.5660300254821777\n",
            "Batch Number: 792\n",
            "Loss: 0.06333806365728378\n",
            "Batch Number: 793\n",
            "Loss: 0.23168063163757324\n",
            "Batch Number: 794\n",
            "Loss: 0.3443104922771454\n",
            "Batch Number: 795\n",
            "Loss: 0.2009296864271164\n",
            "Batch Number: 796\n",
            "Loss: 0.1748262494802475\n",
            "Batch Number: 797\n",
            "Loss: 0.09946154803037643\n",
            "Batch Number: 798\n",
            "Loss: 0.22053305804729462\n",
            "Batch Number: 799\n",
            "Loss: 0.11725592613220215\n",
            "Batch Number: 800\n",
            "Loss: 0.03344707563519478\n",
            "Batch Number: 801\n",
            "Loss: 0.1315132975578308\n",
            "Batch Number: 802\n",
            "Loss: 0.5822622179985046\n",
            "Batch Number: 803\n",
            "Loss: 0.0919715166091919\n",
            "Batch Number: 804\n",
            "Loss: 0.14774379134178162\n",
            "Batch Number: 805\n",
            "Loss: 0.18122759461402893\n",
            "Batch Number: 806\n",
            "Loss: 0.013586893677711487\n",
            "Batch Number: 807\n",
            "Loss: 0.234629824757576\n",
            "Batch Number: 808\n",
            "Loss: 0.06799744814634323\n",
            "Batch Number: 809\n",
            "Loss: 0.7578410506248474\n",
            "Batch Number: 810\n",
            "Loss: 0.33800217509269714\n",
            "Batch Number: 811\n",
            "Loss: 0.1347750425338745\n",
            "Batch Number: 812\n",
            "Loss: 0.12646494805812836\n",
            "Batch Number: 813\n",
            "Loss: 0.31991371512413025\n",
            "Batch Number: 814\n",
            "Loss: 0.3934260308742523\n",
            "Batch Number: 815\n",
            "Loss: 0.23463912308216095\n",
            "Batch Number: 816\n",
            "Loss: 0.5959498286247253\n",
            "Batch Number: 817\n",
            "Loss: 0.38324639201164246\n",
            "Batch Number: 818\n",
            "Loss: 0.1098184734582901\n",
            "Batch Number: 819\n",
            "Loss: 0.03509381413459778\n",
            "Batch Number: 820\n",
            "Loss: 0.3261144161224365\n",
            "Batch Number: 821\n",
            "Loss: 0.10465913265943527\n",
            "Batch Number: 822\n",
            "Loss: 0.03519439324736595\n",
            "Batch Number: 823\n",
            "Loss: 0.08335653692483902\n",
            "Batch Number: 824\n",
            "Loss: 0.20231226086616516\n",
            "Batch Number: 825\n",
            "Loss: 0.24455367028713226\n",
            "Batch Number: 826\n",
            "Loss: 0.3069559335708618\n",
            "Batch Number: 827\n",
            "Loss: 0.09273318201303482\n",
            "Batch Number: 828\n",
            "Loss: 0.4476870596408844\n",
            "Batch Number: 829\n",
            "Loss: 0.16604438424110413\n",
            "Batch Number: 830\n",
            "Loss: 0.1952468305826187\n",
            "Batch Number: 831\n",
            "Loss: 0.08047943562269211\n",
            "Batch Number: 832\n",
            "Loss: 0.3686838746070862\n",
            "Batch Number: 833\n",
            "Loss: 0.39891359210014343\n",
            "Batch Number: 834\n",
            "Loss: 0.10979926586151123\n",
            "Batch Number: 835\n",
            "Loss: 0.543467104434967\n",
            "Batch Number: 836\n",
            "Loss: 0.17840096354484558\n",
            "Batch Number: 837\n",
            "Loss: 0.19693295657634735\n",
            "Batch Number: 838\n",
            "Loss: 0.11314177513122559\n",
            "Batch Number: 839\n",
            "Loss: 0.21350102126598358\n",
            "Batch Number: 840\n",
            "Loss: 0.09045188874006271\n",
            "Batch Number: 841\n",
            "Loss: 0.12878413498401642\n",
            "Batch Number: 842\n",
            "Loss: 0.2272147387266159\n",
            "Batch Number: 843\n",
            "Loss: 0.4053526818752289\n",
            "Batch Number: 844\n",
            "Loss: 0.4505647122859955\n",
            "Batch Number: 845\n",
            "Loss: 0.3480585515499115\n",
            "Batch Number: 846\n",
            "Loss: 0.18119633197784424\n",
            "Batch Number: 847\n",
            "Loss: 0.4776802062988281\n",
            "Batch Number: 848\n",
            "Loss: 0.09884434193372726\n",
            "Batch Number: 849\n",
            "Loss: 0.1299075335264206\n",
            "Batch Number: 850\n",
            "Loss: 0.07133504003286362\n",
            "Batch Number: 851\n",
            "Loss: 0.333368182182312\n",
            "Batch Number: 852\n",
            "Loss: 0.25750139355659485\n",
            "Batch Number: 853\n",
            "Loss: 0.13904552161693573\n",
            "Batch Number: 854\n",
            "Loss: 0.09080418199300766\n",
            "Batch Number: 855\n",
            "Loss: 0.06859347969293594\n",
            "Batch Number: 856\n",
            "Loss: 0.13852699100971222\n",
            "Batch Number: 857\n",
            "Loss: 0.38476046919822693\n",
            "Batch Number: 858\n",
            "Loss: 0.15363802015781403\n",
            "Batch Number: 859\n",
            "Loss: 0.10853450745344162\n",
            "Batch Number: 860\n",
            "Loss: 0.0846153125166893\n",
            "Batch Number: 861\n",
            "Loss: 0.2542780041694641\n",
            "Batch Number: 862\n",
            "Loss: 0.23736976087093353\n",
            "Batch Number: 863\n",
            "Loss: 0.2679765820503235\n",
            "Batch Number: 864\n",
            "Loss: 0.07339581102132797\n",
            "Batch Number: 865\n",
            "Loss: 0.07801254838705063\n",
            "Batch Number: 866\n",
            "Loss: 0.22165822982788086\n",
            "Batch Number: 867\n",
            "Loss: 0.7066320776939392\n",
            "Batch Number: 868\n",
            "Loss: 0.106535904109478\n",
            "Batch Number: 869\n",
            "Loss: 0.36036983132362366\n",
            "Batch Number: 870\n",
            "Loss: 0.28090423345565796\n",
            "Batch Number: 871\n",
            "Loss: 0.12394223362207413\n",
            "Batch Number: 872\n",
            "Loss: 0.39276859164237976\n",
            "Batch Number: 873\n",
            "Loss: 0.28787901997566223\n",
            "Batch Number: 874\n",
            "Loss: 0.1834326982498169\n",
            "Batch Number: 875\n",
            "Loss: 0.3415781557559967\n",
            "Batch Number: 876\n",
            "Loss: 0.12302249670028687\n",
            "Batch Number: 877\n",
            "Loss: 0.2470775693655014\n",
            "Batch Number: 878\n",
            "Loss: 0.014983034692704678\n",
            "Batch Number: 879\n",
            "Loss: 0.27302393317222595\n",
            "Batch Number: 880\n",
            "Loss: 0.26232555508613586\n",
            "Batch Number: 881\n",
            "Loss: 0.10875719040632248\n",
            "Batch Number: 882\n",
            "Loss: 0.11094122380018234\n",
            "Batch Number: 883\n",
            "Loss: 0.34702473878860474\n",
            "Batch Number: 884\n",
            "Loss: 0.31169161200523376\n",
            "Batch Number: 885\n",
            "Loss: 0.11829705536365509\n",
            "Batch Number: 886\n",
            "Loss: 0.027887990698218346\n",
            "Batch Number: 887\n",
            "Loss: 0.25299569964408875\n",
            "Batch Number: 888\n",
            "Loss: 0.26956942677497864\n",
            "Batch Number: 889\n",
            "Loss: 0.06773330271244049\n",
            "Batch Number: 890\n",
            "Loss: 0.061356205493211746\n",
            "Batch Number: 891\n",
            "Loss: 0.31251317262649536\n",
            "Batch Number: 892\n",
            "Loss: 0.04859482869505882\n",
            "Batch Number: 893\n",
            "Loss: 0.27992573380470276\n",
            "Batch Number: 894\n",
            "Loss: 0.30731603503227234\n",
            "Batch Number: 895\n",
            "Loss: 0.2717556953430176\n",
            "Batch Number: 896\n",
            "Loss: 0.27189746499061584\n",
            "Batch Number: 897\n",
            "Loss: 0.03637853264808655\n",
            "Batch Number: 898\n",
            "Loss: 0.29948684573173523\n",
            "Batch Number: 899\n",
            "Loss: 0.03752563148736954\n",
            "Batch Number: 900\n",
            "Loss: 0.03576350212097168\n",
            "Batch Number: 901\n",
            "Loss: 0.06118224188685417\n",
            "Batch Number: 902\n",
            "Loss: 0.35043248534202576\n",
            "Batch Number: 903\n",
            "Loss: 0.18562979996204376\n",
            "Batch Number: 904\n",
            "Loss: 0.30557939410209656\n",
            "Batch Number: 905\n",
            "Loss: 0.049077924340963364\n",
            "Batch Number: 906\n",
            "Loss: 0.3457280695438385\n",
            "Batch Number: 907\n",
            "Loss: 0.28146955370903015\n",
            "Batch Number: 908\n",
            "Loss: 0.5712994337081909\n",
            "Batch Number: 909\n",
            "Loss: 0.11377564817667007\n",
            "Batch Number: 910\n",
            "Loss: 0.27405229210853577\n",
            "Batch Number: 911\n",
            "Loss: 0.30340513586997986\n",
            "Batch Number: 912\n",
            "Loss: 0.11374640464782715\n",
            "Batch Number: 913\n",
            "Loss: 0.10611464083194733\n",
            "Batch Number: 914\n",
            "Loss: 0.6211159229278564\n",
            "Batch Number: 915\n",
            "Loss: 0.07979688048362732\n",
            "Batch Number: 916\n",
            "Loss: 0.34116098284721375\n",
            "Batch Number: 917\n",
            "Loss: 0.2062128335237503\n",
            "Batch Number: 918\n",
            "Loss: 0.16642968356609344\n",
            "Batch Number: 919\n",
            "Loss: 0.11873984336853027\n",
            "Batch Number: 920\n",
            "Loss: 0.05868731811642647\n",
            "Batch Number: 921\n",
            "Loss: 0.3764582574367523\n",
            "Batch Number: 922\n",
            "Loss: 0.202019602060318\n",
            "Batch Number: 923\n",
            "Loss: 0.3048299252986908\n",
            "Batch Number: 924\n",
            "Loss: 0.2810840606689453\n",
            "Batch Number: 925\n",
            "Loss: 0.1448609083890915\n",
            "Batch Number: 926\n",
            "Loss: 0.07209064066410065\n",
            "Batch Number: 927\n",
            "Loss: 0.10015811771154404\n",
            "Batch Number: 928\n",
            "Loss: 0.2036541998386383\n",
            "Batch Number: 929\n",
            "Loss: 0.10270710289478302\n",
            "Batch Number: 930\n",
            "Loss: 0.09041702002286911\n",
            "Batch Number: 931\n",
            "Loss: 0.742759644985199\n",
            "Batch Number: 932\n",
            "Loss: 0.05144599825143814\n",
            "Batch Number: 933\n",
            "Loss: 0.2648967206478119\n",
            "Batch Number: 934\n",
            "Loss: 0.9606160521507263\n",
            "Batch Number: 935\n",
            "Loss: 0.15969829261302948\n",
            "Batch Number: 936\n",
            "Loss: 0.22821243107318878\n",
            "Batch Number: 937\n",
            "Loss: 0.0772872343659401\n",
            "Batch Number: 938\n",
            "Loss: 0.10496383160352707\n",
            "Batch Number: 939\n",
            "Loss: 0.13097234070301056\n",
            "Batch Number: 940\n",
            "Loss: 0.6272281408309937\n",
            "Batch Number: 941\n",
            "Loss: 0.10709686577320099\n",
            "Batch Number: 942\n",
            "Loss: 0.20855610072612762\n",
            "Batch Number: 943\n",
            "Loss: 0.29368406534194946\n",
            "Batch Number: 944\n",
            "Loss: 0.6798481345176697\n",
            "Batch Number: 945\n",
            "Loss: 0.14109455049037933\n",
            "Batch Number: 946\n",
            "Loss: 0.17460115253925323\n",
            "Batch Number: 947\n",
            "Loss: 0.26754286885261536\n",
            "Batch Number: 948\n",
            "Loss: 0.24255572259426117\n",
            "Batch Number: 949\n",
            "Loss: 0.3548051118850708\n",
            "Batch Number: 950\n",
            "Loss: 0.11974339932203293\n",
            "Batch Number: 951\n",
            "Loss: 0.35251525044441223\n",
            "Batch Number: 952\n",
            "Loss: 0.20370379090309143\n",
            "Batch Number: 953\n",
            "Loss: 0.09274247288703918\n",
            "Batch Number: 954\n",
            "Loss: 0.007656185422092676\n",
            "Batch Number: 955\n",
            "Loss: 0.2977598309516907\n",
            "Batch Number: 956\n",
            "Loss: 0.18489864468574524\n",
            "Batch Number: 957\n",
            "Loss: 0.030820870772004128\n",
            "Batch Number: 958\n",
            "Loss: 0.07015886157751083\n",
            "Batch Number: 959\n",
            "Loss: 0.24681797623634338\n",
            "Batch Number: 960\n",
            "Loss: 0.07702723890542984\n",
            "Batch Number: 961\n",
            "Loss: 0.1924121081829071\n",
            "Batch Number: 962\n",
            "Loss: 0.0932384580373764\n",
            "Batch Number: 963\n",
            "Loss: 0.4161302149295807\n",
            "Batch Number: 964\n",
            "Loss: 0.19763706624507904\n",
            "Batch Number: 965\n",
            "Loss: 1.097043752670288\n",
            "Batch Number: 966\n",
            "Loss: 0.3564005494117737\n",
            "Batch Number: 967\n",
            "Loss: 0.1535804122686386\n",
            "Batch Number: 968\n",
            "Loss: 0.43348827958106995\n",
            "Batch Number: 969\n",
            "Loss: 0.17049510776996613\n",
            "Batch Number: 970\n",
            "Loss: 0.18972423672676086\n",
            "Batch Number: 971\n",
            "Loss: 0.1839302033185959\n",
            "Batch Number: 972\n",
            "Loss: 0.44312506914138794\n",
            "Batch Number: 973\n",
            "Loss: 0.21011845767498016\n",
            "Batch Number: 974\n",
            "Loss: 0.270076721906662\n",
            "Batch Number: 975\n",
            "Loss: 0.12464263290166855\n",
            "Batch Number: 976\n",
            "Loss: 0.09616389870643616\n",
            "Batch Number: 977\n",
            "Loss: 0.08449605107307434\n",
            "Batch Number: 978\n",
            "Loss: 0.3945748507976532\n",
            "Batch Number: 979\n",
            "Loss: 0.08764072507619858\n",
            "Batch Number: 980\n",
            "Loss: 0.09042868763208389\n",
            "Batch Number: 981\n",
            "Loss: 0.19402718544006348\n",
            "Batch Number: 982\n",
            "Loss: 0.12588807940483093\n",
            "Batch Number: 983\n",
            "Loss: 0.13854137063026428\n",
            "Batch Number: 984\n",
            "Loss: 0.16974660754203796\n",
            "Batch Number: 985\n",
            "Loss: 0.5104236006736755\n",
            "Batch Number: 986\n",
            "Loss: 0.10550477355718613\n",
            "Batch Number: 987\n",
            "Loss: 0.06969550251960754\n",
            "Batch Number: 988\n",
            "Loss: 0.015402333810925484\n",
            "Batch Number: 989\n",
            "Loss: 0.051178913563489914\n",
            "Batch Number: 990\n",
            "Loss: 0.285008043050766\n",
            "Batch Number: 991\n",
            "Loss: 0.2935880124568939\n",
            "Batch Number: 992\n",
            "Loss: 0.2837960422039032\n",
            "Batch Number: 993\n",
            "Loss: 0.3797008991241455\n",
            "Batch Number: 994\n",
            "Loss: 0.11315479129552841\n",
            "Batch Number: 995\n",
            "Loss: 0.22401535511016846\n",
            "Batch Number: 996\n",
            "Loss: 0.05210770294070244\n",
            "Batch Number: 997\n",
            "Loss: 0.10133232921361923\n",
            "Batch Number: 998\n",
            "Loss: 0.043899696320295334\n",
            "Batch Number: 999\n",
            "Loss: 0.07763643562793732\n",
            "Batch Number: 1000\n",
            "Loss: 0.04046959802508354\n",
            "Batch Number: 1001\n",
            "Loss: 0.2540982663631439\n",
            "Batch Number: 1002\n",
            "Loss: 0.10126633942127228\n",
            "Batch Number: 1003\n",
            "Loss: 0.24238646030426025\n",
            "Batch Number: 1004\n",
            "Loss: 0.2693561017513275\n",
            "Batch Number: 1005\n",
            "Loss: 0.05904748663306236\n",
            "Batch Number: 1006\n",
            "Loss: 0.07927235215902328\n",
            "Batch Number: 1007\n",
            "Loss: 0.13465674221515656\n",
            "Batch Number: 1008\n",
            "Loss: 0.18141593039035797\n",
            "Batch Number: 1009\n",
            "Loss: 0.24451270699501038\n",
            "Batch Number: 1010\n",
            "Loss: 0.11561975628137589\n",
            "Batch Number: 1011\n",
            "Loss: 0.28457269072532654\n",
            "Batch Number: 1012\n",
            "Loss: 0.1985258162021637\n",
            "Batch Number: 1013\n",
            "Loss: 0.2936362326145172\n",
            "Batch Number: 1014\n",
            "Loss: 0.3122827708721161\n",
            "Batch Number: 1015\n",
            "Loss: 0.2533157467842102\n",
            "Batch Number: 1016\n",
            "Loss: 0.1683981865644455\n",
            "Batch Number: 1017\n",
            "Loss: 0.910281777381897\n",
            "Batch Number: 1018\n",
            "Loss: 0.6190297603607178\n",
            "Batch Number: 1019\n",
            "Loss: 0.11678941547870636\n",
            "Batch Number: 1020\n",
            "Loss: 0.16656985878944397\n",
            "Batch Number: 1021\n",
            "Loss: 0.14340972900390625\n",
            "Batch Number: 1022\n",
            "Loss: 0.4703923165798187\n",
            "Batch Number: 1023\n",
            "Loss: 0.07610058039426804\n",
            "Batch Number: 1024\n",
            "Loss: 0.293306440114975\n",
            "Batch Number: 1025\n",
            "Loss: 0.235203817486763\n",
            "Batch Number: 1026\n",
            "Loss: 0.27103811502456665\n",
            "Batch Number: 1027\n",
            "Loss: 0.08758734166622162\n",
            "Batch Number: 1028\n",
            "Loss: 0.0564844124019146\n",
            "Batch Number: 1029\n",
            "Loss: 0.23841872811317444\n",
            "Batch Number: 1030\n",
            "Loss: 0.6863265037536621\n",
            "Batch Number: 1031\n",
            "Loss: 0.5255541801452637\n",
            "Batch Number: 1032\n",
            "Loss: 0.16948625445365906\n",
            "Batch Number: 1033\n",
            "Loss: 0.25735679268836975\n",
            "Batch Number: 1034\n",
            "Loss: 0.27374324202537537\n",
            "Batch Number: 1035\n",
            "Loss: 0.31632304191589355\n",
            "Batch Number: 1036\n",
            "Loss: 0.2842237651348114\n",
            "Batch Number: 1037\n",
            "Loss: 0.10925173759460449\n",
            "Batch Number: 1038\n",
            "Loss: 0.44561606645584106\n",
            "Batch Number: 1039\n",
            "Loss: 0.664655327796936\n",
            "Batch Number: 1040\n",
            "Loss: 0.25897273421287537\n",
            "Batch Number: 1041\n",
            "Loss: 0.04020991548895836\n",
            "Batch Number: 1042\n",
            "Loss: 0.021775929257273674\n",
            "Batch Number: 1043\n",
            "Loss: 0.4642711281776428\n",
            "Batch Number: 1044\n",
            "Loss: 0.16444739699363708\n",
            "Batch Number: 1045\n",
            "Loss: 0.3661440312862396\n",
            "Batch Number: 1046\n",
            "Loss: 0.19400642812252045\n",
            "Batch Number: 1047\n",
            "Loss: 0.10397541522979736\n",
            "Batch Number: 1048\n",
            "Loss: 0.08568430691957474\n",
            "Batch Number: 1049\n",
            "Loss: 0.3903714120388031\n",
            "Batch Number: 1050\n",
            "Loss: 0.08432728052139282\n",
            "Batch Number: 1051\n",
            "Loss: 0.25542309880256653\n",
            "Batch Number: 1052\n",
            "Loss: 0.2989657521247864\n",
            "Batch Number: 1053\n",
            "Loss: 0.29682716727256775\n",
            "Batch Number: 1054\n",
            "Loss: 0.8653502464294434\n",
            "Batch Number: 1055\n",
            "Loss: 0.3629395067691803\n",
            "Batch Number: 1056\n",
            "Loss: 0.06270527094602585\n",
            "Batch Number: 1057\n",
            "Loss: 0.07654314488172531\n",
            "Batch Number: 1058\n",
            "Loss: 0.11309999227523804\n",
            "Batch Number: 1059\n",
            "Loss: 0.07933016866445541\n",
            "Batch Number: 1060\n",
            "Loss: 0.22288143634796143\n",
            "Batch Number: 1061\n",
            "Loss: 0.7134087085723877\n",
            "Batch Number: 1062\n",
            "Loss: 0.1512715369462967\n",
            "Batch Number: 1063\n",
            "Loss: 0.13662374019622803\n",
            "Batch Number: 1064\n",
            "Loss: 0.051065731793642044\n",
            "Batch Number: 1065\n",
            "Loss: 0.6624376773834229\n",
            "Batch Number: 1066\n",
            "Loss: 0.14346010982990265\n",
            "Batch Number: 1067\n",
            "Loss: 0.15975962579250336\n",
            "Batch Number: 1068\n",
            "Loss: 0.34278222918510437\n",
            "Batch Number: 1069\n",
            "Loss: 0.14975817501544952\n",
            "Batch Number: 1070\n",
            "Loss: 0.20203442871570587\n",
            "Batch Number: 1071\n",
            "Loss: 0.2596062123775482\n",
            "Batch Number: 1072\n",
            "Loss: 0.15430259704589844\n",
            "Batch Number: 1073\n",
            "Loss: 0.12988542020320892\n",
            "Batch Number: 1074\n",
            "Loss: 0.15020154416561127\n",
            "Batch Number: 1075\n",
            "Loss: 0.13261936604976654\n",
            "Batch Number: 1076\n",
            "Loss: 0.19422364234924316\n",
            "Batch Number: 1077\n",
            "Loss: 0.14195238053798676\n",
            "Batch Number: 1078\n",
            "Loss: 0.09791891276836395\n",
            "Batch Number: 1079\n",
            "Loss: 0.15592679381370544\n",
            "Batch Number: 1080\n",
            "Loss: 0.22856175899505615\n",
            "Batch Number: 1081\n",
            "Loss: 0.16128145158290863\n",
            "Batch Number: 1082\n",
            "Loss: 0.06738057732582092\n",
            "Batch Number: 1083\n",
            "Loss: 0.26019802689552307\n",
            "Batch Number: 1084\n",
            "Loss: 0.08973321318626404\n",
            "Batch Number: 1085\n",
            "Loss: 0.3509519696235657\n",
            "Batch Number: 1086\n",
            "Loss: 0.3022175431251526\n",
            "Batch Number: 1087\n",
            "Loss: 0.24409924447536469\n",
            "Batch Number: 1088\n",
            "Loss: 0.18576472997665405\n",
            "Batch Number: 1089\n",
            "Loss: 0.3430863618850708\n",
            "Batch Number: 1090\n",
            "Loss: 0.06523729860782623\n",
            "Batch Number: 1091\n",
            "Loss: 0.1055426150560379\n",
            "Batch Number: 1092\n",
            "Loss: 0.30471253395080566\n",
            "Batch Number: 1093\n",
            "Loss: 0.2817046046257019\n",
            "Batch Number: 1094\n",
            "Loss: 0.2328888475894928\n",
            "Batch Number: 1095\n",
            "Loss: 0.28564056754112244\n",
            "Batch Number: 1096\n",
            "Loss: 0.35194915533065796\n",
            "Batch Number: 1097\n",
            "Loss: 0.10626651346683502\n",
            "Batch Number: 1098\n",
            "Loss: 0.17840056121349335\n",
            "Batch Number: 1099\n",
            "Loss: 0.13142138719558716\n",
            "Batch Number: 1100\n",
            "Loss: 0.13845756649971008\n",
            "Batch Number: 1101\n",
            "Loss: 0.314680278301239\n",
            "Batch Number: 1102\n",
            "Loss: 0.18636956810951233\n",
            "Batch Number: 1103\n",
            "Loss: 0.4161565899848938\n",
            "Batch Number: 1104\n",
            "Loss: 0.23041950166225433\n",
            "Batch Number: 1105\n",
            "Loss: 0.22868788242340088\n",
            "Batch Number: 1106\n",
            "Loss: 0.32376623153686523\n",
            "Batch Number: 1107\n",
            "Loss: 0.11603742092847824\n",
            "Batch Number: 1108\n",
            "Loss: 0.06716521829366684\n",
            "Batch Number: 1109\n",
            "Loss: 0.13324470818042755\n",
            "Batch Number: 1110\n",
            "Loss: 0.15786142647266388\n",
            "Batch Number: 1111\n",
            "Loss: 0.24296660721302032\n",
            "Batch Number: 1112\n",
            "Loss: 0.020390499383211136\n",
            "Batch Number: 1113\n",
            "Loss: 0.16314397752285004\n",
            "Batch Number: 1114\n",
            "Loss: 0.1772233247756958\n",
            "Batch Number: 1115\n",
            "Loss: 0.12410755455493927\n",
            "Batch Number: 1116\n",
            "Loss: 0.09034693986177444\n",
            "Batch Number: 1117\n",
            "Loss: 0.04041115567088127\n",
            "Batch Number: 1118\n",
            "Loss: 0.08660470694303513\n",
            "Batch Number: 1119\n",
            "Loss: 0.17067615687847137\n",
            "Batch Number: 1120\n",
            "Loss: 0.430422306060791\n",
            "Batch Number: 1121\n",
            "Loss: 0.13468943536281586\n",
            "Batch Number: 1122\n",
            "Loss: 0.17826339602470398\n",
            "Batch Number: 1123\n",
            "Loss: 0.12744541466236115\n",
            "Batch Number: 1124\n",
            "Loss: 0.29534173011779785\n",
            "Batch Number: 1125\n",
            "Loss: 0.3328559100627899\n",
            "Batch Number: 1126\n",
            "Loss: 0.21255555748939514\n",
            "Batch Number: 1127\n",
            "Loss: 0.3115609288215637\n",
            "Batch Number: 1128\n",
            "Loss: 0.25440192222595215\n",
            "Batch Number: 1129\n",
            "Loss: 0.15479198098182678\n",
            "Batch Number: 1130\n",
            "Loss: 0.07654207944869995\n",
            "Batch Number: 1131\n",
            "Loss: 0.46135178208351135\n",
            "Batch Number: 1132\n",
            "Loss: 0.20716063678264618\n",
            "Batch Number: 1133\n",
            "Loss: 0.17745046317577362\n",
            "Batch Number: 1134\n",
            "Loss: 0.11759187281131744\n",
            "Batch Number: 1135\n",
            "Loss: 0.4018450379371643\n",
            "Batch Number: 1136\n",
            "Loss: 0.09815439581871033\n",
            "Batch Number: 1137\n",
            "Loss: 0.0720062330365181\n",
            "Batch Number: 1138\n",
            "Loss: 0.16021130979061127\n",
            "Batch Number: 1139\n",
            "Loss: 0.1850372552871704\n",
            "Batch Number: 1140\n",
            "Loss: 0.3052498698234558\n",
            "Batch Number: 1141\n",
            "Loss: 0.17311224341392517\n",
            "Batch Number: 1142\n",
            "Loss: 0.07208377122879028\n",
            "Validation\n",
            "Validation Loss: 0.038920044898986816\n",
            "Validation Loss: 0.2651370167732239\n",
            "Validation Loss: 0.1291550248861313\n",
            "Validation Loss: 0.16348613798618317\n",
            "Validation Loss: 0.1718975454568863\n",
            "Validation Loss: 0.3046310842037201\n",
            "Validation Loss: 0.3746858537197113\n",
            "Validation Loss: 0.27391287684440613\n",
            "Validation Loss: 0.438218891620636\n",
            "Validation Loss: 0.4635888636112213\n",
            "Validation Loss: 0.31328120827674866\n",
            "Validation Loss: 0.6462303996086121\n",
            "Validation Loss: 0.6063807606697083\n",
            "Validation Loss: 0.33100301027297974\n",
            "Validation Loss: 0.9173189401626587\n",
            "Validation Loss: 0.4564898610115051\n",
            "Validation Loss: 0.029500901699066162\n",
            "Validation Loss: 0.5875200629234314\n",
            "Validation Loss: 0.42640191316604614\n",
            "Validation Loss: 0.5992302894592285\n",
            "Validation Loss: 0.08296304941177368\n",
            "Validation Loss: 0.5484558939933777\n",
            "Validation Loss: 0.1366635113954544\n",
            "Validation Loss: 0.31391388177871704\n",
            "Validation Loss: 0.25656965374946594\n",
            "Validation Loss: 0.65826815366745\n",
            "Validation Loss: 0.09782193601131439\n",
            "Validation Loss: 0.17563901841640472\n",
            "Validation Loss: 0.21154704689979553\n",
            "Validation Loss: 0.18709945678710938\n",
            "Validation Loss: 0.8337961435317993\n",
            "Validation Loss: 0.5828869938850403\n",
            "Validation Loss: 0.13014034926891327\n",
            "Validation Loss: 0.5799080729484558\n",
            "Validation Loss: 1.4198899269104004\n",
            "Validation Loss: 0.5161300301551819\n",
            "Validation Loss: 0.26539450883865356\n",
            "Validation Loss: 0.30130359530448914\n",
            "Validation Loss: 0.4492398202419281\n",
            "Validation Loss: 0.2575572729110718\n",
            "Validation Loss: 0.4774913787841797\n",
            "Validation Loss: 1.2782318592071533\n",
            "Validation Loss: 0.06887754797935486\n",
            "Validation Loss: 0.4855705201625824\n",
            "Validation Loss: 0.14065706729888916\n",
            "Validation Loss: 0.118904247879982\n",
            "Validation Loss: 0.11534591019153595\n",
            "Validation Loss: 0.1599142998456955\n",
            "Validation Loss: 0.45813876390457153\n",
            "Validation Loss: 0.16621874272823334\n",
            "Validation Loss: 0.1751488298177719\n",
            "Validation Loss: 0.6791191697120667\n",
            "Validation Loss: 0.07699815183877945\n",
            "Validation Loss: 0.30427050590515137\n",
            "Validation Loss: 0.6517571806907654\n",
            "Validation Loss: 0.26847076416015625\n",
            "Validation Loss: 0.758999764919281\n",
            "Validation Loss: 0.649599015712738\n",
            "Validation Loss: 0.32992589473724365\n",
            "Validation Loss: 0.15415208041667938\n",
            "Validation Loss: 0.36075982451438904\n",
            "Validation Loss: 0.18483267724514008\n",
            "Validation Loss: 1.9894546270370483\n",
            "Validation Loss: 0.0800752118229866\n",
            "Validation Loss: 0.3187479078769684\n",
            "Validation Loss: 0.4733642637729645\n",
            "Validation Loss: 0.656277596950531\n",
            "Validation Loss: 1.109550952911377\n",
            "Validation Loss: 0.09509523957967758\n",
            "Validation Loss: 0.5985498428344727\n",
            "Validation Loss: 0.5373032689094543\n",
            "Validation Loss: 0.48768624663352966\n",
            "Validation Loss: 0.11200244724750519\n",
            "Validation Loss: 0.08752590417861938\n",
            "Validation Loss: 0.34700584411621094\n",
            "Validation Loss: 0.15227563679218292\n",
            "Validation Loss: 0.44765982031822205\n",
            "Validation Loss: 1.083351969718933\n",
            "Validation Loss: 1.367764949798584\n",
            "Validation Loss: 0.4391675591468811\n",
            "Validation Loss: 0.8936811685562134\n",
            "Validation Loss: 0.5642246603965759\n",
            "Validation Loss: 0.3932126760482788\n",
            "Validation Loss: 0.7734328508377075\n",
            "Validation Loss: 0.12847013771533966\n",
            "Validation Loss: 0.26012393832206726\n",
            "Validation Loss: 0.5332717895507812\n",
            "Validation Loss: 0.27860039472579956\n",
            "Validation Loss: 0.09349755197763443\n",
            "Validation Loss: 0.19291308522224426\n",
            "Validation Loss: 0.8925163149833679\n",
            "Validation Loss: 0.8138793110847473\n",
            "Validation Loss: 0.1763473004102707\n",
            "Validation Loss: 0.9653329849243164\n",
            "Validation Loss: 0.7388917207717896\n",
            "Validation Loss: 0.10651620477437973\n",
            "Validation Loss: 0.17302384972572327\n",
            "Validation Loss: 0.5026710629463196\n",
            "Validation Loss: 1.104033350944519\n",
            "Validation Loss: 1.1300475597381592\n",
            "Validation Loss: 0.5950645208358765\n",
            "Validation Loss: 2.3036975860595703\n",
            "Validation Loss: 0.24978385865688324\n",
            "Validation Loss: 1.5284398794174194\n",
            "Validation Loss: 0.9007119536399841\n",
            "Validation Loss: 0.029134083539247513\n",
            "Validation Loss: 0.9511699676513672\n",
            "Validation Loss: 0.21609030663967133\n",
            "Validation Loss: 0.25174498558044434\n",
            "Validation Loss: 0.280189573764801\n",
            "Validation Loss: 1.1298400163650513\n",
            "Validation Loss: 0.1958620548248291\n",
            "Validation Loss: 0.7182144522666931\n",
            "Validation Loss: 0.13202108442783356\n",
            "Validation Loss: 0.7065698504447937\n",
            "Validation Loss: 2.685746192932129\n",
            "Validation Loss: 0.09975626319646835\n",
            "Validation Loss: 0.1667049080133438\n",
            "Validation Loss: 0.18212838470935822\n",
            "Validation Loss: 0.808504045009613\n",
            "Validation Loss: 1.0810900926589966\n",
            "Validation Loss: 0.06594925373792648\n",
            "Validation Loss: 0.11818856000900269\n",
            "Validation Loss: 0.5837921500205994\n",
            "Validation Loss: 0.7156946659088135\n",
            "Validation Loss: 0.37102437019348145\n",
            "Validation Loss: 2.0066163539886475\n",
            "Validation Loss: 0.3202402591705322\n",
            "Validation Loss: 1.6876055002212524\n",
            "Validation Loss: 0.6338939070701599\n",
            "Validation Loss: 0.25565844774246216\n",
            "Validation Loss: 1.0382274389266968\n",
            "Validation Loss: 1.9279783964157104\n",
            "Validation Loss: 0.832310140132904\n",
            "Validation Loss: 0.21934370696544647\n",
            "Validation Loss: 0.3132869601249695\n",
            "Validation Loss: 0.6461932063102722\n",
            "Validation Loss: 1.172564148902893\n",
            "Validation Loss: 0.151764914393425\n",
            "Validation Loss: 2.3094165325164795\n",
            "Validation Loss: 0.21248356997966766\n",
            "Validation Loss: 0.28670522570610046\n",
            "Validation Loss: 0.3406932055950165\n",
            "Validation Loss: 1.244994044303894\n",
            "Validation Loss: 0.7259360551834106\n",
            "Validation Loss: 0.6983785033226013\n",
            "Validation Loss: 0.5591699481010437\n",
            "Validation Loss: 0.32764726877212524\n",
            "Validation Loss: 0.46802982687950134\n",
            "Validation Loss: 0.22727136313915253\n",
            "Validation Loss: 1.539284586906433\n",
            "Validation Loss: 0.19668248295783997\n",
            "Validation Loss: 0.4804905951023102\n",
            "Validation Loss: 0.10500209778547287\n",
            "Validation Loss: 0.7562844157218933\n",
            "Validation Loss: 0.07878001779317856\n",
            "Validation Loss: 0.8431808352470398\n",
            "Validation Loss: 0.2058127373456955\n",
            "Validation Loss: 1.510756015777588\n",
            "Validation Loss: 0.6678465008735657\n",
            "Validation Loss: 0.09756018221378326\n",
            "Validation Loss: 0.8672866225242615\n",
            "Validation Loss: 0.7184564471244812\n",
            "Validation Loss: 0.6663778424263\n",
            "Validation Loss: 0.3464983105659485\n",
            "Validation Loss: 0.5280941128730774\n",
            "Validation Loss: 0.7389860153198242\n",
            "Validation Loss: 0.9723068475723267\n",
            "Validation Loss: 0.29455575346946716\n",
            "Validation Loss: 1.0985697507858276\n",
            "Validation Loss: 0.5511258840560913\n",
            "Validation Loss: 0.7545133233070374\n",
            "Validation Loss: 0.13300065696239471\n",
            "Validation Loss: 0.6490471363067627\n",
            "Validation Loss: 0.4467921257019043\n",
            "Validation Loss: 0.24179458618164062\n",
            "Validation Loss: 1.0407932996749878\n",
            "Validation Loss: 0.8297578692436218\n",
            "Validation Loss: 0.6590849161148071\n",
            "Validation Loss: 0.9432048797607422\n",
            "Validation Loss: 0.7573811411857605\n",
            "Validation Loss: 0.18901847302913666\n",
            "Validation Loss: 0.3757101595401764\n",
            "Validation Loss: 0.8554951548576355\n",
            "Validation Loss: 0.6606830358505249\n",
            "Validation Loss: 0.827900230884552\n",
            "Validation Loss: 0.4504567086696625\n",
            "Validation Loss: 0.19760756194591522\n",
            "Validation Loss: 1.4848248958587646\n",
            "Validation Loss: 0.22267024219036102\n",
            "Validation Loss: 0.8426374793052673\n",
            "Validation Loss: 1.096641182899475\n",
            "Validation Loss: 0.311871737241745\n",
            "Validation Loss: 0.18938522040843964\n",
            "Validation Loss: 0.7144711017608643\n",
            "Validation Loss: 1.1648906469345093\n",
            "Validation Loss: 1.1892231702804565\n",
            "Validation Loss: 0.19428038597106934\n",
            "Validation Loss: 0.8239511847496033\n",
            "Validation Loss: 0.7725999355316162\n",
            "Validation Loss: 0.6605031490325928\n",
            "Validation Loss: 0.23633168637752533\n",
            "Validation Loss: 0.18160627782344818\n",
            "Validation Loss: 0.2723810374736786\n",
            "Validation Loss: 0.09836510568857193\n",
            "Validation Loss: 0.3341517150402069\n",
            "Validation Loss: 0.1976032704114914\n",
            "Validation Loss: 0.13782773911952972\n",
            "Validation Loss: 0.5947293639183044\n",
            "Validation Loss: 0.4789571762084961\n",
            "Validation Loss: 0.18571998178958893\n",
            "Validation Loss: 0.1348099708557129\n",
            "Validation Loss: 0.04114602506160736\n",
            "Validation Loss: 0.14462487399578094\n",
            "Validation Loss: 0.5189644694328308\n",
            "Validation Loss: 0.09467703849077225\n",
            "Validation Loss: 0.36896246671676636\n",
            "Validation Loss: 0.586986780166626\n",
            "Validation Loss: 0.2967287003993988\n",
            "Validation Loss: 0.2966766357421875\n",
            "Validation Loss: 0.63458651304245\n",
            "Validation Loss: 0.28335365653038025\n",
            "Validation Loss: 0.24184294044971466\n",
            "Validation Loss: 0.748736560344696\n",
            "Validation Loss: 0.1052771732211113\n",
            "Validation Loss: 0.14724288880825043\n",
            "Validation Loss: 0.4435212314128876\n",
            "Validation Loss: 0.32389962673187256\n",
            "Validation Loss: 0.03808598592877388\n",
            "Validation Loss: 0.33864521980285645\n",
            "Validation Loss: 0.09398351609706879\n",
            "Validation Loss: 0.20785246789455414\n",
            "Validation Loss: 0.21275046467781067\n",
            "Validation Loss: 0.26768749952316284\n",
            "Validation Loss: 0.6683098077774048\n",
            "Validation Loss: 0.06924710422754288\n",
            "Validation Loss: 0.465695858001709\n",
            "Validation Loss: 0.19755123555660248\n",
            "Validation Loss: 0.33519676327705383\n",
            "Validation Loss: 0.21814997494220734\n",
            "Validation Loss: 0.3266642689704895\n",
            "Validation Loss: 0.15395264327526093\n",
            "Validation Loss: 0.3479691445827484\n",
            "Validation Loss: 0.12001402676105499\n",
            "Validation Loss: 1.624312400817871\n",
            "Validation Loss: 0.3358829915523529\n",
            "Validation Loss: 1.0617965459823608\n",
            "Validation Loss: 0.5181384086608887\n",
            "Validation Loss: 1.6047428846359253\n",
            "Validation Loss: 0.5262225270271301\n",
            "Validation Loss: 0.4298665523529053\n",
            "Validation Loss: 0.33949026465415955\n",
            "Validation Loss: 0.4809942841529846\n",
            "Validation Loss: 0.21591606736183167\n",
            "Validation Loss: 0.23360760509967804\n",
            "Validation Loss: 0.33958783745765686\n",
            "Validation Loss: 0.8431596755981445\n",
            "Validation Loss: 0.38276633620262146\n",
            "Validation Loss: 0.7010748982429504\n",
            "Validation Loss: 0.1457832157611847\n",
            "Validation Loss: 1.2330013513565063\n",
            "Validation Loss: 2.139979124069214\n",
            "Validation Loss: 2.7188315391540527\n",
            "Validation Loss: 0.9906926155090332\n",
            "Validation Loss: 0.23711161315441132\n",
            "Validation Loss: 0.3697302043437958\n",
            "Validation Loss: 0.19784210622310638\n",
            "Validation Loss: 0.9307730793952942\n",
            "Validation Loss: 0.6779059767723083\n",
            "Validation Loss: 0.4516996443271637\n",
            "Validation Loss: 0.2248968631029129\n",
            "Validation Loss: 0.6422702074050903\n",
            "Validation Loss: 0.3974524736404419\n",
            "Validation Loss: 1.2010196447372437\n",
            "Validation Loss: 0.04895786568522453\n",
            "Validation Loss: 0.20234373211860657\n",
            "Validation Loss: 1.612894058227539\n",
            "Validation Loss: 0.6495146155357361\n",
            "Validation Loss: 0.5022010803222656\n",
            "Validation Loss: 0.15627720952033997\n",
            "Validation Loss: 0.22719871997833252\n",
            "Validation Loss: 0.5040211081504822\n",
            "Validation Loss: 0.6941056251525879\n",
            "Validation Loss: 0.34641098976135254\n",
            "Validation Loss: 0.35656651854515076\n",
            "Validation Loss: 0.6951150298118591\n",
            "Validation Loss: 0.9326974749565125\n",
            "Validation Loss: 0.3720874488353729\n",
            "Validation Loss: 0.017920633777976036\n",
            "Validation Loss: 0.574221670627594\n",
            "Validation Loss: 0.780244767665863\n",
            "Validation Loss: 1.1639466285705566\n",
            "Validation Loss: 1.213039755821228\n",
            "Validation Loss: 0.4075411260128021\n",
            "Average Training Loss: 0.22914467874389213\n",
            "Average Validation Loss: 0.5413812378193347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example data (replace with your actual data)\n",
        "epochs = range(1, 6)\n",
        "train_loss = train_loss_epochs\n",
        "val_loss = val_loss_epochs\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss per Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "reckIreTU5qJ",
        "outputId": "a8b53a88-f4f8-4eab-8904-f051766755e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from scipy.stats import pearsonr\n",
        "model.eval()\n",
        "predicted_scores = []\n",
        "true_scores = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids, token_type_ids, attention_mask, targets = batch\n",
        "        outputs = model(input_ids=input_ids.to(device), token_type_ids=token_type_ids.to(device), attention_mask=attention_mask.to(device))\n",
        "        logits = outputs.logits.squeeze().cpu().numpy()  # Flatten the logits and move to CPU\n",
        "        targets = targets.squeeze().cpu().numpy()  # Flatten the targets and move to CPU\n",
        "        predicted_scores.extend(logits)\n",
        "        true_scores.extend(targets)\n",
        "\n",
        "# Compute Pearson correlation coefficient\n",
        "pearson_corr, _ = pearsonr(predicted_scores, true_scores)\n",
        "print(\"Pearson correlation coefficient:\", pearson_corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x30vdAsr8L3H",
        "outputId": "7b6e68bd-1b2d-4ed0-b2a9-8c3f0d3ed89c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation coefficient: 0.8822675584768873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'BERT_STS_1A.pth')"
      ],
      "metadata": {
        "id": "adhaFh0RNUP-"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}