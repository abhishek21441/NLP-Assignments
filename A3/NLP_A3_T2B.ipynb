{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e357f572f4045f5a872664ccb068ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhis\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf71443b29e84dd0b98a45cd20f9347c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6f089b8a0848c5b73fac32de32f154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load T5 model and tokenizer\n",
    "model_name = \"google-t5/t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load validation and test datasets\n",
    "validation_dataset = load_dataset(\"wmt16\", \"de-en\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wmt16\", \"de-en\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, test_data = [], []\n",
    "\n",
    "for i in validation_dataset[\"translation\"]:\n",
    "    temp = []\n",
    "    temp.append(i[\"de\"])\n",
    "    temp.append(i[\"en\"])\n",
    "    temp = tuple(temp)\n",
    "    val_data.append(temp)\n",
    "\n",
    "for i in test_dataset[\"translation\"]:\n",
    "    temp = []\n",
    "    temp.append(i[\"de\"])\n",
    "    temp.append(i[\"en\"])\n",
    "    temp = tuple(temp)\n",
    "    test_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prefix for zero-shot translation\n",
    "prefix = \"translate English to German: \"\n",
    "\n",
    "# Generate translations for validation set\n",
    "valid_translations = []\n",
    "valid_references = []\n",
    "for example in val_data:\n",
    "    input_text = prefix + example[1]\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "    output_ids = model.generate(input_ids)\n",
    "    translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    valid_translations.append(translated_text)\n",
    "    valid_references.append(example[0])\n",
    "\n",
    "# Generate translations for test set\n",
    "test_translations = []\n",
    "test_references = []\n",
    "for example in test_data:\n",
    "    input_text = prefix + example[1]\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "    output_ids = model.generate(input_ids)\n",
    "    translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    test_translations.append(translated_text)\n",
    "    test_references.append(example[0])\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "meteor_metric = load_metric(\"meteor\")\n",
    "bertscore_metric = load_metric(\"bertscore\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score_valid = bleu_metric.compute(predictions=valid_translations, references=valid_references)\n",
    "bleu_score_test = bleu_metric.compute(predictions=test_translations, references=test_references)\n",
    "\n",
    "# Calculate METEOR score\n",
    "meteor_score_valid = meteor_metric.compute(predictions=valid_translations, references=valid_references)\n",
    "meteor_score_test = meteor_metric.compute(predictions=test_translations, references=test_references)\n",
    "\n",
    "# Calculate BERTScore\n",
    "bertscore_score_valid = bertscore_metric.compute(predictions=valid_translations, references=valid_references)\n",
    "bertscore_score_test = bertscore_metric.compute(predictions=test_translations, references=test_references)\n",
    "\n",
    "# Print evaluation metrics for validation set\n",
    "print(\"Validation Set Metrics:\")\n",
    "print(f\"BLEU Score: {bleu_score_valid['bleu']}\")\n",
    "print(f\"METEOR Score: {meteor_score_valid['meteor']}\")\n",
    "print(f\"BERTScore Score: {bertscore_score_valid['bertscore'].mean()}\")\n",
    "\n",
    "# Print evaluation metrics for test set\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"BLEU Score: {bleu_score_test['bleu']}\")\n",
    "print(f\"METEOR Score: {meteor_score_test['meteor']}\")\n",
    "print(f\"BERTScore Score: {bertscore_score_test['bertscore'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: Eine Gruppe von Menschen, die vor einem Iglu stehen.\n",
      "Translations saved to output.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_metric\n",
    "import csv\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = \"google-t5/t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define prefix for zero-shot translation\n",
    "prefix = \"translate English to German: \"\n",
    "\n",
    "# Define function for translation\n",
    "def translate_sentence(sentence):\n",
    "    input_text = prefix + sentence\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "    output_ids = model.generate(input_ids)\n",
    "    translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Define function to translate English sentences from a CSV file to German\n",
    "def translate_csv_to_german(input_csv_file, output_csv_file):\n",
    "    with open(input_csv_file, 'r', encoding='utf-8') as input_file:\n",
    "        csv_reader = csv.DictReader(input_file)\n",
    "        rows = list(csv_reader)\n",
    "\n",
    "    translated_rows = []\n",
    "    for row in rows:\n",
    "        english_sentence = row['en']\n",
    "        translated_sentence = translate_sentence(english_sentence)\n",
    "        row['de'] = translated_sentence\n",
    "        translated_rows.append(row)\n",
    "\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        fieldnames = ['en', 'de']\n",
    "        csv_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(translated_rows)\n",
    "\n",
    "# Example usage:\n",
    "# Translate a single English sentence to German\n",
    "english_sentence = \"A group of people standing in front of an igloo.\"\n",
    "translated_sentence = translate_sentence(english_sentence)\n",
    "print(\"Translated Sentence:\", translated_sentence)\n",
    "\n",
    "# Translate sentences from a CSV file to German\n",
    "input_csv_file = \"input_2B.csv\"\n",
    "output_csv_file = \"output_task2B.csv\"\n",
    "translate_csv_to_german(input_csv_file, output_csv_file)\n",
    "print(\"Translations saved to output.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
